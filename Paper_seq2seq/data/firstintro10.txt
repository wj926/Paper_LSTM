Density estimation is a popular unsupervised learning technique for recovering distributions from data.
Brain-Computer Interfaces (BCIs) translate the intent of a subject measured from brain signals directly into control commands, e.
The general ranking problem has widespread applications including commercial search engines and recommender systems.
The soldiers on a parade ground form a neat rectangle by interacting with their neighbors.
Machine learning has been applied to a number of tasks involving an input domain with a special topology: one-dimensional for sequences, two-dimensional for images, three-dimensional for videos and for 3-D capture.
Dimension reductions in the lα norm (0 < α ≤ 2) have numerous applications in data mining, information retrieval, and machine learning.
Determining the assumptions that guide human learning and inference is one of the central goals of cognitive science.
Human visual activity often involves search.
Active learning addresses the issue that, in many applications, labeled data typically comes at a higher cost (e.
In the so-called Ensemble Clustering problem, the target is to ‘combine’ multiple clustering solutions or partitions of a set into a single consolidated clustering that maximizes the information shared (or ‘agreement’) among all available clustering solutions.
A central tenet of the Bayesian program is the representation of beliefs by distributions, which assign probability to each of a set of hypotheses.
Why are we concerned by deviations? The efficiency of an algorithm can be summarized by its expected risk, but this does not precise the fluctuations of its risk.
In recent years optimization methods for convex models have seen significant progress.
Spike driven synaptic plasticity mechanisms have been thoroughly investigated in recent years to solve two important problems of learning: 1) how to modify the synapses in order to generate new memories 2) how to protect old memories against the passage of time, and the overwriting of new memories by ongoing activity.
When an agent is faced with the task of learning how to behave in a stochastic environment, a common approach is to model the situation using a Markov Decision Process.
We present an optimal way to combine binary classifiers in the Neyman-Pearson sense: for a given upper bound on false alarms (false positives), we find the set of combination rules maximizing the detection rate (true positives).
One quintessential problem in statistical learning [9, 20] is to construct a classifier from labeled training data (xi , yi ) ∼iid pX,Y (x, y).
Plotting a peristimulus time histogram (PSTH), or a spike density function (SDF), from spiketrains evoked by and aligned to a stimulus onset is often one of the first steps in the analysis of neurophysiological data.
Measurement of similarity is a critical asset of state of the art in computer vision.
Most problems in machine intelligence can be formulated as probabilistic inference using probabilistic models defined on structured knowledge representations.
The human visual system provides an arena in which objects compete for our visual attention, and a given object may win the competition with support from a number of influences.
Binocular disparity, the displacement between the image locations of an object between two eyes or cameras, is an important depth cue.
In this paper we consider apprenticeship learning in the setting of large, complex domains.
An important problem in statistics and machine learning consists in testing whether the distributions of two random variables are identical under the alternative that they may differ in some ways.
In recent years maximum variance unfolding (MVU), introduced by Saul et al.
Many applications of graphical models have traditionally dealt with discrete state spaces, where each variable is multinomial distributed given its parents [1].
Synchrony is an important topic in neuroscience.
Learning a good classifier requires a sufficient number of labeled training instances.
In many practical situations, a classification task can often be divided into related sub-tasks.
Movement planning and control is a very difficult problem in real-world applications.
Here, we present an algorithm for support vector machine (SVM) classification using indefinite kernels.
Matrix analysis techniques, e.
A frequently encountered problem in many fields is the analysis of histogram data to extract meaningful latent factors from it.
Semi-supervised inductive learning concerns the problem of automatically learning a decision rule from a set of both labeled and unlabeled data, which has received a great deal of attention due to enormous demands of real world learning tasks ranging from data mining to medical diagnosis [1].
In unsupervised problems where observed data has sequential, recursive, spatial, relational, or other kinds of structure, we often employ statistical models with latent variables to tease apart the underlying dependencies and induce meaningful semantic parts.
Boosting algorithms use simple “base classifiers” to build more complex, but more accurate, aggregate classifiers.
In recent years, latent annotation of PCFG has been shown to perform as well as or better than standard lexicalized methods for treebank parsing [1, 2].
A key requirement of biological or artificial agents acting in a random dynamical environment is estimating the state of the environment based on noisy observations.
In many real world systems, uncertainty can arise in both the prediction of the system’s behavior, and the observability of the system’s state.
A major puzzle for understanding learning in biological organisms is the relationship between experimentally well-established learning rules for synapses (such as STDP) on the microscopic level and adaptive changes of the behavior of biological organisms on the macroscopic level.
Background.
Semantic memory refers to our ability to learn and retrieve facts and relationships about concepts without reference to a specific learning episode.
Domain adaptation addresses a common situation that arises when applying machine learning to diverse data.
Markov Random Field (MRF) based exponential family of distribution allows for representing distributions in an intuitive parametric form.
Part-of-speech tagging is a basic problem in natural language processing and a building block for many components.
Two issues facing the use of statistical learning methods in applications are scale and privacy.
Gaussian processes (GP’s) are a widely used method for Bayesian non-linear non-parametric regression and classification [13, 16].
In recent years, the recognition of object categories has become a major focus of computer vision and has shown substantial progress, partly thanks to the adoption of techniques from machine learning and the development of better probabilistic representations [1, 3].
Clustering is one of the most common tools of unsupervised data analysis.
Graphical models are widely used in many areas, including statistical machine learning, computer vision, bioinformatics, and communications.
Suppose that we have labeled data L = {(X 1 , Y1 ), .
Most contemporary SMT systems view parallel data as independent sentence-pairs whether or not they are from the same document-pair.
Protein structure prediction is one of the most important unsolved problems in biology today.
Computational models of visual cortex, and in particular those based on sparse coding, have recently enjoyed much attention.
Neuronal activity, particularly in cerebral cortex, is highly variable.
Hierarchically structured data abound across a wide variety of domains.
Loopy Belief Propagation (LBP) and its variants [6, 9, 13] have been shown empirically to be effective in solving many instances of hard problems in a wide range of fields.
In the field of reinforcement learning, perhaps the most popular way to estimate the future discounted reward of states is the method of temporal difference learning.
The notion of algorithmic stability has been used effectively in the past to derive tight generalization bounds [2–4,6].
A common problem in speech processing is to identify properties of a speech segment such as the language, speaker, topic, or dialect.
Cortical neural networks are characterized by a large degree of recurrent excitatory connectivity, and local inhibitory connections.
Markov jump processes (MJPs) underpin our understanding of many important systems in science and technology.
In this paper we provide provably privacy-preserving versions of belief propagation, Gibbs sampling, and other local message-passing algorithms on large distributed networks.
Visual input to the retina consists of complex light intensity patterns.
Image motion is an important cue used by both biological and artificial visual systems to extract information about the environment.
The max-weight independent set (MWIS) problem is the following: given a graph with positive weights on the nodes, find the heaviest set of mutually non-adjacent nodes.
The goal of transfer learning [1] is to learn new tasks with fewer examples given information gained from solving related tasks, with each task corresponding to the distribution/probability measure generating the samples for that task.
Web search engine evaluation is an expensive process: it requires relevance judgments that indicate the degree of relevance of each document retrieved for each query in a testing set.
The state of the art in real-time face detection has progressed rapidly in recently years.
Major strides have been made in understanding the detailed dynamics of decision making in simple two-alternative forced choice (2AFC) tasks, at both the behavioral and neural levels.
Microarray and other high-throughput measurement devices have been applied to examine specimens such as cancer tissues of biological and/or clinical interest.
Many clustering frameworks have already been proposed, with numerous applications in machine learning, exploratory data analysis, computer vision and speech processing.
Traditionally, meteorological radars, such as the National Weather Service NEXRAD system, are tasked to always scan 360 degrees.
To arrive at a better understanding of human brain function, functional neuroimaging traditionally studies the brain’s responses to controlled stimuli.
Competitive games and sports can benefit from statistical skill ratings for use in matchmaking as well as for providing criteria for the admission to tournaments.
Actor-critic (AC) algorithms are based on the simultaneous online estimation of the parameters of two structures, called the actor and the critic.
Clustering is the problem of discovering “meaningful” groups in given data.
One of the most popular approaches to collaborative filtering is based on low-dimensional factor models.
Many fields have developed heuristic methods for preprocessing data to improve performance.
Although understanding attention is interesting purely from a scientific perspective, there are numerous applications in engineering, marketing and even art that can benefit from the understanding of both attention per se, and the allocation of resources for attention and eye movements.
What use is an episodic memory? It might seem that the possibility of a fulminant recreation of a former experience plays a critical role in enabling us to act appropriately in the world [1].
Current reinforcement-learning (RL) techniques hold great promise for creating a general type of artificial intelligence (AI), specifically autonomous (software) agents that learn difficult tasks with limited feedback (Sutton & Barto, 1998).
Decision making under uncertainty is one of the principal concerns of Artificial Intelligence and Machine Learning.
A computer graphics artist sits down to use a simple renderer to find appropriate surfaces for a typical reflectance model.
We consider a setting where a number of agents need to repeatedly make decisions in the face of uncertainty.
In many real world problems, the proportion of data points in different classes is highly skewed: some classes dominate the data set (majority classes), and the remaining classes may have only a few examples (minority classes).
Energy consumption is a major and growing concern throughout the IT industry as well as for customers and for government regulators concerned with energy and environmental matters.
Consider a training dataset D = {(xi , yi )}, i = 1 .
Subspace-based techniques for face recognition, such as Eigenfaces [1] and Fisherfaces [2], take advantage of the large redundancy present in most images to compute a lowerdimensional representation of their input data and stored patterns, and perform classification in the reduced subspace.
Social tags are a key part of “Web 2.
Stochastic processes defined on graphs offer a compact representation for the Markov structure in a large collection of random variables.
Supervised learning has proven an effective technique for learning a classifier when the quantity of labeled data is large enough to represent a sufficient sample from the true labeling function.
The so-called cocktail party problem refers to a situation where several sound sources are simultaneously active, e.
Graphical models [14, 8] are a powerful tool for probabilistic reasoning over sets of random variables.
Computing an optimal policy for a partially observable Markov decision process (POMDP) is an intractable problem [10, 9].
Neuromorphic analog, VLSI devices [12] try to derive organizational and computational principles from biologically plausible models of neural systems, aiming at providing in the long run an electronic substrate for innovative, bio-inspired computational paradigms.
Over the last decade, mathematical explorations into the statistics of natural scenes have led to the observation that these scenes, as complex and varied as they appear, have an underlying structure that is sparse [1].
Generative systems that model the relationship between face and speech offer a wide range of exciting prospects.
Very accurate pedestrian detectors are an important technical goal; approximately half-a-million pedestrians are killed by cars each year (1997 figures, in [1]).
Substantial progress has been made recently on the problem of fitting high dimensional linear regression models of the form Yi = X iT β + i , for i = 1, .
Recently, we have witnessed a tremendous increase in the sizes of data sets generated and processed by acquisition and computing systems.
Regression data are often modelled as noisy observations of an underlying process.
Measuring dependence of random variables is one of the main concerns of statistical inference.
Suppose that a medical center has decided to use machine learning techniques to induce a diagnostic tool from records of previous patients.
Permutations arise naturally in a variety of real situations such as card games, data association problems, ranking analysis, etc.
Many machine learning methods have computational bottlenecks in the form of nested summations that become intractable for large datasets.
We are interested in learning controllers for high-dimensional, highly non-linear dynamical systems, continuous in state, action, and time.
Bioinformatics provides a rich source for the application of techniques from machine learning.
Survey propagation (SP) is an algorithm for solving k-SAT recently developed in the physics community [1, 2] that exhibits excellent empirical performance on “hard” instances.
Hidden Markov Models (HMMs) assume a generative model for sequential data whereby a sequence of states (or sample path) is drawn from a Markov chain in a hidden experiment.
An efficient optimization algorithm is one that quickly finds a good minimum for a given cost function.
Classical time-series forecasting models, such as ARMA models [6], assume that forecasting is performed at a fixed horizon, which is implicit in the model.
In tasks such as sensor placement for environmental temperature monitoring or experimental design, one has to select among a large set of possible, but expensive, observations.
The psychophysics of visual saliency and attention have been extensively studied during the last decades.
Functional Magnetic Resonance Imaging (fMRI) poses a large-scale, noisy and altogether difficult problem for machine learning algorithms.
Since the pioneering work of Shannon [1], Markov models have not only been taught in elementary information theory classes, but also served as indispensable tools and building blocks for sequence modeling in many fields, including natural language processing, bioinformatics [2], and compression [3].
Speech dereverberation, which may be viewed as a denoising technique, is crucial for many speech related applications, such as hands-free teleconferencing and automatic speech recognition.
In visual scene interpretation the goal is to assign image pixels to one of several semantic classes or scene elements, thus jointly performing segmentation and recognition.
People with severe motor disabilities (spinal cord injury (SCI), amyotrophic lateral sclerosis (ALS), etc.
Recent years have seen widespread application of reproducing kernel Hilbert space (r.
Many applications for autonomous decision making (e.
Many problems in natural language, vision, and computational biology require the joint modeling of many dependent variables.
Computational models for sensory processing are still in their infancy, but one promising approach has been to compare aspects of sensory processing with aspects of machine-learning algorithms crafted to solve the same putative task.
Continuous state-space Markov Decision Processes (MDPs) are notoriously difficult to solve.
There is a growing need to analyze large collections of electronic text.
Very large data sets, such as collections of images, text, and related data, are becoming increasingly common, with examples ranging from digitized collections of books by companies such as Google and Amazon, to large collections of images at Web sites such as Flickr, to the recent Netflix customer recommendation data set.
Boosting provides a ready method for improving existing learning algorithms for classification.
The last few years have seen significant interest in “deep” learning algorithms that learn layered, hierarchical representations of high-dimensional data.
The problem of on-line learning linear-threshold functions from labeled data is one which have spurred a substantial amount of research in Machine Learning.
Utility of discriminant analysis in EEG Brain computer interface (BCI) algorithms [1][2][3][4] aim to decode brain activity, on a singletrial basis, in order to provide a direct control pathway between a user’s intentions and a computer.
Optimal control provides a potentially useful methodology to design nonlinear control laws (policies) u = u(x) which give the appropriate action u for any state x.
An important class of “distances” between multivariate probability distributions P and Q are the AliSilvey or f -divergences [1, 6].
Here we will be concerned with the generative model y = Φx + , (1) where Φ ∈ R is a dictionary of features, x ∈ R is a vector of unknown weights, y is an observation vector, and  is uncorrelated noise distributed as N (; 0, λI).
Structured prediction models commonly involve complex inference problems for which finding exact solutions is intractable [1].
Kernel machines such as the Support Vector Machine are attractive because they can approximate any function or decision boundary arbitrarily well with enough training data.
Collaborative filtering has gained much attention in the machine learning community due to the need for it in webshops such as those of Amazon, Apple and Netflix.
We address the problem of marker-less articulated pose and shape estimation of the human body from images using a detailed parametric body model [3].
Invariances are one of the most powerful forms of prior knowledge in machine learning; they have a long history [9, 1] and their application has been associated with some of the major success stories in pattern recognition.
Let {yi,j : 1 ≤ i < j ≤ n} denote data measured on pairs of a set of n objects or nodes.
Extensive games are a natural model for sequential decision-making in the presence of other decision-makers, particularly in situations of imperfect information, where the decision-makers have differing information about the state of the game.
In many supervised learning methods, overfitting is controlled through the use of regularization penalties for limiting model complexity.
Multi-task learning is an area of active research in machine learning and has received a lot of attention over the past few years.
Large repositories of private or public software source code, such as the open source projects available on the Internet, create considerable new opportunities and challenges for statistical machine learning, information retrieval, and software engineering.
The recognition of objects in a scene often consists of matching representations of image regions to an object model while rejecting background regions.
Value function representations are dominant in algorithms for dynamic programming (DP) and reinforcement learning (RL).
In many scenarios the data of interest consist of relational observations on the edges of networks.
Rankers such as humans, search engines, and classifiers, output full or partial rankings representing preference relations over n items.
Most of the research on Reinforcement Learning (RL) [13] has studied solutions to finite Markov Decision Processes (MDPs).
Let us examine the resource bottlenecks of SVMs in a binary classification setting to explain our proposed solution.
There has been much interest in developing machine learning methods involving complex loss functions beyond those used in regression and classi£cation problems [13].
Data samples may sometimes be characterized in multiple ways, e.
Applications in various domains such as text/web mining and bioinformatics often lead to very highdimensional data.
In regression, we want to explain or to predict a response variable y from a set of explanatory variables x = (x1 , .
The Information Bottleneck (IB) approach [2] allows the investigation of learning algorithms for unsupervised and semi-supervised learning on the basis of clear optimality principles from information theory.
We consider inference based on a countable set of models (sets of probability distributions), focusing on two tasks: model selection and model averaging.
Let P be a distribution on X × Y , where X is an arbitrary set and Y ⊂ R is closed.
In many applications, e.
In recent times, one of the major challenges in kernel methods has been design of kernels on structured data e.
Graphical models such as Markov Random Fields (MRFs) have been successfully applied to a wide variety of fields, from computer vision to computational biology.
Handwriting recognition is traditionally divided into offline and online recognition.
Nearest neighbor (NN) searching is a fundamental operation in machine learning, databases, signal processing, and a variety of other disciplines.
Dynamic Bayesian networks (DBNs) are graphical model representations of discrete-time stochastic processes.
Discrete random fields are a powerful tool to obtain a probabilistic formulation for various applications in Computer Vision and related areas [3].
Change-points detection tasks are pervasive in various fields, ranging from audio [10] to EEG segmentation [5].
In recent years semi-supervised learning, i.
Partially Observable Markov Decision Processes (POMDPs) provide a powerful model for sequential decision making under state uncertainty.
The problem of online convex optimization can be formulated as a repeated game between a player and an adversary.
Languages evolve over time, with words changing in form, meaning, and the ways in which they can be combined into sentences.
The experience of the passage of time, as well as the timing of events and intervals, has long been of interest in psychology, and has more recently attracted attention in neuroscience as well.
Clustering is one of the most basic problems of unsupervised learning with applications in a wide variety of fields.
Recently, there has been renewed interest in the problem of multi-task learning, see [2, 4, 5, 14, 16, 19] and references therein.
Continuous-time diffusion processes, described by stochastic differential equations (SDEs), arise naturally in a range of applications from environmental modelling to mathematical finance [13].
Bayesian networks remain the cornerstone of modern AI.
One of the basic problems in modeling controlled, partially observable, stochastic dynamical systems is representing and tracking state.
Energy minimization is the problem of finding a maximum a posteriori (MAP) configuration in a Markov random field (MRF).
From online auctions to Texas Hold’em, AI is captivated by multi-agent interactions based on competition.
Gaussian processes are a flexible and popular approach to non-parametric modelling.
A central goal of systems neuroscience is to identify the functional relationship between environmental stimuli and a neural response.
The application of system identification techniques to the study of sensory neural systems has a long history.
Boosting methods have been used with great success in many applications like OCR, text classification, natural language processing, drug discovery, and computational biology [13].
Multivariate real-valued data appears in many real-world data sets, and a lot of research is being focused on the development of multivariate real-valued distributions.
Products in today’s e-market are described using both visual and textual information.
Dimensionality reduction is a two-step process: 1) Transform the data so that more information will survive the projection, and 2) project the data into fewer dimensions.
Although a great deal is known about the low-level features computed by the human visual system, determining the information used to make high-level visual classifications is an active area of research.
The quest for efficient machine learning techniques which (a) have favorable generalization capacities, (b) are flexible for adaptation to a specific task, and (c) are cheap to implement is a pervasive theme in literature, see e.
The curse of dimensionality has traditionally been the bane of nonparametric statistics, as reflected for instance in convergence rates that are exponentially slow in dimension.
Few algorithms are better known in machine learning and statistics than expectation-maximization (EM) [5].
One of the main purposes of unsupervised learning is to produce good representations for data, that can be used for detection, recognition, prediction, or visualization.
A core issue in sensory coding is to seek out and model statistical regularities in high-dimensional data.
Survival analysis is a well-established field in medical statistics concerned with analyzing/predicting the time until the occurrence of an event of interest–e.
In the online linear optimization problem (as in Kalai and Vempala [2005]), at each timestep the learner chooses a decision xt from a decision space D ⊂ Rn and incurs a cost Lt · xt , where the loss vector Lt is in Rn .
Semi-supervised learning has attracted an increasing amount of research interest recently [3, 15].
Latent Dirichlet Allocation (LDA) [1] is a language model which clusters co-occurring words into topics.
A limitation of supervised learning is that it requires a set of instance labels which are often difficult or expensive to obtain.
Functional Magnetic Resonance Imaging (fMRI) is a non-invasive imaging technique that can quantify changes in cerebral venous oxygen concentration.
Many problems in machine learning involve sequences of real-valued multivariate observations.
The standard k-armed bandits problem has been well-studied in the literature (Lai & Robbins, 1985; Auer et al.
A central goal of computational neuroscience is to understand how the brain transforms sensory input into spike trains, and considerable effort has focused on the development of statistical models that can describe this transformation.
Internet content providers, such as MSN, Google and Yahoo, all depend on the correct functioning of the wide-area Internet to communicate with their users and provide their services.
Conditional random fields (CRFs) are undirected graphical models that have been successfully applied to the classification of relational and temporal data [1].
Unsupervised learning using mixture models assumes that one latent cause is associated with each data point.
Kernel independence measures have been widely applied in recent machine learning literature, most commonly in independent component analysis (ICA) [2, 11], but also in fitting graphical models [1] and in feature selection [22].
Barn owls, the champions of sound localization, show systematic errors when localizing sounds.
The noisy-or and noisy-and-not conditional probability distributions are frequently studied in cognitive science for modeling causal reasoning [1], [2],[3] and are also used as probabilistic models for artificial intelligence [4].
A common assumption in supervised learning is that training and test samples follow the same distribution.
Learning algorithms need to make assumptions about the problem domain in order to generalise well.
Modelling hierarchical aspects in complex stochastic processes is an important research issue in many application domains ranging from computer vision, text information extraction, computational linguistics to bioinformatics.
We consider the problem of modeling correlated outputs from a single Gaussian process (GP).
Language model (LM) adaptation is crucial to automatic speech recognition (ASR) as it enables higher-level contextual information to be effectively incorporated into a background LM improving recognition performance.
Relational data are observations of relationships between sets of objects and it is therefore natural to consider representing relations1 as arrays of random variables, e.
Recent years have seen a surge of readily available data for complex and varied domains.
Online learning methods for linear classifiers, such as the perceptron and passive-aggressive (PA) algorithms [4], have been thoroughly analyzed and are widely used.
Traditional machine learning relies on the availability of a large amount of labeled data to train a model in the same feature space.
A recent line of research in machine learning has focused on regularization based on block-structured norms.
Markov Decision Processes (MDPs) are a very popular formalism for decision making under uncertainty (Puterman, 1994).
A long standing problem in neuroscience has been collecting enough data to robustly estimate the response function of a neuron.
Offline handwriting recognition is generally observed to be harder than online handwriting recognition [14].
Factor analysis is the task of explaining data by means of a set of latent factors.
Causal relationships are fundamental to science because they enable predictions of the consequences of actions [1].
Neural responses are typically studied by averaging noisy spiking activity across multiple experimental trials to obtain firing rates that vary smoothly over time.
Batch learning (also called statistical learning) and online learning are two different supervised machine-learning frameworks.
A common inference task in graphical models is finding the most likely setting of the values of the variables (the MAP assignment).
Matching shapes in images has many applications, including image retrieval, alignment, and registration [1, 2, 3, 4].
Understanding neural coding is at the heart of theoretical neuroscience.
We study the problem of predicting the labelling of a graph in the online learning framework.
Policy search, also known as policy learning, has become an accepted alternative of value functionbased reinforcement learning [2].
In a Markov decision process (MDP) M with finite state space S and finite action space A, a learner in state s ∈ S needs to choose an action a ∈ A.
A key attribute of visual perception is the ability to extract invariances from visual input.
Probabilistic graphical models gained popularity in the recent decades due to their intuitive representation and because they enable the user to query about the value distribution of variables of interest [19].
Multi-class classification is a problem that arises in many applications of machine learning.
A key idea in reinforcement learning (RL) is to learn an action-value function which can then be used to derive a good control policy [15].
Given a reproducing kernel Hilbert space (RKHS) of a kernel k : X × X → R and training set D := ((x1 , y1 ), .
We present the Gaussian Process Density Sampler (GPDS), a generative model for probability density functions, based on a Gaussian process.
Machine learning builds models of the world using training data from the application domain and prior knowledge about the problem.
Most learning theory models such as the standard PAC learning framework [13] are based on the assumption that sample points are independently and identically distributed (i.
Hierarchical probabilistic models of discrete data have emerged as powerful tool for large-scale text analysis.
We address the problem of unsupervised learning of object classifiers for visually polysemous words.
As an observer moves through the environment, the retinal image changes over time to create multiple complex motion flows, including translational, circular and radial motion.
Real-time extraction of information from composite neural recordings is a significant challenge in neural interfacing.
Modeling sequences is an important problem since there is a vast amount of natural data, such as speech and videos, that is inherently sequential.
In the binary classification problem we are given realizations (x1 , y1 ), .
We consider a Partially Observable Markov Decision Problem (POMDP) (see e.
Visual attention plays an important role in the human visual system.
We address a supervised learning problem over a set of training data {xi , yi }ni=1 where xi ∈ X ⊂ Rp is a p-dimensional input vector and yi is a univariate response.
Principal component analysis (PCA) is a popular change of variables technique used in data compression, predictive modeling, and visualization.
Performing a behavioral action such as picking up a sandwich and bringing it to one’s mouth is a motor control task achieved easily every day by millions of people.
The problem of “holistic scene understanding” encompasses a number of notoriously difficult computer vision tasks.
We consider the question of determining a real-valued function on the space of permutations of n elements with very limited observations.
Animal behaviour is guided by rewards that can be received in different situations and by modulatory factors, such as stress and motivation.
We address the problem of finding taxonomies in data: that is, to cluster the data, and to specify in a systematic way how the clusters relate.
It is often the case that an observed waveform is the superposition of elementary waveforms, taken from a limited set and added with variable latencies and variable but positive amplitudes.
In the earliest days of artificial intelligence, the bottom-most layer of neural networks consisted of randomly connected “associator units” that computed random binary functions of their inputs [1].
Boosting algorithms are efficient procedures that can be used to convert a weak learning algorithm (one which outputs a weak hypothesis that performs only slightly better than random guessing for a binary classification task) into a strong learning algorithm (one which outputs a high-accuracy classifier).
In applications such as medical diagnosis, credit risk screening or information retrieval, one aims at ordering instances under binary label information.
In decision problems where errors incur a severe loss, one may have to build classifiers that abstain from classifying ambiguous examples.
Kernel learning [5, 9, 7] has received a lot of attention in recent studies of machine learning.
The Farwell-Donchin speller [4], also known as the “P300 speller,” is a Brain-Computer Interface which enables users to spell words provided that they can see sufficiently well.
Consider the task of naming the sum of two numbers, e.
Random Fields (RFs) have played an increasingly important role in the fields of image denoising, texture discrimination, image segmentation and many other important problems in computer vision.
In this paper we consider linear regression problems with least-square error.
In the last two decades, kernel methods have been a prolific theoretical and algorithmic machine learning framework.
Researchers in biological vision have long hypothesized that image contours (ordered sets of edge pixels, or contour points) are a compact yet descriptive representation of object shape.
Matching pairs of objects is a fundamental operation of unsupervised learning.
Learning to rank is aimed at constructing a model for ordering objects by means of machine learning.
Gaussian processes (GPs) are used for Bayesian non-parametric estimation of unobserved or latent functions.
The Shannon/Nyquist sampling theorem tells us that in order to preserve information when uniformly sampling a signal we must sample at least two times faster than its bandwidth.
Approximate dynamic programming methods often offer surprisingly good performance in practical problems modeled as Markov Decision Processes (MDP) [6, 2].
Current machine learning is frequently concerned with the estimation of functions with multivariate output.
Humans daily perform sequential decision-making under uncertainty to choose products, services, careers, and jobs; and to mate and survive as species.
Understanding the meaning of a sentence involves both syntactic and semantic analysis.
Ranking is the central problem for many information retrieval applications such as web search, collaborative filtering and document retrieval [8].
We study a problem setting of transfer learning in which classifiers for multiple tasks have to be learned from biased samples.
Linear prediction is the cornerstone of an extensive number of machine learning algorithms, including SVM’s, logistic and linear regression, the lasso, boosting, etc.
Understanding how the dynamics of a neural network is shaped by the network structure, and consequently facilitates the functions implemented by the neural system, is at the core of using mathematical models to elucidate brain functions [1].
Building models that make good predictions about the world can be a complicated task.
Humans demonstrate a variety and versatility of movements far beyond the reach of current anthropomorphic robots.
Constructing models of biological systems, e.
Neural activities are highly non-stationary and vary from time to time according to stimuli and internal state changes.
Column generation (CG) [3] is a technique widely used in linear programming (LP) for solving large-sized problems.
Brain-Computer Interfaces (BCIs) are devices that enable a subject to communicate without utilizing the peripheral nervous system, i.
Most accounts of the processes underlying human learning, decision-making, and perception assume that stimuli have fixed sets of features.
It was some fifty years after James (1950)’s famously poetic description of our capacities for attention that more analytically-directed experiments began, based originally on dichotic listening Cherry (1953).
Designing markets to achieve certain goals is gaining renewed importance with the prevalence of many novel markets, ranging from prediction markets [13] to markets for e-services [11].
A major goal of current information retrieval research is to develop algorithms that can improve retrieval effectiveness by inferring a more complete picture of the user’s information need, beyond that provided by the user’s query text.
Ten years ago, an eight-year lasting collaborative effort resulted in the first completely sequenced genome of a multi-cellular organism, the free-living nematode Caenorhabditis elegans.
We consider the problem of (approximately) minimizing a stochastic objective F (w) = Eθ [f (w; θ)] (1) where the optimization is with respect to w ∈ W, based on an i.
A recent trend in statistical machine translation (SMT) has been the use of synchronous grammar based formalisms, permitting polynomial algorithms for exploring exponential forests of translation options.
The fact that most visual learning problems deal with high dimensional data has made dimensionality reduction an inherent part of the current research.
Model-free policy gradient algorithms allow for the optimization of control policies on systems which are impractical to model effectively, whether due to cost, complexity or uncertainty in the very structure and dynamics of the system (Kohl & Stone, 2004; Tedrake et al.
The work we report here falls under the general heading of state estimation, i.
The area of high-dimensional statistical inference is concerned with the behavior of models and algorithms in which the dimension p is comparable to, or possibly even larger than the sample size n.
Recognizing human actions from videos is a task of obvious scientific and practical importance.
Evolution is likely to favor those biological organisms which are able to maximize the chance of achieving correct decisions in response to multiple unreliable sources of evidence.
The problem of modeling relational information among objects, such as pairwise relations represented as graphs, arises in a number of settings in machine learning.
The binary classification of examples x is usually performed with recourse to the mapping ŷ = sign[f (x)], where f is a function from a pre-defined class F, and ŷ the predicted class label.
Structured estimation [18, 20] and related techniques has proven very successful in many areas ranging from collaborative filtering to optimal path planning, sequence alignment, graph matching and named entity tagging.
This paper presents an algorithm for solving the following class of online resource allocation problems.
Mammalian brains consist of billions of neurons, each capable of independent electrical activity.
The goal of most machine learning problems is to generalize from a limited number of training examples.
Trust tasks such as the Dictator, Ultimatum and Investor-Trustee games provide an empirical basis for investigating social cooperation and reciprocity [11].
Signals may be manipulated, transmitted or stored more efficiently if they are transformed to a representation in which there is no statistical redundancy between the individual components.
The stochastic approximation method supplies the theoretical underpinnings behind many well-studied algorithms in machine learning, notably policy gradient and temporal differences for reinforcement learning, inference for tracking and filtering, on-line learning [1, 17, 19], regret minimization in repeated games, and parameter estimation in probabilistic graphical models, including expectation maximization (EM) and the contrastive divergences algorithm.
Online advertisements (ads) are a rapidly growing source of income for many Internet content providers.
In many domains, researchers are interested in predicting the effects of interventions, or manipulating variables, on other observed variables.
Regularization has emerged as a dominant theme in machine learning and statistics, providing an intuitive and principled tool for learning from high-dimensional data.
Active learning is a paradigm in which the learner has the ability to sequentially select examples for labeling.
The expectation propagation (EP) message passing algorithm is often considered as the method of choice for approximate Bayesian inference when both good accuracy and computational efficiency are required [5].
Actor-critic (AC) algorithms [22] were probably among the first algorithmic approaches to reinforcement learning (RL).
Multi-armed bandit problems describe typical situations where learning and optimization should be balanced in order to achieve good cumulative performances.
Time series classification arises in diverse application.
Problem statement This paper is motivated by the problem fitting of binary distributions to experimental data.
Kullback-Leibler divergence, mutual information and differential entropy are central to information theory [5].
Recent studies have shown that mapping random variables into a suitable reproducing kernel Hilbert space (RKHS) gives a powerful and straightforward method of dealing with higher-order statistics of the variables.
Inferring structured predictions based on high-dimensional, often multi-modal and hybrid covariates remains a central problem in data mining (e.
Sparse approximation is a key technique developed in engineering and the sciences which approximates an input signal, X, in terms of a “sparse” combination of fixed bases B.
When modeling discrete time series data, the hidden Markov model [1] (HMM) is one of the most widely used and successful tools.
It is accepted that neural activity self-regulates to prevent neural circuits from becoming hyper- or hypoactive by means of homeostatic processes [14].
Consider a set of input vectors x1 , .
For many important problems in machine learning, we have a limited amount of labeled training data and a very high-dimensional feature space.
A typical mixture model is composed of a number of separately parameterized density models each of which has two important properties: 1.
A natural scene contains several multi-modal sensory cues to the true underlying values of its physical properties.
One common error human subjects make in statistical inference is that they detect hidden patterns and causes in what are genuinely random data.
Pattern clustering is a classic topic in pattern recognition and machine learning.
Probabilistic topic models provide a suite of algorithms for finding low dimensional structure in a corpus of documents.
Matching pursuit refers to a family of algorithms that generate a set of bases for learning in a greedy fashion.
The problem we address in this paper is, broadly speaking, function approximation.
With the advent of the Internet, it is now possible to use huge training sets to address challenging tasks in machine learning.
In the context of ranking, several performance measures may be considered.
The need to partition a sequence of observations into several homogeneous segments arises in many applications, ranging from speaker segmentation to pop song indexation.
In many real-world classification problems, it is not equally easy or affordable to verify membership in different classes.
The human immunodeficiency virus (HIV) has one of the highest levels of genetic variability yet observed in nature.
Despite of its widespread success in regression problems, Gaussian process (GP) has two limitations.
Algorithms for automatically discovering hierarchical structure from data play an important role in machine learning.
Most popular optimization algorithms, like the Levenberg-Marquardt algorithm (LMA) use simple “controllers” that modulate the behavior of the optimization algorithm based on the state of the optimization process.
Regularized Least Squares (RLS) algorithms have been drawing people’s attention since they were proposed due to their ability to avoid over-fitting problems and to express solutions as kernel expansions in terms of the training data [4, 9, 12, 13].
The web has become the central distribution channel for information from traditional sources such as news outlets as well as rapidly growing user-generated content.
Privacy-preserving machine learning is an emerging problem, due in part to the increased reliance on the internet for day-to-day tasks such as banking, shopping, and social networking.
Barn owl is a nocturnal predator with strong able auditory and visual localization system.
A critical problem in machine learning is that of scaling: Algorithms should be effective computationally and statistically as various dimensions of a problem are scaled.
Metal ions play important roles in protein function and structure and metalloproteins are involved in a number of diseases for which medicine is still seeking effective treatment, including cancer, Parkinson, dementia, and AIDS [10].
In the context of importance sampling, the ratio of two probability density functions is called the importance.
In many real-world prediction problems, the response variable of interest is clustered hierarchically.
An important step toward understanding the neural basis of vision is to develop computational models that describe how complex visual stimuli are mapping onto evoked neuronal responses.
Phylogenetic analysis plays a significant role in modern biological applications such as ancestral sequence reconstruction and multiple sequence alignment [1, 2, 3].
The problem of visualizing high dimensional data often arises in the context of exploratory data analysis.
In his highly influential paper, [1], Kleinberg advocates the development of a theory of clustering that will be “independent of any particular algorithm, objective function, or generative data model.
Statistical dependence measures have been proposed as a unifying framework to address many machine learning problems.
Ensemble methods [5] return a weighted vote of baseline classifiers.
Principal component analysis (PCA) is widely used for data pre-processing, data compression and dimensionality reduction.
Mechanistic system modeling employing nonlinear ordinary or delay differential equations 1 (ODEs or DDEs) is oftentimes hampered by incomplete knowledge of the system structure or the specific parameter values defining the observed dynamics [16].
Synapses are the primary locations in neural systems where information is processed and transmitted.
Most of the facts that we know about the world are not learned through first-hand experience, but are the result of information being passed from one person to another.
Kernel-based methods have been highly popular in statistical learning, starting with Parzen windows, kernel regression, locally weighted regression and radial basis function networks, and leading to newer formulations such as Reproducing Kernel Hilbert Spaces, Support Vector Machines, and Gaussian process regression [1].
The broad problem we consider is how to equip artificial agents with the ability to form useful high-level behaviors, or skills, from available primitives.
Dimensionality reduction is a common and often necessary step in most machine learning applications and high-dimensional data analyses.
Many problems in statistical pattern recognition and analysis require the classification and analysis of high dimensional data vectors.
Discriminative questions such as “What is it?” (categorization) and “Where is it?” (detection) are central to machine vision and have received much attention in recent years.
Principal component analysis (PCA) has been extensively used for data analysis and processing.
Spectral graph-theoretic methods have been used widely in unsupervised and semi-supervised learning recently.
Manifold learning (ML) methods have attracted substantial attention due to their demonstrated potential.
Regularization using the `1 -norm has attracted a lot of interest in the statistics [1], signal processing [2], and machine learning communities.
Images of natural environments contain a rich diversity of spatial structure at both coarse and fine scales.
Graphical models, such as Bayesian networks and factor graphs [1], are widely used to represent and visualise fixed dependency relationships between random variables.
Magnetoencephalography (MEG) and related electroencephalography (EEG) use an array of sensors to take electromagnetic field (or voltage potential) measurements from on or near the scalp surface with excellent temporal resolution.
Sparse signal models have been used successfully in a variety of applications including waveletbased image processing and pattern recognition.
Unsupervised learning of structured variables in data is a difficult problem that has received considerable recent attention.
Economists and computer scientists are often concerned with inferring people’s preferences from their choices, developing econometric methods (e.
Spike sorting (see [1] and [2] for review and methodological background) is the name given to the problem of grouping action potentials by source neuron.
Linear dynamical systems (LDSs) are useful in describing dynamical phenomena as diverse as human motion [9], financial time-series [4], maneuvering targets [6, 10], and the dance of honey bees [8].
In 2001, Jaeger [1] and Maass [2] independently introduced the idea of using a fixed, randomly connected recurrent neural network of simple units as a set of basis filters (operating at the edge-ofstability where the system has fading memory).
The area of high-dimensional statistics deals with estimation in the “large p, small n” setting, where p and n correspond, respectively, to the dimensionality of the data and the sample size.
The importance of dimension reduction for predictive modeling and visualization has a long and central role in statistical graphics and computation In the modern context of high-dimensional data analysis this perspective posits that the functional dependence between a response variable y and a large set of explanatory variables x ∈ Rp is driven by a low dimensional subspace of the p variables.
The problem of bipartite graph inference is to predict the presence or absence of edges between heterogeneous objects known to form the vertices of the bipartite graph, based on the observation about the heterogeneous objects.
Many learning problems can be naturally formulated in terms of multi-category classification or multi-task regression.
Reinforcement Learning [1] addresses the problem of how autonomous agents can improve their behavior using their experience.
A very active supervised learning trend has been flourishing over the last decade: it studies functions known as surrogates — upperbounds of the empirical risk, generally with particular convexity properties —, whose minimization remarkably impacts on empirical / true risks minimization [3, 4, 10].
Structure learning of dynamic Bayesian networks allows conditional dependencies to be identified in time-series data with the assumption that the data are generated by a distribution that does not change with time (i.
The existence of nested beliefs is one of the defining characteristics of a multi-agent world.
Bandit problems arise in many settings, including clinical trials, scheduling, on-line parameter tuning of algorithms or optimization of controllers based on simulations.
Undirected graphical models are a popular tool in machine learning; they represent real-valued energy functions of the form X X 0 E 0 (y) := Ei0 (yi ) + Eij (yi , yj ) , (1) i∈V (i,j)∈E where the terms in the first sum range over the nodes V = {1, 2, .
In Principal Components Analysis (PCA) we seek to reduce the dimensionality of a D-dimensional data vector to a smaller K-dimensional vector, which represents an embedding of the data in a lower dimensional space.
A number of recent techniques address the problem of metric learning, in which a distance function between data objects is learned based on given (or inferred) similarity constraints between examples [4, 7, 11, 16, 5, 15].
Visual category recognition is a vital thread in computer vision research.
Learning algorithms that can perform in a distributed asynchronous manner are of interest for several different reasons.
Odor sensors are diverse in terms of their sensitivity to odor identity and concentrations.
Principal Component Analysis (PCA) [9] is one of the primary statistical techniques for feature extraction and data modeling.
The Problem.
A common assumption in theoretical models of learning such as the standard PAC model [16], as well as in the design of learning algorithms, is that training instances are drawn according to the same distribution as the unseen test examples.
Latent variable models capture underlying structure in data by explaining observations as part of a more complex, partially observed system.
Message passing algorithms, in particular Belief Propagation (BP), have been very successful in efficiently computing interesting properties of succinctly represented large spaces, such as joint probability distributions.
It is known that visual cells (visual features) selectively respond to imagery patterns in perception.
Online regret minimizing algorithms provide some of the most successful algorithms for many machine learning problems, both in terms of the speed of optimization and the quality of generalization.
Analysis of large scale sequential data has become an important task in machine learning and data mining, inspired by applications such as biological sequence analysis, text and audio mining.
The Singular Value Decomposition (SVD) is a fundamental linear algebraic operation whose abundant useful properties have placed it at the computational center of many methods in machine learning and related fields.
Sparse and overcomplete image models were first introduced in [1] for modeling the spatial receptive fields of simple cells in the human visual system.
Brain computer interfaces (BCI) have seen a rapid development towards faster and more userfriendly systems for thought-based control of devices such as video games, wheel chairs, robotic devices etc.
Markov Decision Processes (MDPs) have been extensively studied in the context of planning and decision-making.
Pattern classification approaches to analyzing functional neuroimaging data have become increasingly popular [12] [3] [4].
In machine learning problems the data often live in a vector space, typically a Euclidean space.
Language and image understanding are two major tasks in artificial intelligence.
Consider the setting where we are given large amounts of unlabeled data together with dual supervision in the form of a few labeled examples as well as a few labeled features, and the goal is to estimate an unknown classification function.
Discrete random fields are a powerful tool for formulating several problems in Computer Vision such as stereo reconstruction, segmentation, image stitching and image denoising [22].
Precise models of technical systems can be crucial in technical applications.
In many areas of machine learning such as clustering, dimensionality reduction, or semi-supervised learning, neighborhood graphs are used to model local relationships between data points and to build global structure from local information.
Clustering is considered as one of the most fundamental unsupervised learning problems.
One of the major tasks of primary visual cortex (V1) is the computation of a representation of orientation in the visual field.
Statistical language modelling is concerned with building probabilistic models of word sequences.
Since the seminal contribution of [14], so-called ROC curves (ROC standing for Receiving Operator Characteristic) have been extensively used in a wide variety of applications (anomaly detection in signal analysis, medical diagnosis, search engines, credit-risk screening) as a visual tool for evaluating the performance of a test statistic regarding its capacity of discrimination between two populations, see [8].
Cell assemblies exhibiting episodes of recurrent coherent activity have been observed in several brain regions including the striatum[1] and hippocampus CA3[2], but how such correlated activity emerges in neural microcircuits is not well understood.
Exploration, in reinforcement learning, refers to the strategy an agent uses to discover new information about the environment.
Nonrigid structure from motion is the process of recovering the time varying 3D coordinates of points on a deforming object from their 2D locations in an image sequence.
Independent component analysis (ICA) is the problem of recovering latent random vector from observations of unknown linear functions of that vector.
It is sometimes possible to find a way of mapping objects in a “data” domain into objects in a “target” domain so that operations in the data domain can be modelled by operations in the target domain.
In machine learning, manifold structure has so far been mainly used in manifold learning [1], to enhance learning methods especially in semi-supervised learning.
We are concerned with machine learning over large datasets.
High-throughput functional data is emerging as an indispensible resource for generating a complete picture of genome-wide gene and protein function.
The inverse dynamics problem for a robotic manipulator is to compute the torques τ needed at the joints to drive it along a given trajectory, i.
An ultimate goal of neuroscience is to elucidate how information is encoded and decoded by neural activities.
Ranking is an important task in information sciences.
Cognitive control can be characterized as the ability to guide behavior according to current goals and plans.
When exposed to a novel visuomotor environment, for instance while wearing prism goggles, subjects initially exhibit large directional errors during reaching movements but are able to rapidly adapt their movement patterns and approach baseline performance levels within around 30-50 reach trials.
In the standard online learning protocol for binary classification the learner receives a sequence of instances generated by an unknown source.
Image annotation, or image labeling, in which the task is to label each pixel or region of an image with a class label, is becoming an increasingly popular problem in the machine learning and machine vision communities [7, 14].
Message passing algorithms such as Belief Propagation (BP) [1] exploit factorization to perform inference.
The Sample compression framework [Littlestone and Warmuth, 1986, Floyd and Warmuth, 1995] has resulted in an important class of learning algorithms known as sample compression algorithms.
Is accurate classification possible in the complete absence of hand-labeled data? A Priori, the answer would seem to be no, unless the learner has knowledge of some additional problem structure.
In recent years, online regret minimizing algorithms have become widely used and empirically successful algorithms for many machine learning problems.
Labeled data can be expensive, time-consuming and difficult to obtain in many applications.
The availability of large amounts of unlabeled data such as text on the Internet is a strong motivation for research in semi-supervised learning [4].
Much research on how people acquire knowledge focuses on discrete structures, such as the nature of categories or the existence of causal relationships.
Multivariate binary data arise in a variety of fields, such as biostatistics [1], econometrics [2] or artificial intelligence [3].
The goal of this study is to prove that the most influential form of reinforcement learning (RL) [1], which relies on the temporal difference (TD) learning rule [2], is equivalent to correlation based learning (Hebb, CL) which is convergent over wide parameter ranges when using a local third factor, as a gating signal, together with a differential Hebbian emulation of CL.
Gaussian summations occur in many machine learning algorithms, including kernel density estimation [1], Gaussian process regression [2], fast particle smoothing [3], and kernel based machine learning techniques that need to solve a linear system with a similarity matrix [4].
Nearly every sentence occurring in natural language can, given appropriate contexts, be interpreted in more than one way.
Consider a set of input vectors x1 , .
Semi-supervised Learning (SSL) takes advantage of a large amount of unlabeled data to enhance classification accuracy.
Classification methods which rely upon the graph Laplacian (see [3, 20, 13] and references therein), have proven to be useful for semi-supervised learning.
Graphical models are used in many different fields.
Magnetic resonance imaging (MRI) [7, 2] is a key diagnostic technique in healthcare nowadays, and of central importance for experimental research of the brain.
It is a long standing hypothesis that sensory systems are adapted to the statistics of their inputs.
An important problem in systems neuroscience is to develop flexible, statistically accurate models of neural responses.
Visual recognition remains a challenging task for machines.
Learning algorithms often assume a data matrix A ∈ Rn×D with n observations and D attributes and operate on the data matrix A through pairwise distances.
Consider the following regression model: Yi = X′i β(ti ) + ǫi , i = 1, .
The linear model is a mainstay of statistical inference.
Comparing speakers in speech signals is a common operation in many applications including forensic speaker recognition, speaker clustering, and speaker verification.
Many dynamic optimization problems can be cast as Markov decision problems (MDPs) and solved, in principle, via dynamic programming.
Pairwise distance computations are fundamental to many important computations in machine learning and are some of the most expensive for large datasets.
In reinforcement learning, Bellman’s dynamic programming equation is typically viewed as a method for determining the value function — the maximum achievable utility at each state.
Neurons in the brain communicate through the firing of action potentials.
Online advertising has become the cornerstone of many sustainable business models in today’s Internet, including search engines (e.
Segmenting semantic objects, and more broadly image parsing, is a fundamentally challenging problem.
Learning with examples having multiple labels is an important problem in machine learning and data mining.
Using a low-dimensional embedding to summarize a high-dimensional data set has been widely used for exploring the structure in the data.
Online learning algorithms are fast, simple, make few statistical assumptions, and perform well in a wide variety of settings.
The minimum description length (MDL) principle recommends to use, among competing models, the one that allows to compress the data+model most [Grü07].
Before we begin, we establish notation for this paper.
Extensive games are a powerful model of sequential decision-making with imperfect information, subsuming finite-horizon MDPs, finite-horizon POMDPs, and perfect information games.
There has been significant recent interest in sparse signal expansions in several settings.
Clustering is the problem of organizing a set of objects into groups, or clusters, in a way as to have similar objects grouped together and dissimilar ones assigned to different groups, according to some similarity measure.
The problem of feature selection has a long history due to its significance in a wide range of important problems, from early ones like pattern recognition to recent ones such as text categorization, gene expression analysis and others.
Object detection is one of the great challenges of computer vision, having received continuous attention since the birth of the field.
The problem of finding and exploiting low-dimensional structure in high-dimensional data is taking on increasing importance in image, audio and video processing, web search, and bioinformatics, where datasets now routinely lie in thousand- or even million-dimensional observation spaces.
Kernel-based methods are now well-established tools for supervised learning, allowing to perform various tasks, such as regression or binary classification, with linear and non-linear predictors [1, 2].
For many machine learning algorithms, the choice of a distance metric has a direct impact on their success.
The statistical problem of testing homogeneity of two samples arises in a wide variety of applications, ranging from bioinformatics to psychometrics through database attribute matching for instance.
The hierarchical Dirichlet process (HDP) [1] has emerged as a powerful model for the unsupervised analysis of text.
The Indian Buffet Process (IBP) [5] is a Bayesian nonparametric approach that models objects as arising from an unbounded number of latent features.
Much recent research in reinforcement learning (RL) has focused on hierarchical RL methods [1] and in particular the options framework [2], which adds to the RL framework principled methods for planning and learning using high level-skills (called options).
Temporal alignment of time series has been an active research topic in many scientific disciplines such as bioinformatics, text analysis, computer graphics, and computer vision.
Multiclass classification is the task of assigning labels from a predefined label-set to instances in a given domain.
In this paper the problem of Multiple Kernel Learning (MKL) is studied where the given kernels are assumed to be grouped into distinct components and each component is crucial for the learning task in hand.
In this paper, we consider the problem of decision-theoretic online learning (DTOL), proposed by Freund and Schapire [1].
The relative merits of different population coding schemes have mostly been studied (e.
Modern learning problems in computer vision, natural language processing, computational biology, and other areas are often based on large data sets of tens of thousands to millions of training instances.
An effective distance function plays an important role in many machine learning and data mining techniques.
A huge amount of images with accompanying text captions are available on the Internet.
Humans make rapid, consistent intuitive inferences about the goals of agents from the most impoverished of visual stimuli.
Given only a few examples of a particular category, people have strong expectations as to which new examples also belong to that same category.
This paper deals with the problem of single-channel signal separation – separating out signals from individual sources in a mixed recording.
The complementary nature of discriminative and generative approaches to machine learning [20] has motivated lots of research on the ways in which these can be combined [5, 12, 15, 18, 9, 24, 27].
Recent work has led to the ability to perform space efficient counting over large vocabularies [Talbot, 2009; Van Durme and Lall, 2009].
When applying off-the-shelf machine learning algorithms to data with spatial dimensions (images, geo-spatial data, fMRI, etc) a central question arises: how to incorporate prior information on the spatial characteristics of the data? For example, if we feed a boosting or SVM algorithm with individual image voxels as features, the voxel spatial information is ignored.
Completing a matrix from a few given entries is a fundamental problem with many applications in machine learning, statistics, and compressed sensing.
Learning from massive datasets, such as text, images, and high throughput biological data, has applications in various scientific and engineering disciplines.
Learning a pairwise similarity measure from data is a fundamental task in machine learning.
Decentralized and partially observable stochastic decision and planning problems are very common, comprising anything from strategic games of chance to robotic space exploration.
Learning to rank has become an important research topic in many fields, such as machine learning and information retrieval.
Many problems in machine learning and statistics involve the estimation of parameters from finite data.
Many companies that provide large scale online services, such as banking services, e-mail services, or search engines, use large server farms, often containing tens of thousands of computers in order to support fast computation with low latency.
In many fundamental problems in machine learning, such as feature selection and active learning, we try to select a subset of a finite set so that some utility of the subset is maximized.
The image basis is a key concept for understanding neural representation of visual images.
The process of training classifiers with small amounts of labeled data and relatively large amounts of unlabeled data is known as semi-supervised learning (SSL).
Gigantic quantities of visual imagery are present on the web and in off-line databases.
In this paper, we address the problem of nearest-neighbor (NN) search in large datasets of high dimensionality.
There has recently been much interest in developing statistical models for analyzing and organizing images, based on image features and, when available, auxiliary information, such as words (e.
The development of non-invasive brain computer interface (BCI) using the electroencephalogram (EEG) signal has become a very hot research topic in the BCI community [1].
In a sequence labeling problem, we are given an input sequence x and need to label each component of x with its class to produce a label sequence y.
Anomaly detection involves detecting statistically significant deviations of test data from nominal distribution.
When used to guide decision-making, linear regression analysis typically treats estimation of regression coefficients separately from their use to make decisions.
In today’s software projects, two types of source code are developed: product and test code.
Consider the problem of learning a nonlinear function f (x) on a high dimensional space x ∈ Rd .
Consider a seller who must pick from a universe of N products, N , a subset M of products to offer to his customers.
Multi-body motion segmentation concerns the separation of motions arising from multiple moving objects in a video sequence.
The last years in BCI research have seen drastically reduced training and calibration times due to the use of machine learning and adaptive signal processing techniques (see [9] and references therein) and novel dry electrodes [18].
The task of generating a topological map from video data has gained prominence in recent years.
Recently there has been great interest in social annotations, also called collaborative tagging or folksonomy, created by users freely annotating objects such as web pages [7], photographs [9], blog posts [23], videos [26], music [19], and scientific papers [5].
Estimating a vector x ∈ Rn from measurements of the form y = Φx + w, (1) where Φ ∈ Rm×n represents a known measurement matrix and w ∈ Rm represents measurement errors or noise, is a generic problem that arises in a range of circumstances.
Transduction relies on the fundamental assumption that training and test data should exhibit similar behavior.
Consider a real-world problem scenario where the challenge is to diagnose a patient who presents with several salient symptoms by performing inference with a probabilistic diagnostic model.
Many practical scenarios call for robots or agents which can learn a visual model on the fly given only a spoken or textual definition of an object category.
In real applications, the model of the problem at hand inevitably embodies some form of uncertainty: the parameters of the model are usually (roughly) estimated from data, which themselves can be uncertain due to various kinds of noises.
Factor analysis and principal component analysis (PCA) are widely used linear techniques for finding dominant patterns in multivariate datasets.
Boosting procedures attempt to improve the accuracy of general machine learning algorithms, through repeated executions on reweighted data.
A long-standing goal of unsupervised learning on images is to be able to learn the shape and form of objects from unlabelled scenes.
In this paper we develop an approach to learn stochastic 3D geometric models of object categories from single view images.
In the past, most articles investigating statistical properties of learning algorithms assumed that the observed data was generated in an i.
For better or worse, human memory is not perfect, causing us to forget.
A fundamental challenge facing reinforcement learning (RL) algorithms is to maintain a proper balance between exploration and exploitation.
Sliced inverse regression (SIR) [7] was proposed for sufficient dimension reduction.
Kernel methods [5, 24] such as Support Vector Machines (SVM) have recently attracted much attention due to their good generalization performance and appealing optimization approaches.
Picture an intense match point at the Wimbledon tennis championship, Nadal on the defense from Federer’s powerful shots.
Machine learning algorithms have been successfully applied to learning classifiers in many domains such as computer vision, fraud detection, and brain image analysis.
It has been long recognized that extracting an informative set of application-specific features from the raw data is essential in practical applications of machine learning, and often contributes even more to the success of learning than the choice of a particular classifier.
Clustering is ubiquitous in science and engineering, with numerous and diverse application domains, ranging from bioinformatics and medicine to the social sciences and the web [15].
Sparse factor models have proven to be a very versatile tool for detailed modeling and interpretation of multivariate data, for example in the context of gene expression data analysis [1, 2].
In recent years there has been an increasing interest in learning with output which differs from the case of standard classification and regression.
In the present paper, we are concerned with a class of non-linear inverse problems appearing in the structure and motion problem of multiview geometry.
Distance metric learning is a fundamental problem in machine learning and pattern recognition.
Many problems in cognitive psychology arise from questions of capacity.
Sketches are everywhere.
Bag-of-words document representations are widely used in text, image, and video processing [14, 1].
This paper proposes an unsupervised approach to the detection of regions of interest (ROIs) from a Web-sized dataset (Fig.
Graph matching and MAP inference are essential problems in computer vision and machine learning that are frequently formulated as integer quadratic programs, where obtaining an exact solution is computationally intractable.
As commercial, social, and scientific data sources continue to grow at an unprecedented rate, it is increasingly important that algorithms to process and analyze this data operate in online, or one-pass streaming settings.
Latent variable generative models are widely used in inducing meaningful representations from unlabeled data.
There has been a steady increase in the performance of object category detection as measured by the annual PASCAL VOC challenges [3].
It is widely believed that one of the main principles underlying functional organization of the early visual system is the reduction of the redundancy of relayed input from the retina.
Modern computational models of verbal memory assume that the recall of items is shaped by their semantic representations.
Supervised learning has emerged as a serious contender in the field of image segmentation, ever since the creation of training sets of images with “ground truth” segmentations provided by humans, such as the Berkeley Segmentation Dataset [15].
It is a commonly accepted hypothesis that adaptation of behavior results from changes in synaptic efficacies in the nervous system.
Gaussian processes (GPs) (see e.
Ranking text documents given a text-based query is one of the key tasks in information retrieval.
From information retrieval to recommender systems, from bioinformatics to financial market analysis, the amount of data available to researchers has exploded in recent years.
A commonly used observation model in the Gaussian process (GP) regression is the Normal distribution.
The accessibility of large quantities of off-line discrete-time dynamic data—state-action sequences drawn from real-world domains—represents an untapped opportunity for widespread adoption of reinforcement learning.
Regression is a foundational problem in machine learning and statistics.
Visual area V1 is the first area of cortex devoted to handling visual input in the human visual system (Dayan & Abbott, 2001).
Consider the problem of repeatedly choosing advertisements to display in sponsored search to maximize our revenue.
We consider the problem of estimating the value function of a given stationary policy of a Markov Decision Process (MDP).
The concave-convex procedure (CCCP) [30] is a majorization-minimization algorithm [15] that is popularly used to solve d.
Natural image statistics are a powerful tool in image processing, computer vision and computational photography.
Semi-supervised learning has attracted a lot of research focus in recently years.
Learning probabilistic graphical models from data serves two primary purposes: (i) finding compact representations of probability distributions to make inference efficient and (ii) modeling unknown data generating mechanisms and predicting causal relationships.
Factor graphs are a widely used representation for modeling complex dependencies amongst hidden variables in structured prediction problems.
Algorithms for fast indexing and search have become important for a variety of problems, particularly in the domains of computer vision, text mining, and web databases.
Nonparametric Bayesian models are now widely used in machine learning.
Compressive sensing (CS), also known as compressive sampling, has recently received increasing attention in many areas of science and engineering [3].
During haptic exploration tasks, forces are applied to the skin of the hand, and in particular to the fingertips, which constitute the most sensitive parts of the hand and are prominently involved in object manipulation/recognition tasks.
Recent technical advances in systems neuroscience allow us to monitor the activity of increasingly large neural ensembles simultaneously (e.
There are two main reasons to learn the structure of graphical models: knowledge discovery (to interpret the learned topology) and density estimation (to compute log-likelihoods and make predictions).
Household scenes commonly contain transparent objects such as glasses and bottles made of various materials (like those in Fig.
We address the problem of variable selection for regression, where a natural grouping structure exists within the explanatory variables, and the goal is to select the correct group of variables, rather than the individual variables.
Recent work in machine learning has highlighted the circumstances that appear to favor deep architectures, such as multilayer neural nets, over shallow architectures, such as support vector machines (SVMs) [1].
To understand how the brain represents and processes information we must account for two complementary properties: information is represented in a distributed fashion, and brain regions are strongly interconnected.
Finding the linear least squares fit to data is a well-known problem, with applications in almost every field of science.
In multitask learning, one is interested in learning a set of related models for predicting multiple (possibly) related outputs (i.
Kernel methods provide a principled framework for nonlinear data modeling, where the inference in the input space can be transferred intactly to any feature space by simply treating the associated kernel as inner products, or more generally, as nonlinear mappings on the data (Schölkopf & Smola, 2002).
Ranking is the central problem in many applications including information retrieval (IR).
Pearl’s belief propagation [1] provides an efficient method for exact computation in the inference with probabilistic models associated to trees.
Sequence labeling is a ubiquitous problem arising in many areas, including natural language processing [1], bioinformatics [2, 3, 4] and computer vision [5].
It has been an extensively sought-after goal to learn an appropriate distance metric in image classification and retrieval problems using simple and efficient algorithms [1–5].
Extensive games provide a general model for describing the interactions of multiple agents within an environment.
The Maximum-Weight Bipartite Matching Problem (henceforth ‘matching problem’) is a fundamental problem in combinatorial optimization [22].
Risk minimization is at the heart of many machine learning algorithms.
Living creatures occupy an environment full of uncertainty due to noisy sensory inputs, incomplete observations, and hidden variables.
What should a learning algorithm optimize on the training data in order to give classifiers having the smallest possible true risk? Many different specifications of what should be optimized on the training data have been provided by using different inductive principles.
On-line learning has been studied for decades.
Graph kernels have recently evolved into a branch of kernel machines that reaches deep into graph mining.
We consider a parameterized hidden Markov model (HMM) deﬁned on continuous state and observation spaces.
Bayesian nonparametrics have recently garnered much attention in the machine learning and statistics communities, due to their elegant treatment of infinite dimensional objects like functions and densities, as well as their ability to sidestep the need for model selection.
In this paper we consider the limit behavior of two popular semi-supervised learning (SSL) methods based on the graph Laplacian: the regularization approach [15] and the spectral approach [3].
Understanding how to recognize complex, high-dimensional audio data is one of the greatest challenges of our time.
Kernel methods are broadly established as a useful way of constructing nonlinear algorithms from linear ones, by embedding points into higher dimensional reproducing kernel Hilbert spaces (RKHSs) [9].
We consider multi-class, multi-label and multi-instance classification (M3 C), a task of learning decision rules from corpora in which each pattern consists of multiple instances1 and is associated with multiple classes.
In high-throughput regression problems, the cost of evaluating test samples is just as important as the accuracy of the regression and most non-parametric regression techniques do not produce models that admit efficient implementation, particularly in hardware.
Semi-supervised learning, i.
Source separation problems arise when a number of signals are mixed together, and the objective is to estimate the underlying sources based on the observed mixture.
Many problems in signal processing, machine learning, and communications can be cast as a linear regression problem where an unknown signal x ∈ RN is related to its observations y ∈ RM via y = Φx + n.
Learning with relational data, or sets of propositions of the form (object, relation, object), has been important in a number of areas of AI and statistical data analysis.
In this work we consider learning on a graph.
Conditional random fields [1], or discriminatively trained undirected graphical models, have become the tool of choice for addressing many important tasks across bioinformatics, natural language processing, robotics, and many other fields [2, 3, 4].
The idea of search bootstrapping is to adjust the parameters of a heuristic evaluation function towards the value of a deep search.
In most real-world machine learning problems (e.
In many fields of science and engineering (among them genomics, financial engineering, natural language processing, remote sensing, and social network analysis), one encounters statistical inference problems in which the number of predictors p is comparable to or even larger than the number of observations n.
Functional MRI (fMRI) studies of human neuroanatomical organization commonly analyze fMRI data across a population of subjects.
For many application areas, such as text analysis and image analysis, learning a tree-based hierarchy is an appealing approach to illuminate the internal structure of the data.
Many problems in modern science and engineering involve high-dimensional data, by which we mean that the ambient dimension p in which the data lies is of the same order or larger than the sample size n.
Students often face the task of memorizing facts such as foreign language vocabulary or state capitals.
Convex optimization forms the backbone of many algorithms for statistical learning and estimation.
Inductive learning the process by which a new concept or category is acquired through observation of exemplars - poses a fundamental theoretical problem for cognitive science.
Probabilistic topic models have become popular tools for the unsupervised analysis of large document collections [1].
In recent years machine learning-based approaches to computer vision have helped to greatly accelerate progress in the field.
Suppose we have a large database of images, and we want to learn to predict who or what is in any given one.
In Natural Language Processing (NLP), the task of event coreference has numerous applications, including question answering, multi-document summarization, and information extraction.
Central to semi-supervised learning is the question how unlabeled data can help in either classification or regression.
Distributions over permutations play an important role in applications such as multi-object tracking, visual feature matching, and ranking.
There has recently been considerable interest in structure learning of Bayesian networks.
Magnetic resonance imaging (MRI) [10, 6] is a very flexible imaging modality.
Online learning has been extensively studied in the machine learning community (Rosenblatt, 1958; Freund & Schapire, 1999; Kivinen et al.
Sparseness is being regarded as one of the key features in machine learning [15] and biology [16].
Stochastic optimal control is of interest in many fields of science and engineering, however it remains hard to solve.
Finding a precise statistical characterization of natural images is an endeavor that has concerned research for more than fifty years now and is still an open problem.
Languages have a complex structure, full of general rules with idiosyncratic exceptions.
Topic models such as latent Dirichlet allocation (LDA) [3] have been recognized as useful tools for analyzing large, unstructured collections of documents.
Detecting abnormal points in small and simple datasets can often be performed by visual inspection, using notably dimensionality reduction techniques.
The abstraction of Markov random field (MRF) allows one to utilize graphical representation to capture inter-dependency between large number of random variables in a succinct manner.
The Indian buffet process (IBP) is an infinitely exchangeable distribution over binary matrices with a finite number of rows and an unbounded number of columns [1, 2].
Neural implementations of reinforcement learning have to solve two basic credit assignment problems: (a) the temporal credit assignment problem, i.
In a typical high dimensional setting, the number of variables p is much larger than the number of observations n.
Neurons in the visual cortex of primates and many other mammals are organized according to their tuning properties.
Far from being static relays, synapses are complex dynamical elements.
There is a growing body of experimental evidence consistent with the idea that animals are somehow able to represent, manipulate and, ultimately, make decisions based on, probability distributions.
Active learning addresses the problem that the algorithm is given a pool of unlabeled data drawn i.
Recent work on deep belief nets (DBNs) [10], [13] has shown that it is possible to learn multiple layers of non-linear features that are useful for object classification without requiring labeled data.
Over the past several decades, researchers in many different fields—statistics, economics, physics, genetics and machine learning—have focused on coming up with more accurate and more efficient approximate solutions to intractable probabilistic inference problems.
Spectral techniques are an authentic workhorse in machine learning, statistics, numerical analysis, and signal processing.
Statistical analysis of social networks and other relational data has been an active area of research for over seventy years and is becoming an increasingly important problem as the scope and availability of social network datasets increase [1].
Linear classifiers (i.
The assumption that training and test data are governed by identical distributions underlies many popular learning mechanisms.
Search engines typically use a ranking function to order results.
The problem of learning rotations finds application in many areas of signal processing and machine learning.
Incremental decremental algorithm for online learning of Support Vector Machine (SVM) was previously proposed in [1], and the approach was adapted to other variants of kernel machines [2–4].
Texture learning and synthesis are important tasks in computer vision and graphics.
Recent work has focused on the learning of similarity metrics within the context of nearest-neighbor (NN) classification [7, 8, 12, 15].
Graphical models have proven themselves to be an effective tool for representing the underlying structure of probability distributions and organizing the computations required for exact and approximate inference.
Structure learning aims to discover the topology of a probabilistic network of variables such that this network represents accurately a given dataset while maintaining low complexity.
Online learning has become the paradigm of choice for tackling very large scale estimation problems.
In this work we study the use of resting state activity for the semi-supervised analysis of human fMRI studies.
Probabilistic topic models [2, 9, 6] are often used to analyze and extract semantic topics from large text collections.
Suppose x ∈ Rn is a sparse vector, meaning its number of nonzero components k is smaller than n.
The Partially Observable Markov Decision Process (POMDP) model has proven attractive in domains where agents must reason in the face of uncertainty because it provides a framework for agents to compare the values of actions that gather information and actions that provide immediate reward.
In recent years, machine learning approaches for analyzing fMRI data have become increasingly popular [15, 24, 18, 16].
Understanding how the nervous system makes sense of uncertain visual stimuli is one of the central goals of perception research.
We study the learning ability of classifiers trained on examples generated from different sources, but where some observations are partially missing.
It is well known that natural images occupy a small fraction of the space of all possible images.
With the advent of compressive sensing and other related applications, there has been growing interest in finding sparse signal representations from redundant dictionaries [3, 5].
Many natural language processing applications, from discriminative training [18, 9] to minimumrisk decoding [16, 34], require the computation of expectations over large-scale combinatorial spaces.
Learning algorithms based on kernels have been used with much success in a variety of tasks [17,19].
Relaxations are a popular approach for tackling intractable optimization problems.
Relative to parametric methods, nonparametric regressors require few structural assumptions on the function being learned.
In machine learning, online algorithms operate by repetitively drawing random examples, one at a time, and adjusting the learning variables using simple calculations that are usually based on the single example only.
Policy Gradient Reinforcement Learning (PGRL) attempts to find a policy that maximizes the average (or time-discounted) reward, based on gradient ascent in the policy parameter space [2, 3, 4].
Markov random fields (MRF’s) provide a powerful tool for representing dependency structure between random variables.
Several models of object recognition drawing inspiration from visual cortex have been developed over the past few decades [3, 8, 6, 12, 10, 9, 7], and have enjoyed substantial empirical success.
Learning algorithms based on kernel methods have enjoyed considerable success in a wide range of supervised learning tasks, such as regression and classification [25].
It is widely acknowledged that identifying the regions that originate malicious traffic on the Internet is vital to network security and management, e.
Visualization as an important tool for exploratory data analysis has attracted much research effort in recent years.
Object class detection has been one of the mainstream research areas in computer vision.
Learning to rank has attracted the focus of many machine learning researchers in the last decade because of its growing application in the areas like information retrieval (IR) and recommender systems.
“Average-case” Investing: Much of mathematical finance theory is devoted to the modeling of stock prices and devising investment strategies that maximize wealth gain, minimize risk while doing so, and so on.
The linear correlation coefficient is of central importance in many studies that deal with spike count data of neural populations.
We seek to integrate knowledge from multiple information sources.
It is well-known that synapses change their synaptic efficacy (“weight”) w in dependence of the difference tpost − tpre of the firing times of the post- and presynaptic neuron according to variations of a generic STDP rule (see [1] for a recent review).
Natural images, although apparently diverse, have a surprisingly regular statistical regularity.
Recently, there has been a lot of interest in the problem of designing compact binary codes for reducing storage requirements and accelerating search and retrieval in large collections of highdimensional vector data [11, 13, 15].
Linear Dyna-style planning extends Dyna to linear function approximation (Sutton, Szepesvári, Geramifard & Bowling, 2008), and can be used in large-scale applications.
In many applications, one would like to discover and model dynamical behaviors which are shared among several related time series.
Electroencephalography (EEG) and magnetoencephalography (MEG) provide an instantaneous and non-invasive measure of brain activity.
Since the presence of supervision in biological learning mechanisms is rare, organisms often have to rely on the ability of these mechanisms to extract statistical regularities from their environment.
Since William James first described the phenomenology of attention [11], psychologists have been struggling to specify the cognitive architecture of this process, how it is limited, and how it helps information processing.
Analysis of biological networks has led to numerous advances in understanding the organizational principles and functional properties of various biological systems, such as gene regulatory systems [1] and central nervous systems [2].
Range search is a fundamental proximity task at the core of many learning problems.
Online decision-making is a learning problem in which one needs to choose a decision repeatedly from a given set of decisions, in an effort to minimize costs over the long run, even in the face of complete uncertainty about future outcomes.
A continuous-time Markov chain (CTMC) is a model of a dynamical system which, upon entering some state, remains in that state for a random real-valued amount of time (called the dwell time or occupancy time) and then transitions randomly to a new state.
Discovering how DNA-binding proteins called transcription factors (TFs) regulate gene expression programs in living cells is fundamental to understanding transcriptional regulatory networks controlling development, cancer, and many human diseases.
A central problem in computational neuroscience is to develop functional models that can accurately describe the relationship between external variables and neural spike trains.
People appear to make rational statistical inferences from noisy, uncertain input in a wide variety of perceptual and cognitive domains [1, 9].
Probabilistic models provide a powerful and intuitive framework for formulating several problems in machine learning and its application areas, such as computer vision and computational biology.
Image understanding is one of the Holy Grail problems in computer vision.
Models based on local features have achieved state-of-the art results in many visual object recognition tasks.
The Fisher linear discriminant analysis (LDA) is a classical method that considers dimensionality reduction and classification jointly.
Conditional maximum entropy models [1, 3], conditional maxent models for short, also known as multinomial logistic regression models, are widely used in applications, most prominently for multiclass classification problems with a large number of classes in natural language processing [1, 3] and computer vision [12] over the last decade or more.
That visual input at a given point is greatly influenced by its spatial context is manifest in a host of neural and perceptual effects (see, e.
This paper studies learning problems of the following form.
Invariance to abstract input variables is a highly desirable property of features for many detection and classification tasks, such as object recognition.
L1 regularization has become an important tool over the last decade with a wide variety of machine learning applications.
Energy issues present one of the largest challenges facing our society.
Conditional random fields (CRFs, [1]) have been successful in modeling complex systems, with applications from speech tagging [1] to heart motion abnormality detection [2].
Let Y be a p-dimensional random vector with distribution P .
The problem of sparse estimation is becoming increasingly important in machine learning and statistics.
In this paper we consider the problem of estimating the support of an arbitrary probability distribution and we are more broadly motivated by the problem of learning from complex high dimensional data.
Linear discriminative online algorithms have been shown to perform very well on binary and multiclass labeling problems [10, 6, 14, 3].
Semi-supervised learning algorithms use both labeled and unlabeled examples.
Consider the indoor image shown in Figure 1.
Researchers have been increasingly relying on cross species analysis to understand how biological systems operate.
Dimensionality reduction is an important aspect of many statistical learning tasks.
In recent years interest has been building [10, 21, 16, 8, 12] in the problem of detecting locations in the visual field that are responsible for auditory signals.
In the past decade, computer vision research in object recognition has firmly established the efficacy of representing images as collections of local descriptors of edge orientation.
The invention of the perceptron [12] goes back to the very beginning of AI more than half a century ago.
Clustering algorithms group data items into categories without requiring human supervision or definition of categories.
Clustering has traditionally been a tool of unsupervised learning.
Over the past decade the amount of available data has increased steadily.
Neurons communicate mostly through noisy sequences of action potentials, also known as spike trains.
A clustering of a finite set V of data points is a partition of V into subsets (called clusters) such that data points in the same cluster are similar to each other.
In 24 hours an electrocardiogram (ECG) can record over 100,000 heartbeats for a single patient.
Accurate wind speed and direction forecasts are essential for efficient integration of wind energy into electrical transmission systems.
There is currently considerable interest in structure learning of dynamic Bayesian networks (DBNs), with a variety of applications in signal processing and computational biology; see e.
Network-structured optimization problems arise in a variety of application domains within the information sciences and engineering.
Consider a classification task where a learner is given training items x1 , .
Suppose you are asked to make a series of moral judgments by rating, on a 1–10 scale, various actions, with a rating of 1 indicating ’not particularly bad or wrong’ and a rating of 10 indicating ’extremely evil.
In the last decade, there has been much research at the interface of computer science and game theory (see e.
How should we perform experiments to determine the most accurate scientific theory among competing candidates, or choose among expensive medical procedures to accurately determine a patient’s condition, or select which labels to obtain in order to determine the hypothesis that minimizes generalization error? In all these applications, we have to sequentially select among a set of noisy, expensive observations (outcomes of experiments, medical tests, expert labels) in order to determine which hypothesis (theory, diagnosis, classifier) is most accurate.
Recently, there has been a push towards combining logical and probabilistic approaches in Artificial Intelligence.
Active learning is well-motivated in many supervised learning scenarios where unlabeled instances are abundant and easy to retrieve but labels are difficult, time-consuming, or expensive to obtain.
Continuous latent factor models, such as factor analysis (FA) and probabilistic principal components analysis (PPCA), are very commonly used density models for continuous-valued data.
Probabilistic grammars are an important statistical model family used in natural language processing [7], computer vision [16], computational biology [19] and more recently, in human activity analysis [12].
Sequential decision-making problems under uncertainty and partial observability are typically modeled using Partially Observable Markov Decision Processes (POMDPs) [1], where the objective is to decide how to act so that the sequence of visited states optimizes some performance criterion.
The exact solution for the reinforcement learning (RL) and planning problems with large state space is difficult or impossible to obtain, so one usually has to aim for approximate solutions.
High-dimensional data sets present challenges that are both statistical and computational in nature.
Control of sensorimotor systems, artificial or biological, is inherently both a spatial and temporal process.
The problem of identifying high-dimensional activation patterns embedded in noise is important for applications such as contamination monitoring by a sensor network, determining the set of differentially expressed genes, and anomaly detection in networks.
Many machine learning algorithms can be formulated as a penalized optimization problem: min l(x) + λφ(x), x (1) where l(x) is the empirical loss function (e.
Support vector machines (SVMs) as proposed in [1, 8] are powerful classifiers, especially in the binary case of two possible classes.
The classic dichotomy between generative and discriminative methods for classification in machine learning can be clearly seen in two distinct performance regimes as the number of training examples is varied [12, 18].
In this paper we tackle the problem of obtaining a tight characterization of the sample complexity which a particular learning rule requires, in order to learn a particular source distribution.
Since the beginning of neuromorphic engineering [1, 2] designers have had great success in building VLSI1 neurons mimicking the behavior of biological neurons using analog circuits [3–8].
Let V be a set of |V | = n nodes and B ⊂ K1/2 ⊂ K be the following sets: B = {0, 1}V K1/2 = {0, 21 , 1}V K = [0, 1]V A function f : B → R is called pseudo-boolean.
Determining the pose of a human body from one or more images is a central problem in Computer Vision.
Many machine learning algorithms minimize a regularized risk [1]: m J(θ) = Ω(θ) + Remp (θ), where Remp (θ) = 1 � l(xi , yi , θ).
Black-box identification approaches are widely used to learn dynamic models from a finite set of input/output data [1].
With the development of advanced data collection techniques, large quantities of high-dimensional data are commonly available in many applications.
Alignment of functional neuroanatomy across individuals forms the basis for the study of the functional organization of the brain.
Consider the advertisement display problem, where a search engine company chooses an ad to display which is intended to interest the user.
Understanding the meanings and contents of images remains one of the most challenging problems in machine intelligence and statistical learning.
Probabilistic topic models like the latent Dirichlet allocation (LDA) [5] have recently been applied to a number of computer vision tasks such as objection annotation and scene classification due to their ability to capture latent semantic compositions of natural images [22, 23, 9, 13].
Research on Multiple Kernel Learning (MKL) needs to follow a two pronged approach.
Complex visual hallucinations [1] can offer a fascinating insight into how the brain realizes visual perception.
In many implantable biomedical microsystems [1, 2], an embedded system capable of recognising high-dimensional, time-varying signals have been demanded.
Consider a classification task, in which the objective is to assign labels Y to vectors of one or more attribute values X.
In applications such as active learning [1, 2, 3, 4], disease/fault diagnosis [5, 6, 7], toxic chemical identification [8], computer vision [9, 10] or the adaptive traveling salesman problem [11], one often encounters the problem of identifying an unknown object while minimizing the number of binary questions posed about that object.
Sparsity has become a popular way to deal with small samples of high dimensional data and, in a broad sense, refers to the possibility of writing the solution in terms of a few building blocks.
The Curse of Dimensionality [2] has inspired research in several directions in Computer Science and has led to the development of several novel techniques such as dimensionality reduction, sketching etc.
Two themes dominate recent progress towards situated visual object recognition.
In the field of population coding, a recurring question is the impact on coding efficiency of so-called noise correlations, i.
Thanks to a sustained world-wide effort, modern automatic speech recognition technology has now reached a level of performance that makes it suitable as an enabling technology for novel applications such as automated dictation, speech based car navigation, multimedia information retrieval, etc.
Multi-instance Learning (MIL) was first proposed by Dietterich et.
In this paper we study the general affine rank minimization problem (ARMP), min rank(X) s.
Action potentials are stereotyped all-or-nothing events, meaning that their amplitude is not considered to transmit any information and only the exact time of occurrence matters.
We address the problem of prediction in graphical models that are computationally challenging because of both high-treewidth and large state-spaces.
Utility elicitation is a key component in many decision support applications and recommender systems, since appropriate decisions or recommendations depend critically on the preferences of the user on whose behalf decisions are being made.
In the classical supervised machine learning paradigm the learner is given a labeled training set of examples and her goal is to find a decision function with the small generalization error on the unknown test examples.
In machine learning it is widely known that if several tasks are related, then learning them simultaneously can improve performance [1–4].
Neuronal elements of the brain constitute an intriguing complex network [4].
Producing large-scale training, validation and test sets is vital for many applications.
Multiple Instance Learning (MIL) has been proposed over 10 years ago as a methodology to learn models under weak labeling constraints [1].
The sequence memoizer (SM) is a Bayesian nonparametric model for discrete sequence data producing state-of-the-art results for language modeling and compression [1, 2].
Let x0 ∈ RN be an unknown vector, and assume that a vector y ∈ Rn of noisy linear measurements of x0 is available.
The emergence of organization is at the core of many complex systems, from neural cell assemblies to living insect societies.
The nature of encoding and processing of sensory information in the visual, auditory and olfactory systems has been extensively investigated in the systems neuroscience literature.
Feature selection, the process of selecting a subset of relevant features, is a key component in building robust machine learning models for classification, clustering, and other tasks.
In recent years sparse coding has become a popular paradigm to learn dictionaries of natural images [10, 1, 4].
Continuous Markov random fields are a general and expressive formalism to model complex probability distributions over multiple continuous random variables.
Learning to rank has recently gained much attention in machine learning, due to its wide applications in real problems such as information retrieval (IR).
Given an undirected, weighted graph, the commute distance between two vertices u and v is defined as the expected time it takes a random walk starting in vertex u to travel to vertex v and back to u.
Image representation (features) is arguably the most fundamental task in computer vision.
Learning from weakly annotated data is a long standing goal for the practical application of machine learning techniques to real world data.
One way the human brain manages the massive amount of sensory information it receives is by learning invariants — regularities in its input that do not change across many stimuli sharing some property of interest.
Acoustic modeling is a fundamental problem in automatic continuous speech recognition.
Optimal classifiers minimize the expected value of a loss function, or risk.
Monte-Carlo tree search (MCTS) is a new approach to online planning that has provided exceptional performance in large, fully observable domains.
Social network analysis has traditionally relied on self-reported data collected via interviews and questionnaires [27].
Online learning algorithms are simple, fast, and require less memory compared to batch learning algorithms.
Huge quantities of text data such as news articles and blog posts arrives in a continuous stream.
Least-squares temporal difference (LSTD) learning [3, 2] is a widely used reinforcement learning (RL) algorithm for learning the value function V π of a given policy π.
Local learning machines such as nearest neighbors classifiers, radial basis function (RBF) kernel machines or linear classifiers predict the class of new data points from their neighbors in the input space.
Rank aggregation aims at combining multiple rankings of objects to generate a better ranking.
This paper is concerned with the problem of model selection (or structure learning) in Gaussian graphical modelling.
Eigenvalue problems associated to a symmetric and positive semi-definite matrix are quite abundant in machine learning and statistics.
In multivariate data analysis, graphical models such as Gaussian Markov Random Fields provide a way to discover meaningful interactions among variables.
In reinforcement learning, an agent interacts with the environment, learning through trial-and-error based on scalar reward signals.
Sparse coding has attracted significant attention in recent years because it has been shown to be effective for some classification problems [12, 10, 9, 13, 11, 14, 2, 5].
Hierarchical reinforcement learning [1] offers an appealing family of approaches to scaling up standard reinforcement learning (RL) [2] methods by enabling the use of both low-level primitive actions and higher-level macro-actions (or skills).
Markov decision processes (MDPs) are a well-studied model of sequential decision-making under uncertainty [11].
There has been an explosion of interest in machine learning over the past decade, much of which has been fueled by the phenomenal success of binary Support Vector Machines (SVMs).
Probabilistic mixture modeling [7] has been widely used for density estimation and clustering applications.
Decision making has often been studied in experiments in which a subject repeatedly chooses actions and rewards are given depending on the action.
We consider ordinary least-squares regression using randomly generated feature spaces.
Simultaneous analysis of groups of system components with similar functions, or subsystems, has recently received considerable attention.
Imagine that you are traveling in a moving car and observe a walker through a fence full of punch holes.
Many application problems need to simultaneously predict several quantities using a common set of variables, e.
Learning kernel functions is an ongoing research topic in machine learning that focuses on learning an appropriate kernel function for a given task.
The prediction of the future state of an evolving graph is a challenge of interest in many applications such as predicting hyperlinks of webpages [16], finding protein-protein interactions [7], studying social networks [9], as well as collaborative filtering and recommendations [6].
CUR decompositions are a recently-popular class of randomized algorithms that approximate a data matrix X ∈ Rn×p by using only a small number of actual columns of X [12, 4].
Bayesian methods in machine learning, although elegant and concrete, have often been criticized not only for their computational cost, but also for their strong assumptions on the correctness of the prior distribution.
We address the reinforcement learning (RL) problem of finding a good policy in an unknown, stochastic, and partially observable domain, given both data from independent exploration and expert demonstrations.
In active learning, a learner is given access to unlabeled data and is allowed to adaptively choose which ones to label.
Graphical models provide an effective framework to model complex systems via simpler local interactions and also provide an insight into the structure of the underlying probabilistic model.
Convolutional neural networks (CNNs) [1] have been successfully applied to many recognition tasks.
The need for distributions over sets of structures arises frequently in computer vision, computational biology, and natural language processing.
In the nervous system, sensory and motor information, as well as internal brain states, are represented by action potentials in populations of neurons.
The advent of functional neuroimaging techniques, in particular fMRI, has for the first time provided non-invasive, large-scale observations of brain processes.
Parametric flow problems have been well-studied in operations research [7].
For many animals, it is vital to be able to quickly locate the source of an unexpected sound, for example to escape a predator or locate a prey.
In many scientific and engineering applications, such as image annotation [28] and web-page classification [6], the available data usually come from diverse domains or are extracted from different aspects, which will be referred to as views.
Convex optimization has become a key tool in many machine learning algorithms.
This paper deals with the problem of recognizing transitive actions in single images.
We consider the problem of maximizing an unknown function f (x) when each evaluation of the function has a high cost.
Compilation to arithmetic circuits (ACs) [1] is one of the most effective methods for exact inference in Bayesian networks.
Look at the two persons in Fig.
The broad goal of supervised learning is to effectively learn unknown functional dependencies between a set of input variables and a set of output variables, given a finite collection of training examples.
Consider the subject of density estimation for high-dimensional continuous random variables, like images.
The k-means clustering algorithm [16] was recently recognized as one of the top ten data mining tools of the last fifty years [20].
The motivation of the present work lies in the growing interest of the machine learning community for learning tasks that are richer than now well-studied classification and regression.
As software systems grow in size and complexity, they become more difficult to develop and maintain.
Assume we have a set of n data observations (x1 , y1 ), .
Coherence is perhaps the most fundamental property of probability estimation.
Q-learning is a popular reinforcement learning algorithm that was proposed by Watkins [1] and can be used to optimally solve Markov Decision Processes (MDPs) [2].
It has been widely accepted that biological sensory systems are adapted to match the statistical properties of the signals in the natural environments.
A key issue in computational neuroscience is the interpretation of neural signaling, as expressed by a neuron’s sequence of action potentials.
Layered models of scenes offer significant benefits for optical flow estimation [8, 11, 25].
In standard supervised learning, each training sample is associated with a label, and the classifier is usually trained through the minimization of the empirical risk on the training set.
Brain image analyses have widely relied on univariate voxel-wise analyses, such as voxel-based morphometry (VBM) for structural MRI [1].
Many bottom up theories of neural encoding posit that sensory systems are optimized to represent sensory information, subject to limitations of noise and resources (e.
Multi-task learning (MTL) [6, 8, 19] refers to the joint training of multiple problems, enforcing a common intermediate parameterization or representation.
Policy gradient methods have been some of the most effective learning algorithms for dynamic control tasks in robotics.
Consider a general two-hidden-layer multilayer perceptron (MLP) having a single (terminal) output, H nodes at the second hidden layer (next to the terminal layer), I nodes at the first hidden layer, and J nodes at the input layer; hence, a J-I-H-1 MLP.
Size, color, and orientation have long been considered elementary features [14] that are available to guide attention and visual search [17].
There has been significant recent research on the analysis of incomplete matrices [10, 15, 1, 12, 13, 18].
Robust statistics is a well established field that analyzes the sensitivity of common estimators to outliers and provides alternative estimators that achieve improved robustness [11, 13, 17, 23].
Unlike standard supervised learning problems which involve simple scalar outputs, structured prediction deals with structured outputs such as sequences, grids, or more general graphs.
Conventional views of language acquisition often assume that human language learners initially use a single source of information to acquire one component of language, which they then use to leverage the acquisition of other linguistic components.
Over the last few years, a growing amount of research on visual recognition has focused on learning low-level and mid-level features using unsupervised learning, supervised learning, or a combination of the two.
Heavy-tailed distributions naturally occur in many real life phenomena, for example in computer networks [23, 14, 16].
Efficient similarity search with large databases is central to many applications of interest, such as example-based learning algorithms, content-based image or audio retrieval, and quantization-based data compression.
Recent studies have shown promising performance of kernel methods for object classification, recognition and localization [1].
The efficient computation of the similarity (or overlap) between sets is a central operation in a variety of applications, such as word associations (e.
Continuous time stochastic processes are receiving increasing attention within the statistical machine learning community, as they provide a convenient and physically realistic tool for modelling and inference in a variety of real world problems.
Image understanding is a central problem in computer vision that has been extensively studied in the forms of various types of tasks.
The CTBT aims to prevent the proliferation and the advancement of nuclear weapon technology by banning all nuclear explosions.
Semi-supervised learning methods make assumptions about how unlabeled data can help in the learning process, such as the manifold assumption (data lies on a low-dimensional manifold) and the cluster assumption (classes are separated by low density regions) [4, 16].
In natural behavior as well as in engineering applications, there is often the need to choose, under time pressure, an action among multiple options with imprecisely known consequences.
One of the most striking features of spike trains is their variability – that is, the same visual stimulus does not elicit the same spike pattern on repeated presentations.
We present a generative probabilistic model for learning concept graphs from text.
Many probabilistic models incorporate multivariate Gaussian distributions to explain dependencies between variables.
Understanding how the brain performs statistical inference is one of the main problems of theoretical neuroscience.
Statistical relational learning has attracted ever-growing interest in the last decade, because of widely available relational data, which can be as complex as citation graphs, the World Wide Web, or relational databases.
Latent Dirichlet Allocation [4] assigns topics to documents and generates topic distributions over words given a collection of texts.
Many computer vision problems such as SfM [26], non-rigid SfM [3] and photometric stereo [11] can be formulated as a matrix factorization problem.
In many forensic domains it is necessary to characterize the degree to which a given piece of evidence is unique.
We consider the nonparametric problem of estimating Rényi α-entropy and mutual information (MI) based on a finite sample drawn from an unknown, absolutely continuous distribution over Rd .
There has recently been significant interest in the analysis of data sets whose individual records are too sensitive to expose directly, examples of which include medical information, financial data, and personal data from social networking sites.
Datasets available for prediction tasks are growing over time, resulting in increasing scale in all their measurable dimensions: separate from the issue of the growing number of examples m and features d, they are also growing in the number of classes k.
In spite of the wide use of clustering in many practical applications, currently, there exists no principled method to guide the selection of a clustering algorithm.
Many computer vision problems inherently involve data that is represented by multiple modalities such as different types of image features, or images and surrounding text.
Latent force models [1] are a new approach for modeling data that allows combining dimensionality reduction with systems of differential equations.
How can we reasonably expect to learn given possibly adversarial data? Overcoming this obstacle has been one of the major successes of the Online Learning framework or, more generally, the so-called competitive analysis of algorithms: rather than measure an algorithm only by the cost it incurs, consider this cost relative to an optimal “comparator algorithm” which has knowledge of the data in advance.
Finding the place (or time) where most or all of a set of one-dimensional signals (or profiles) jointly change in some specific way is an important question in several fields.
Hierarchical Bayesian modeling has become a mainstay in machine learning and applied statistics.
In the classical K-armed bandit problem, an agent selects at each time step one of the K arms and receives a reward that depends on the chosen action.
Many applications require classification techniques dealing with train and/or test instances with missing features: e.
The concept of sparsity is widely used in the signal processing, machine learning and statistics communities for model fitting and solving inverse problems.
Motion capture systems have become widespread in many application areas such as robotics [18], physical therapy, sports sciences [10], virtual reality [15], artificial movie generation [13], computer games [1], etc.
The focus of this paper is a novel Bayesian framework for learning with probabilistic deterministic finite automata (PDFA) [9].
This paper presents: (1) a new formulation of image segmentation as the maximum-weight independent set (MWIS) problem; and (2) a new algorithm for solving MWIS.
In stochastic optimization, a decision maker makes a decision and faces a random cost based on that decision.
Diffusion Tensor Imaging (DTI or DT-MR) is an imaging modality that facilitates measurement of the diffusion of water molecules in tissues.
The concept of parsimony is central in many scientific domains.
Action potentials play the central role in neuron-to-neuron communication.
This paper addresses the problem of evaluating a given model in terms of its predictive performance.
In traditional bandit models, the learner is presented with a set of K actions.
We develop a novel generative model which combines the powerful invariance properties achieved through the use of hidden variables in epitome [2] and stel (structural element) models [6, 8].
A graph G = (V, E) denotes a collection of entities, represented by vertices V , along with some relationship between pairs, represented by edges E.
The last few years have seen a proliferation of human efforts to collect labeled image data sets for the purpose of training and evaluating visual recognition systems.
Motivated by [KS02, KK99] among others, Li, Littman and Walsh [LLW08] introduced the KWIK framework for online learning, standing for knows what it knows.
The sparse signal recovery problem has been studied in many areas including machine learning [18, 19, 22], signal processing [8, 14, 17], and mathematics/statistics [2, 5, 7, 10, 11, 12, 13, 20].
In this work, we focus on the pool-based active learning, which selects an unlabeled instance from a given pool for manually labeling.
Reinforcement learning can be used to handle policy search problems in unknown environments.
Many of the world’s languages are sensitive to word order.
Trace-norm regularization is a popular approach for matrix completion and collaborative filtering, motivated both as a convex surrogate to the rank [7, 6] and in terms of a regularized infinite factor model with connections to large-margin norm-regularized learning [17, 1, 15].
Integrating AI with Human Computer Interaction has received significant attention in recent years [8, 11, 13, 3, 2].
Latent variable models provide an elegant formulation for several applications of machine learning.
If Sparse Coding (SC, [1]) or Independent Component Analysis (ICA; [2, 3]) are applied to image patches, basis functions are inferred that closely resemble Gabor wavelet functions.
Graphical models such as Markov Random Fields (MRFs) have been successfully applied to a wide variety of applications such as image understanding [1], error correcting codes [2], protein folding [3] and multi-agent systems coordination [4].
As graphical models are applied to more complex problems, it is increasingly necessary to learn parameters from data.
We are increasingly confronted with very high dimensional data in speech signals, images, geneexpression data, and other sources.
This paper is about the following problem: suppose we are given a large data matrix M , and we know it can be decomposed as M = L0 + C0 , where L0 is a low-rank matrix, and C0 is non-zero in only a fraction of the columns.
Preference elicitation (PE) is an important component of interactive decision support systems that aim to make optimal recommendations to users by actively querying their preferences.
Multivariate real-valued distributions are of paramount importance in a variety of fields ranging from computational biology and neuro-science to economics to climatology.
Markov Chain Monte Carlo (MCMC) methods have gained enormous popularity over a wide variety of research fields [6, 8], owing to their ability to compute expectations with respect to complex, high dimensional probability distributions.
The counting problem is the estimation of the number of objects in a still image or video frame.
Analysis of “relational data”, such as the hyperlink structure on the Internet, friend links on social networks, or bibliographic citations between scientific articles, is useful in many aspects.
A foundational concept in modern machine learning is to construct models for data by balancing the complexity of the model against fidelity to the measurements.
Learning algorithms based on 𝑙1 regularization have a long history in machine learning and statistics.
Many modern software systems compute a result as the solution, or approximate solution, to an optimization problem.
We wish to estimate the value function of a policy in an unknown decision process in a high dimensional and partially-observable environment.
In the past years, kernel methods, like support vector machines (SVM), have achieved great success in many learning problems, such as classification and regression.
The framework we propose is applicable in the following setup: let C denote a combinatorial space, by which we mean a finite but large set, where P testing membership is tractable, but enumeration is not, and suppose that the goal is to compute x∈C f (x), where f is a positive function.
In this work, we are interested in the problem of extracting the CPU instructions that comprise a binary executable file.
The contribution of this paper is the learning theoretical analysis of kernel-based least squares regression in combination with conjugate gradient techniques.
Learning procedures based on positive semidefinite (psd) kernel functions, like Support vector machines (SVMs), have emerged as powerful tools for several learning tasks with wide applicability [13].
A domain adaptation approach for NLP tasks, termed E ASYA DAPT (EA), augments the source domain feature space using features from labeled data in target domain [1].
Latent representations of data are wide-spread tools in supervised and unsupervised learning.
There are many applications where a classifier must be designed under computational constraints.
Consider empirical risk minimization for a hypothesis class H = {h : X → R} w.
Using Markov random fields (MRFs) one can capture global statistical properties in large scale probabilistic networks while only explicitly modeling the interactions of neighboring sites.
A central preoccupation of learning theory is to understand what statistical estimation based on a finite data set reveals about the underlying distribution from which the data were sampled.
Two salient features of cortical networks are the numerous recurrent lateral connections within a cortical area and the high ratio of cortical cells to sensory input channels.
Inverse reinforcement learning aims to find a reward function for a Markov decision process, given only example traces from its optimal policy.
Sparse linear models have become a popular framework for dealing with various unsupervised and supervised tasks in machine learning and signal processing.
A problem common to both epidemiology and to systems biology is to infer causal relationships between phenotypic measurements (biomarkers) and disease outcomes or quantitative traits.
In medicine, as in many other disciplines, decisions are often based upon a comparative analysis.
Neurons are the predominant component of the nervous system and understanding them is a major challenge in modern neuroscience research [1].
In active learning [10, 13, 16], the learner draws unlabeled data from the unknown distribution defined on the learning task and actively queries some labels from an oracle.
Apprenticeship learning is a variant of reinforcement learning, first introduced by Abbeel & Ng [1] (see also [2, 3, 4, 5, 6]), designed to address the difficulty of correctly specifying the reward function in many reinforcement learning problems.
Consider the binary classification problem, where each input is classified into +1 or −1.
Many difficult problems have been shown to admit elegant and tractably computable representations via optimization over the set of positive semidefinite (PSD) matrices.
In real-world applications of machine learning, often the sampling of the training and test instances may differ, which results in a mismatch between the two distributions.
Undirected graphical models have emerged as a useful tool because they allow for a stochastic description of complex associations in high-dimensional data.
Learning curves are a convenient way of characterising the performance that can be achieved with machine learning algorithms: they give the generalisation error  as a function of the number of training examples n, averaged over all datasets of size n under appropriate assumptions about the data-generating process.
Assume a standard setting with data D = {(xi , yi )}ni=1 , where (xi , yi ) are sampled iid from the joint distribution p(x, y) on Rd × {±1}.
For more than a decade, kernel methods such as support vector machines (SVMs) have belonged to the most successful learning methods.
Communication and inference are intimately linked.
Many learning problems involve models represented in matrix form.
Sequential decision making in stochastic dynamic environments, also called the “planning problem,” is often modeled using a Markov Decision Process (MDP, cf [1, 2, 3]).
Elucidating neural encoding is one of the most important issues in neuroscience.
Discovering causal relations from observational data is an important, ubiquitous problem in science.
Optical flow refers to the deformation of the domain of an image that results from ego- or scene motion.
Modern robots are designed to perform complicated planning and control tasks, such as manipulating objects, navigating in outdoor environments, and driving in urban settings.
This paper brings together two topics: online convex optimization and sampling from logconcave distributions over convex bodies.
Recently, multi-label classification (MLC) has been drawing increasing attention from the machine learning community (e.
Human knowledge and expertise is often tied to particular contexts.
Gaussian process classifiers (GPCs) [12] provide a Bayesian approach to nonparametric classification with the key advantage of producing predictive class probabilities.
We consider online learning in finite Markov decision processes (MDPs) with a fixed, known dynamics.
Data clustering is a fundamental problem in many fields, such as machine learning, data mining and computer vision [1].
Acting optimally under uncertainty requires comparing the expected utility of each possible action, but in most situations of practical interest this expectation is impossible to calculate exactly: the hidden states that must be integrated over may be high-dimensional and the probability density may not take on any simple form.
Networks of various types, formed by a large number of neurons through synapses, are the substrate of brain functions.
Consider the problem of recognizing an image that contains a single hand-written digit that has been approximately normalized but may have been written in one of a number of different styles.
We encounter complex dynamic scenes in everyday life.
Like insects with unmovable compound eyes, most current computer vision systems use images of uniform resolution.
One of the fundamental problems in computational biology is to understand associations between genomic variations and phenotypic effects.
Object detection remains one of the core objectives of computer vision, either as an objective per se, for instance for automatic focusing on faces in digital cameras, or as means to get high-level understanding of natural scenes for robotics and image retrieval.
A general form of estimating a quantity w ∈ Rn from an empirical measurement set X by minimizing a regularized or penalized functional is ŵ = argmin{L(Iw (X )) + λJ (w)}, (1) w where Iw (X ) ∈ Rm expresses the relationship between w and data X , L(.
Note by authors after publication: The results in Figure 3 could not be reproduced in subsequent experiments and should be considered invalid.
How neuronal networks can store a memory trace for recent sequences of stimuli is a central question in theoretical neuroscience.
In the online learning framework, the learner is faced with a sequence of data appearing at discrete time intervals.
In many machine learning domains, several sub-tasks operate on the same raw data to provide correlated outputs.
In recent years, individuals and corporate entities have gathered large quantities of personal data.
Markov networks (also known as Markov random fields, etc.
The problem of ﬁnding a low-rank approximation of a target matrix through matrix factorization (MF) attracted considerable attention recently since it can be used for various purposes such as reduced rank regression [19], canonical correlation analysis [8], partial least-squares [27, 21], multi-class classiﬁcation [1], and multi-task learning [7, 29].
Boosting [17] refers to a general technique of combining rules of thumb, or weak classifiers, to form highly accurate combined classifiers.
We consider pairwise Markov Random Field (MRF) over n binary variables x = x1 , .
Recently, it has been shown that learning multiple tasks together helps learning [8, 19, 9] when the tasks are related, and one is able to use an appropriate notion of task relatedness.
Structural aspects of models are often critical to obtaining flexible, expressive model families.
Learning multiple tasks has been studied for more than a decade [6, 24, 11].
This paper studies a family of decision-making problems in which discrete events occur on a continuous time scale.
During the past decade the effects of exact spike timing on the change of synaptic connectivity have been studied extensively.
The study of brain functional connectivity, as revealed through distant correlations in the signals measured by functional Magnetic Resonance Imaging (fMRI), represents an easily accessible, albeit indirect marker of brain functional architecture; in the recent years, it has given rise to fundamental insights on brain organization by representing it as a modular graph with large functionallyspecialized networks [1, 2, 3].
Policy gradient methods [18] in Reinforcement Learning have gained popularity, due to the guaranteed improvement in control performance over iterations (which is often lacking in approximate policy or value iteration) as well as the discovery of more efficient gradient estimation methods.
The challenge of inferring whether X causes Y (“X → Y ”) or vice versa (“Y → X”) from joint observations of the pair (X, Y ) has recently attracted increasing interest [1, 2, 3, 4, 5, 6, 7, 8].
Computing systems today are ubiquitous, and range from the very small (e.
This paper presents a new computational framework, called random forest random field (RF)2 , which provides a principled way to jointly reason about multiple, statistically dependent random variables and their attributes.
The last two decades have been marked by significant advances in modeling multivariate probability density functions (PDFs) on graphs.
Estimating the 3D pose of an articulated body or of a deformable surface from monocular images is one of the fundamental problems in computer vision.
Camera shake is a common problem of handheld, longer exposed photographs occurring especially in low light situations, e.
As the cornerstone of Bayesian nonparametric modeling, Dirichlet processes (DP) [22] have been applied to a wide variety of inference and estimation problems [3, 10, 20] with Dirichlet process mixtures (DPMs) [15, 17] being one of the most successful.
The immense volume of textual information available on the web provides an important opportunity and challenge for AI: Can we develop methods that can learn domain knowledge by reading natural texts such as news articles and web pages.
1 Video Annotation and Tracking with Active Learning Carl Vondrick UC Irvine Deva Ramanan UC Irvine vondrick@mit.
Paraphrase detection determines whether two phrases of arbitrary length and form capture the same meaning.
Most reinforcement learning [1] algorithms are value-function based—learning is performed by estimating the expected return (discounted sum of rewards) obtained by following the current policy from each state, and then updating the policy based on the resulting so-called value function.
We are primarily interested in learning in domains where there is only a small amount of labeled data but advice can be provided by a domain expert.
When diagnosed with cancer, most patients ask about their prognosis: “how long will I live”, and “what is the success rate of each treatment option”.
Mirror Descent is a first-order optimization procedure which generalizes the classic Gradient Descent procedure to non-Euclidean geometries by relying on a “distance generating function” specific to the geometry (the squared �2 norm in the case of standard Gradient Descent) [14, 4].
This paper introduces a new type of regularity in the reinforcement learning (RL) and planning problems with finite-action spaces that suggests that the convergence rate of the performance loss to zero is faster than what previous analyses had indicated.
Submodularity has been and continues to be an important property in many fields.
Network datasets comprising edge measurements Aij ∈ {0, 1} of a binary, symmetric, and antireflexive relation on a set of n nodes, 1 ≤ i < j ≤ n, are fast becoming of paramount interest in the statistical analysis and data mining literatures [1].
We consider the problem of finding a good approximation of the maximum of a function f : X → R using a finite budget of evaluations of the function.
Recent years have witnessed the emergence of several reinforcement-learning techniques that make it possible to learn a decision policy from a batch of sample transitions.
Nearest neighbor (NN) classifiers would be one of the classical and perhaps the simplest non-linear classification algorithms.
Increasingly in modern settings, in domains across science and engineering, one is faced with the challenge of working with high-dimensional models where the number of parameters is large, particularly when compared to the number of observations.
Boosting is a simple and efficient machine learning algorithm which provides state-of-the-art performance on many tasks.
Biological systems are constantly interacting with a dynamic, noisy environment, which they can only assess through noisy sensors.
Feature learning is an important problem in statistics and machine learning, characterized by the goal of (typically) inferring a low-dimensional set of features for representation of high-dimensional data.
Consider the linear regression model y = Xβ ∗ + ε, (1) where y is a vector of observations, X ∈ Rn×p a design matrix, ε a vector of noise and β ∗ a vector of coefficients to be estimated.
We nowadays routinely face high-dimensional datasets in diverse application areas such as biology, astronomy, and finance.
With the advent of the Internet, many machine learning applications are faced with very large and inherently high-dimensional datasets, resulting in challenges in scaling up training algorithms and storing the data.
The design of efficient algorithms for large scale similarity search (such as nearest neighbor search) has been a central problem in computer science.
Selective prediction is the study of predictive models that can automatically qualify their own predictions and output “don’t know” when they are not sufficiently confident.
There are many suggestions that humans and other animals employ multiple approaches to learned decision making [1].
Kernel methods [24, 21] allow to obtain nonlinear learning machines from simpler, linear ones; nowadays they can almost completely be applied out-of-the-box [3].
Methods for solving sequential decision problems with delayed reward, where the problems are formulated as Markov decision processes (MDPs), have been compared to the learning mechanisms of animal brains [3, 4, 9, 10, 13, 20, 22].
The objective of inverse reinforcement learning (IRL) is to determine the decision making agent’s underlying reward function from its behaviour data and the model of environment [1].
Reinforcement learning (RL) is often used to solve single tasks for which it is tractable to learn a good policy with minimal initial knowledge.
Traditional image categorization methods usually consider an image as one indiscrete entity, which, however, neglects an important fact that the semantic meanings (labels) of an image mostly arise from its constituent regions, but not the entire image.
The problem of sampling K representative data points from a large dataset arises frequently in various applications.
Kernel methods have been popular in machine learning and pattern analysis for their superior performance on a wide spectrum of learning tasks.
Learning multiple related tasks is increasingly important in modern applications, ranging from the prediction of tests scores in social sciences and the classification of protein functions in systems biology to the categorisation of scenes in computer vision and more recently to web search and ranking.
Vector Auto-regressive models (VAR) are an important class of models for analyzing multivariate time series data.
The concept of parsimony is central in many scientific domains.
Over the last decade the use of Gaussian Processes (GPs) as non-parametric regression models has grown significantly.
“Learning to learn”, or the ability to learn abstract representations that support transfer to novel but related tasks, lies at the core of many problems in computer vision, natural language processing, cognitive science, and machine learning.
Modeling temporal dependence is an important consideration in many learning problems.
(a) (b) (c) (d) (e) (f) Figure 1: Which of these images are the most memorable? See footnote 1 for the answer key.
Various algorithms have been proposed to solve exploration / exploitation or bandit problems.
Classification is a fundamental task of machine learning, and is by now well understood in its basic variants.
Approximation algorithms and heuristic approximations are commonly used to speed up the running time of algorithms in machine learning and data analysis.
The concept of parsimony is central in many scientific domains.
This paper addresses the problem of ranking a set of objects based on a limited number of pairwise comparisons (rankings between pairs of the objects).
In the learning literature, there are several previous results on learning probabilistic tree models, including various Expectation Maximization-based inference algorithms.
Clustering, a fundamental and ubiquitous problem in machine learning, is the task of organizing data points into homogenous groups using a given measure of similarity.
In this paper we consider the problem of selecting correlated variables in a high dimensional space.
Nonparametric Bayesian latent variable models have recently gained remarkable popularity in statistics and machine learning, partly owning to their desirable “nonparametric” nature which allows practitioners to “sidestep” the difficult model selection problem, e.
Bayesian accounts of cortical processing posit that the brain implements a probabilistic model to learn and reason about the causes underlying sensory inputs.
With the ever-growing amount of digital image data in multimedia databases, there is a great need for algorithms that can provide effective semantic indexing.
Learning to classify an object into a relevant target class surfaces in many domains such as document categorization, object recognition in computer vision, and web advertisement.
The explosion of digital sensing technology has unleashed a veritable data deluge that has pushed current signal processing algorithms to their limits.
Multi-structure model fitting is concerned with estimating the multiple instances (or structures) of a geometric model embedded in the input data.
Much recent research has focused on training deep, multi-layered networks of feature extractors applied to challenging visual tasks like object recognition.
Real world problems often produce high dimensional features with sophisticated statistical dependency structures.
Consider a clinical problem with M subpopulations, in which one should decide between Km options for treating subjects from each subpopulation m.
Continuous-time stochastic models play a prominent role in many scientific fields, from biology to physics to economics.
Consider the problem of estimating signal using noisy observation under the model: f (t) = cg(a t − φ) + e(t) , where the random quantities c ∈ R is the scale, a ∈ R is the rate, φ ∈ R is the phase shift, and e(t) ∈ R is the additive noise.
This study considers whether recurrent networks of spiking neurons can be seen as a generative model not only of stationary patterns but also of temporal sequences.
Visual recognition is a fundamental computer vision problem that demands sophisticated image representations—due to both the large number of object categories a system should ultimately recognize, as well as the noisy cluttered conditions in which training examples are often captured.
Fitted value iteration (FVI), both in the model-based [4] and model-free [5, 15, 16, 17] settings, has become a method of choice for various applied batch reinforcement learning problems.
Tensors (multi-way arrays) generalize matrices and naturally represent data having more than two modalities.
Many machine learning applications involve planning under uncertainty.
The problem of finding the best balanced cut of a graph is an important problem in computer science [9, 24, 13].
Unsupervised feature learning has recently emerged as a viable alternative to manually designing feature representations.
We consider the problem of sensing an unknown function f : X → R (where X ⊂ Rd ), where f belongs to span of a large set of (known) features {ϕk }1≤k≤K of L2 (X ): f (x) = K � αk ϕk (x), k=1 def where α ∈ RK is the unknown parameter, and is assumed to be S-sparse, i.
Sparse inference has found numerous applications in statistics and machine learning [1, 2, 3].
We consider the domain adaptation scenarios where we have very few or no labeled data from target domain but a large amount of labeled data from multiple related source domains with different data distributions.
The exponential family of distributions is ubiquitous in statistical machine learning.
We begin with the generative model Y = ΦX0 + E, (1) m×t where Φ ∈ R is a dictionary of basis vectors or features, X0 ∈ R is a matrix of unknown coefficients we would like to estimate, Y ∈ Rn×t is an observed signal matrix, and E is a noise matrix with iid elements distributed as N (0, λ).
In recent years the importance of taking advantage of the structure of convex optimization problems has become a topic of intense research in the machine learning community.
The application of non-parametric Bayesian techniques to time series has been an active field in the recent years, and has led to many successful continuous time models.
Recent advances in manifold learning and nonlinear dimensionality reduction have led to powerful, new methods for the analysis and visualization of high dimensional data [14, 1, 20, 24, 16].
This paper considers the problem of stochastic convex optimization under bandit feedback which is a generalization of the classical multi-armed bandit problem, formulated by Robbins in 1952.
Calculating marginal probabilities for a graphical model generally requires summing over exponentially many states, and is NP-hard in general [1].
Deformable Part Models (DPMs) deliver state-of-the-art object detection results [4] on challenging benchmarks when trained discriminatively, and have become a standard in object recognition research.
Brain computer interfaces (BCIs) allow people to control devices directly using brain signals [19].
In standard formulations of prediction problems, it is assumed that the covariates are fully-observed and sampled independently from some underlying distribution.
Nonlinear probabilistic modeling of high dimensional time series data is a key challenge for the machine learning community.
Image segmentation, a partitioning of an image into disjoint regions such that each region is homogeneous, is an important preprocessing step for many of the state-of-the-art algorithms for high-level image/scene understanding for three reasons.
We study the problem of learning to rank from pairwise preferences, and solve an open problem that has led to development of many heuristics but no provable results.
Problems with high dimensionality have become common over the recent years.
Q-learning [20] is a well-known model-free reinforcement learning (RL) algorithm that finds an estimate of the optimal action-value function.
The sparse Principal Component Analysis (Sparse PCA) problem is a variant of the classical PCA problem, which accomplishes a trade-off between the explained variance along a normalized vector, and the number of non-zero components of that vector.
Multiple Instance Learning (MIL) is a variation of the classical learning methods for problems with incomplete knowledge on the instances (or examples) [4].
In each trial of a standard visual short-term memory (VSTM) experiment (e.
Recently, compressive sensing [5] and sparse representation [19] have been hot research topics and also have found abundant applications in signal processing and machine learning.
Multiarmed bandits with side information are an elegant mathematical model for many real-life interactive systems, such as personalized online advertising, personalized medical treatment, and so on.
“Reinforcement learning” and “negative-feedback models of homeostatic regulation” are two control theory-based computational frameworks that have had major contributions in learning and motivation literatures in behavioral psychology, respectively.
Graphical models are a central tool in modern machine learning applications, as they provide a natural methodology for succinctly representing high-dimensional distributions.
The proliferation of social networks on the web has spurred many significant advances in modeling networks [1, 2, 4, 12, 13, 15, 16, 26].
The objective of transfer in reinforcement learning (RL) [10] is to speed-up RL algorithms by reusing knowledge (e.
The belief propagation algorithm [1] was originally proposed as an efficient method for the exact computation in the inference with graphical models associated to trees; the algorithm has been extended to general graphs with cycles and called Loopy Belief Propagation (LBP) algorithm.
Comparing probability distributions is a fundamental task in statistical data processing.
As the scope of machine learning applications has increased, the complexity of the classification tasks that are commonly tackled has grown dramatically.
Over the last several decades, information theory [1, 2] has played a major role in our effort to understand the neural code in the brain [3, 4].
In this paper we consider the following rank minimization problems: min{f (X) : rank(X) ≤ r, X ∈ X ∩ Ω}, (1) min{f (X) + ν rank(X) : X ∈ X ∩ Ω} (2) X X for some r, ν ≥ 0, where X is a closed convex set, Ω is a closed unitarily invariant set in <m×n , and f : <m×n → < is a continuously differentiable function (for the definition of unitarily invariant set, see Section 2.
We consider optimization problems where the objective function is costly to evaluate and may be accessed only by evaluating it at requested points.
A particularly effective image representation has developed in recent years, formed by computing the statistics of oriented gradients quantized into various spatial and orientation selective bins.
Image categorization is the task of classifying an image as containing an objects from a predefined list of categories.
Local Coordinate Coding (LCC) [18] is a coding scheme that encodes the data locally so that any non-linear (α, β, p)-Lipschitz smooth function (see Definition 1 in Section 2 for details) over the data manifold can be approximated using linear functions.
Many real-world problems involve multiple related classificatrion/regression tasks.
Is it possible to achieve the same test performance as the best classifier in hindsight? The answer to this question is “probably not.
Multi-class image segmentation and labeling is one of the most challenging and actively studied problems in computer vision.
Online learning algorithms, which have received much attention in recent years, enjoy an attractive combination of computational efficiency, lack of distributional assumptions, and strong theoretical guarantees.
Animals and humans often use vision to find things: mushrooms in the woods, keys on a desk, a predator hiding in tall grass.
The minimization of an objective function which is only available through unbiased estimates of the function values or its gradients is a key methodological problem in many disciplines.
We design improved algorithms for Euclidean k-means in the streaming model.
Computing the appearance distance between two windows is a fundamental operation in a wide variety of computer vision techniques.
Probabilistic programming simplifies the development of probabilistic models by allowing modelers to specify a stochastic process using syntax that resembles modern programming languages.
The goal of matrix factorization (MF) is to approximate an observed matrix by a low-rank one.
The Chinese restaurant process (CRP) is a distribution on partitions of integers [2].
We focus on stochastic convex optimization problems of the form Z F (x; ξ)dP (ξ), minimize f (x) for f (x) := EP [F (x; ξ)] = x∈X (1) Ξ where X ⊆ Rd is a closed convex set, P is a probability distribution over Ξ, and F (· ; ξ) is convex for all ξ ∈ Ξ, so that f is convex.
Determining the major products of chemical reactions given the input reactants and conditions is a fundamental problem in organic chemistry.
We consider the problem of selecting the right state-representation in an average-reward reinforcement learning problem.
We are now able to record from hundreds—and very likely soon from thousands—of neurons in vivo.
Sparse modeling (Olshausen and Field, 1996; Aharon et al.
Outsourcing information processing to large groups of anonymous workers has been made easier by the internet.
Given a training set of normal events, the anomaly detection problem aims to identify unknown, anomalous events that deviate from the normal set.
Scene understanding is an important task in neural information processing systems.
We consider the problem of training statistical mixture models, in particular mixtures of Gaussians and some natural generalizations, on massive data sets.
The operation of neural circuits fundamentally depends on the capacity of neurons to perform complex, nonlinear mappings from their inputs to their outputs.
There is increasing evidence that visual cortex contains discrete patches involved in processing faces but not other objects [2, 3, 4, 5, 6, 7].
Samples of multivariate data are often connected with labels or parameters.
The recent progress in sensor technology has made possible a plethora of novel applications, which typically require increasingly large amount of multidimensional data, such as large-scale images, 3D video sequences, and neuroimaging data.
The idea that images can be hierarchically parsed into objects and their parts has a long history in computer vision, see for example [15].
According to the recently quite influential statistical approach to perception, our brain represents not only the most likely interpretation of a stimulus, but also its corresponding uncertainty.
On the basis of i.
Many machine learning algorithms implement empirical risk minimization or a regularized variant of it.
Much of machine learning research can be viewed as an exploration of ways to compensate for scarce prior knowledge about how to solve a specific task by extracting (usually implicit) knowledge from vast amounts of data.
Learning or inferring a hidden structure from discrete samples is a fundamental problem in data analysis, ubiquitous in a broad range of application fields.
Understanding the factors that influence the performance of a statistical procedure is a key step for finding a way to improve it.
The problem of causal induction is central to science, and is something at which people are remarkably skilled, especially given its apparent difficulty.
Kernel methods have long provided powerful tools for generalizing linear statistical approaches to nonlinear settings, through an embedding of the sample to a high dimensional feature space, namely a reproducing kernel Hilbert space (RKHS) [16].
Learning to produce temporal sequences is a general problem that the brain needs to solve.
Fitting parametric probabilistic models to data is a basic task in statistics and machine learning.
Variational Message Passing [20] is a message passing implementation of the mean-field approximation [1, 2], also known as variational Bayes (VB).
Kernel methods are widely used to address a variety of learning problems including classification, regression, structured prediction, data fusion, clustering and dimensionality reduction [22, 23].
Multi-electrode array recording and similar methods provide measurements of activity from dozens of neurons simultaneously, and thus allow unprecedented insights into the statistical structure of neural population activity.
Inexpensive RGB-D sensors that augment an RGB image with depth data have recently become widely available.
We consider the reinforcement learning problem in which one attempts to find a good policy for controlling a stochastic nonlinear dynamical system.
Stochastic approximation (online) approaches, such as stochastic gradient descent and stochastic dual averaging, have become the optimization method of choice for many learning problems, including linear SVMs.
Topic modeling holds much promise for improving the ways users search, discover, and organize online content by automatically extracting semantic themes from collections of text documents.
Dimensionality reduction methods are important for data analysis and processing, with their use motivated mainly from two considerations: (1) the impracticality of working with high-dimensional spaces along with the deterioration of performance due to the curse of dimensionality; and (2) the realization that many classes of signals reside on manifolds of much lower dimension than that of their ambient space.
The analysis of action potentials (“spikes”) from neural-recording devices is a problem of longstanding interest (see [21, 1, 16, 22, 8, 4, 6] and the references therein).
In this work we consider the problem of efficient object-class recognition in large image collections.
One of the central problems in statistics is the linear regression in which the goal is to accurately estimate a regression vector β ? ∈ Rp from the noisy observations y = Xβ ? + w, n×p (1) n where X ∈ R is the measurement or design matrix, and w ∈ R is the stochastic observation vector noise.
Consider the situation where we want to search and navigate a database, but the underlying relationships between the objects are unknown and are accessible only through a comparison oracle.
In the eighteen years since variational inference was first proposed for neural networks [10] it has not seen widespread use.
Owing to the great advancements in measurement technology, a huge amount of data is generated in the ﬁeld of science, engineering, and medicine, and accordingly, there is an increasing demand for estimating the hidden states underlying the observed signals.
User feedback has become an invaluable source of training data for optimizing information retrieval systems in a rapidly expanding range of domains, most notably content recommendation (e.
The Euclidean distance between two vectors x and y in d-dimensional space is a typical distance measure that reflects their proximity in the space.
Boosting is a popular approach to classifier design in machine learning.
Undirected graphical models, also known as Markov random fields, are used in a variety of domains, including statistical physics, natural language processing and image analysis among others.
In this paper we consider large-margin halfspace learning in the PAC model: there is a target halfspace f (x) = sign(w · x), where w is an unknown unit vector, and an unknown probability distribution D over the unit ball Bn = {x ∈ Rn : kxk2 ≤ 1} which has support on {x ∈ Bn : |w·x| ≥ γ}.
The notion of “representativeness” appeared in cognitive psychology as a proposal for a heuristic that people might use in the place of performing a probabilistic computation [1, 2].
Visual 3D scene understanding is an important component in applications such as autonomous driving and robot navigation.
There is an increasing tendency to consider machine learning as a problem in optimization: define a loss function, add constraints and/or regularizers and formulate it as a preferably convex program.
Machine learning has become a central tool in areas such as speech recognition, natural language translation, machine question answering, and visual object detection.
Semantic segmentation (i.
Coding efficiency is a well-known objective for the evaluation and design of signal processing systems, and provides a theoretical framework for understanding biological sensory systems.
Classification problems with many classes arise in many important domains and pose significant computational challenges.
The problem of learning-by-example has the promise to create strong models from a restricted number of cases; certainly humans show the ability to generalize from limited experience.
One of the fundamental questions in computational neuroscience is how synapses are modified by neural activity [1, 2].
Most existing analyses of active learning are based on an i.
One of the main goals of scene understanding is the semantic segmentation of images: label a diverse set of object properties, at multiple scales, while at the same time identifying the spatial extent over which such properties hold.
The goal in matrix factorization is to recover a low-rank matrix from irrelevant noise and corruption.
In this paper we address the task of nonnegative dictionary learning described by V ≈ W H, (1) where V , W , H are nonnegative matrices of dimensions F × N , F × K and K × N , respectively.
Matrix-variate normal (MVN) models have important applications in various fields.
In the papers [1, 10, 11], an array of tools has been developed to study the minimax value of diverse sequential problems under the worst-case assumption on Nature.
Determining interactions between entities based on observations is a major challenge when analyzing biological and social network data [1, 12, 15].
We consider the extension of the classical online problem of predicting outcomes from a finite alphabet to the matrix domain.
In many machine learning problems, one is often confronted with very high dimensional data.
As time progresses, the choices humans make often change.
Value functions are an essential concept for determining optimal policies in both optimal control [1] and reinforcement learning [2, 3].
The oft used linear regression paradigm models a dependent variable Y as a linear function of a vector-valued independent variable X.
Log-linear models, also known as maximum entropy models or multiclass logistic regression, have found a wide range of applications in machine learning.
Human actions are ubiquitous and represent essential information for understanding the content of many still images such as consumer photographs, news images, sparsely sampled surveillance videos, and street-side imagery.
Increasingly, optimization problems in machine learning are very high-dimensional, where the number of variables is very large.
Consider building a sofa detector using a database of annotated images containing sofas and many other classes, as shown in Figure 1.
Most statistical regression models in use today are of the form: g(y) = f (x1 )+f (x2 )+· · ·+f (xD ).
Many sequential decision-making problems are commonly modelled as an extensive form game.
While being rooted in information retrieval [1], the so-called F-measure is nowadays routinely used as a performance metric for different types of prediction problems, including binary classification, multi-label classification (MLC), and certain applications of structured output prediction, like text chunking and named entity recognition.
Crowdsourcing is becoming an increasingly important methodology for collecting labeled data, as demonstrated among others by Amazon Mechanical Turk, reCAPTCHA, Netflix, and the ESP game.
Penalized likelihood estimation has evolved into a major area of research, with `1 [22] and other regularization penalties now used routinely in a rich variety of domains.
Machine learning algorithms have found applications in diverse domains such as computer vision, bio-informatics and speech recognition.
The graph Laplacian is a popular tool for unsupervised and semi-supervised learning problems on graphs.
A central problem in systems neuroscience is to understand the probabilistic relationship between sensory stimuli and neural responses.
We derive new rates of convergence in terms of dimension for the popular approach of Nearest Neighbor (k-NN) regression.
This paper considers the model combination problem, where the goal is to combine multiple models in order to achieve improved accuracy.
Renewal processes are stochastic point processes on the real line where intervals between successive points (times) are drawn i.
In undirected graphical models (UGMs), a graph is defined as G = (V, E), where V = {1, · · · , K} is the set of nodes and E ⊂ V × V is the set of edges between the nodes.
Decision-theoretic online learning (DTOL) is a framework to capture learning problems that proceed in rounds.
Manifold Embedding In many areas of machine learning, pattern recognition, information retrieval and computer vision, we are confronted with high-dimensional data that lie in or close to a manifold of intrinsically lowdimension.
With machine learning comes the question of how to effectively teach.
Statistical models of natural signals have provided a rich framework to describe how sensory neurons process and adapt to ecologically-valid stimuli [28, 12].
Episodic memory such as that in the hippocampus acts like a palimpsest – each new entity to be stored is overlaid on top of its predecessors, and, in turn, is submerged by its successors.
In temporal prediction tasks, Temporal Difference (TD) learning provides a method for learning long-term expected rewards (the “value function”) using only trajectories from the system.
Partially observable Markov decision process (POMDP) provides a principled general framework for planning with imperfect state information.
We address the problem of recovering a low-rank m-by-n matrix X of which a few entries are observed, possibly with noise.
One of the most common approaches to collaborative filtering and matrix completion is trace-norm regularization [1, 2, 3, 4, 5].
Biclustering is the problem of identifying a (typically) sparse set of relevant columns and rows in a large, noisy data matrix.
In a range of applications, a dynamic decision making problem exhibits two distinctly different kinds of phases: experimentation and commitment.
Research in multi-label classification has seen a substantial growth in recent years (e.
In Machine Learning, model quality is most often limited by the lack of sufficient training data.
In cluster analysis we are concerned with identifying subsets of n objects that share some similarity and therefore potentially belong to the same sub-population.
It has long been argued that many key questions in neuroscience can best be posed in informationtheoretic terms; the efficient coding hypothesis discussed in [2, 3, 1], represents perhaps the bestknown example.
The design of effective approximate inference methods for continuous variables often requires considering the curvature of the target distribution.
Consider approximating a p-dimensional data point x by a linear combination x ≈ Bw of m (possibly linearly dependent) codewords in a dictionary B = [b1 , b2 , .
Planning research typically assumes that the planning system is provided complete and correct models of the actions.
The on-line domain is a rich environment for observing social contagion — the tendency of new information, ideas, and behaviors to spread from person to person through a social network [1, 4, 6, 10, 12, 14, 17, 19].
Combinatorial optimization techniques have been actively applied to many machine learning applications, where submodularity often plays an important role to develop algorithms [10, 16, 27, 14, 15, 19, 1].
Low-rank matrix recovery is the following problem: let M be some unknown matrix of dimension d and rank r  d, and let A1 , A2 , .
Anomaly detection is a crucial problem in processing large-scale data sets when our goal is to find rare or unusual events.
Producing a relevant and accurate caption for an arbitrary image is an extremely challenging problem, perhaps nearly as difficult as the underlying general image understanding task.
The motivation of this paper is to understand the intrinsic structure and properties of suitable loss functions for the problem of multiclass prediction, which includes multiclass probability estimation.
The problem of finding maximal cliques in a weighted graph is faced in many applications from computer vision to social networks.
Inverse reinforcement learning (IRL) methods learn a reward function in a Markov decision process (MDP) from expert demonstrations, allowing the expert’s policy to be generalized to unobserved situations [7].
The contemporary problem of exploring huge collections of discrete data, from biological sequences to text documents, has prompted the development of increasingly sophisticated statistical models.
Consider the problem of learning to optimize a complex system subject to varying environmental conditions.
In cluster analysis, the objective is to segment a dataset into subgroups, such that data points in the same subgroup are more similar to each other (in a sense that will be specified) than to those in other subgroups.
Visual recognition is a major focus of research in computer vision, machine learning, and robotics.
The task of recovering intrinsic images is to separate a given input image into its material-dependent properties, known as reflectance or albedo, and its light-dependent properties, such as shading, shadows, specular highlights, and inter-reflectance.
In many regression/classification problems, the features exhibit certain hierarchical or structural relationships, the usage of which can yield an interpretable model with improved regression/classification performance [25].
Image categorization / object recognition has been one of the most important research problems in the computer vision community.
Most scene understanding tasks (e.
According to [18], more than 33 million people worldwide are infected with the human immunodeficiency virus (HIV), for which there exists no cure.
The current trend towards ‘big data’ has created a strong demand for techniques to efficiently extract structure from ever-accumulating unstructured datasets.
Privacy-preserving data analysis has received increasing attention in recent years.
The analysis of the structure and evolution of network data is an increasingly important task in a variety of disciplines, including biology and engineering.
Causal discovery refers to a special class of statistical and machine learning methods that infer causal relationships between variables from data and prior knowledge [1, 2, 3].
There exist many sources of unstructured data that have been partially or completely categorized by human editors.
Probabilistic graphical models provide a compact and principled representation for capturing complex statistical dependencies among a set of random variables.
Intelligent decision making in real-world scenarios requires an agent to take into account its limitations in sensing and actuation.
The identification of individual spikes in extracellularly recorded voltage traces is a critical step in the analysis of neural data for much of systems neuroscience.
Background.
Domain adaptation addresses the problem of generalizing from a source distribution for which we have ample labeled training data to a target distribution for which we have little or no labels [3, 14, 28].
One of the central problems in systems neuroscience is to understand how neural spike responses convey information about environmental stimuli, which is often called the neural coding problem.
While difficulty adjustment is common practise in many traditional games (consider, for instance, the handicap in golf or the handicap stones in go), the case for dynamic difficulty adjustment in electronic games has been made only recently [7].
Probabilistic models of text and topics, known as topic models, are powerful tools for exploring large data sets and for making inferences about the content of documents.
As problem domains become more complex, human guidance becomes increasingly necessary to improve agent performance.
Predictive analysis of networked data is a fast-growing research area whose application domains include document networks, online social networks, and biological networks.
The problem of modeling temporal dependencies in temporal streams of discrete events arises in a wide variety of applications.
Recently, sparse coding [3, 18] has attracted much attention in computer vision research.
We are interested in probablistic models for sequences arising from the study of genetic variations in a population of organisms (particularly humans).
Over the last twenty years, researchers have used a number of unsupervised learning algorithms to model a range of neural phenomena in early sensory processing.
Amplitude and frequency demodulation (AFD) is the process by which a signal (yt ) is decomposed into the product of a slowly varying envelope or amplitude component (at ) and a quickly varying sinusoidal carrier (cos(φt )), that is yt = at cos(φt ).
A fruitful modelling approach for extracting meaningful information from highly structured multivariate datasets is based on matrix factorisations (MFs).
Not all Vapnik-Chervonenkis classes are created equal.
Individuals are often asked to convey their opinions and sentiments in the form of quantitative judgments.
We study the optimization of an unknown function f by requesting n experiments, each specifying an input x and producing a noisy observation of f (x).
A Restricted Boltzmann Machine (RBM) [24, 10] is a learning system consisting of two layers of binary stochastic units, a hidden layer and a visible layer, with a complete bipartite interaction graph.
Semidefinite programming (SDP) has become a tool of great importance in optimization in the past years.
Sparsity has been shown to work well for learning feature representations that are robust for object recognition [1, 2, 3, 4, 5, 6, 7].
Algorithms for filtering and prediction have a venerable history studded by quantum leaps by Wiener, Kolmogorov, Mortensen, Zakai, Duncan among others.
The formalism of probabilistic graphical models can be employed to represent dependencies among a large set of random variables in the form of a graph [1].
Linear stochastic bandit problem is a sequential decision-making problem where in each time step we have to choose an action, and as a response we receive a stochastic reward, expected value of which is an unknown linear function of the action.
Gaussian Markov Random Fields; Covariance Estimation.
Is it possible to leverage the solution of one classification problem to solve another? This is a question that has received increasing attention in recent years from the machine learning community, and has been studied in a variety of settings, including multi-task learning, covariate shift, and transfer learning.
We consider a stochastic convex optimization problem of the form minw∈W L(w), where L(w) = Ez [�(w, z)], based on an empirical sample of instances z1 , .
When are the objects in two images the same?1 Although people recognize and categorize objects successfully and effortlessly, object recognition in machine learning is an incredibly difficult problem and people’s success is a puzzle to cognitive scientists.
Monte-Carlo Tree Search (MCTS) has become a popular approach for decision making in large domains.
One distinguishing property of life is its temporal dynamics, and it is hence only natural that time lapse experiments play a crucial role in current research on signaling pathways, drug discovery and developmental biology [17].
The goal of reinforcement learning (RL) is to find an optimal decision-making policy that maximizes the return (i.
In a multi-armed bandit (MAB) problem, a player is presented with a sequence of trials.
Hidden Markov Models (HMM) provide one of the simplest examples of structured data observed through a noisy channel.
The last several years has revealed a new trend in Machine Learning: prediction and learning problems rolled into prize-driven competitions.
Belief propagation (BP) has become the standard procedure to decode channel codes, since in 1996 MacKay [7] proposed BP to decode codes based on low-density parity-check (LDPC) matrices with linear complexity.
Multiple Kernel Learning (MKL) proposed by [20] is one of the most promising methods that adaptively select the kernel function in supervised kernel learning.
A clustering is defined as a partitioning of a set of elements into subsets called clusters.
The recent development of conditional random fields (CRFs) [1], max-margin Markov networks (M3Ns) [2], and structured support vector machines (SSVMs) [3] has triggered a wave of interest in the prediction of complex outputs.
In fields such as ecology, marketing, and the social sciences, data about identifiable individuals is rarely available, either because of privacy issues or because of the difficulty of tracking individuals over time.
Consider a polling institute that has to estimate as accurately as possible the average income of a country, given a ﬁnite budget for polls.
It is well known that speech conveys various yet mixed information where there are linguistic information, a major component, and non-verbal information such as speaker-specific and emotional components [1].
Models such as Deep Belief Networks (DBNs) [2], stacked denoising autoencoders [3], convolutional networks [4], as well as classifiers based on sophisticated feature extraction techniques have from ten to perhaps fifty hyper-parameters, depending on how the experimenter chooses to parametrize the model, and how many hyper-parameters the experimenter chooses to fix at a reasonable default.
Suppose an observation y ∈ Rm is available where y = Ax + u + Dξ.
Fundamental to describing the behavior of neurons in response to sensory stimuli or to inputs from other neurons is the need for succinct models that can be estimated and validated with limited data.
Multi-class Gaussian process classifiers (MGPCs) are a Bayesian approach to non-parametric multiclass classification with the advantage of producing probabilistic outputs that measure uncertainty in the predictions [1].
Tracking human 3D articulated motions from video sequences is well known to be a challenging machine vision problem.
Metric learning (ML), which aims at learning dissimilarities by determining the importance of different input features and their correlations, has become a very active research field over the last years [23, 5, 3, 14, 22, 7, 12].
Graphical models are useful for representing relationships between large numbers of random variables in probabilistic models spanning a wide range of applications, including information extraction and data integration.
In many tasks, bounded resources and physical constraints force decisions to be made based on limited information [1, 2].
Many real-world datasets have representations in the form of multiple views [1, 2].
In many areas of application, problems are naturally expressed as a Gibbs measure, where the distribution over the domain X is given by, for x ∈ X : q(x) = X q̃(x) exp{−βE(x)} = , with Z(β) = q̃(x).
We propose a novel and general method to approximate the partition function of intricate probability distributions defined over combinatorial spaces.
One of the most basic learning settings studied in the online learning framework is learning from experts.
Least-squares analysis was introduced by Gauss in 1795 and has since has bloomed into a staple of the data analyst.
Boosting is the task of converting inaccurate weak learners into a single accurate predictor.
Two-sample hypothesis tests are concerned with the question of whether two samples of data are generated from the same distribution.
Consider a gambler who is trying to predict the next bit in a sequence of bits.
Learning an unknown halfspace from labeled examples that satisfy a margin constraint (meaning that no example may lie too close to the separating hyperplane) is one of the oldest and most intensively studied problems in machine learning, with research going back at least five decades to early seminal work on the Perceptron algorithm [5, 26, 27].
With its small memory footprint, robustness against noise, and rapid learning rates, Stochastic Gradient Descent (SGD) has proved to be well suited to data-intensive machine learning tasks [3, 5, 24].
Due to its fast query speed and low storage cost, hashing [1, 5] has been successfully used for approximate nearest neighbor (ANN) search [28].
The work of Benjamin Libet [1, 2] and others [3, 4] has challenged our intuitive notions of the relation between decision making and conscious voluntary action.
The accurate prediction of molecular energetics in chemical compound space (CCS) is a crucial ingredient for compound design efforts in chemical and pharmaceutical industries.
We begin with the likelihood model y = Φx + ǫ, (1) where Φ ∈ Rn×m is a dictionary of unit ℓ2 -norm basis vectors, x ∈ Rm is a vector of unknown coefficients we would like to estimate, y ∈ Rn is the observed signal, and ǫ is noise distributed as N (ǫ; 0, λI) (later we consider more general likelihood models).
In real-world applications of visual object recognition, performance is time-sensitive.
Dimension reduction is involved in most of modern data analysis, in which high dimensional data must be handled.
The uncertainty of plan execution can be modeled by using probabilistic effects in actions, and therefore the probability of reaching different states from a given state and action.
In this paper, we consider the problem of debugging pipelines consisting of a set of data processing operators.
The multi-label classification problem is an extension of the traditional multiclass classification problem.
Nearest neighbor search, a.
Estimating semantic 3D information from monocular images is an important task in applications such as autonomous driving and personal robotics.
The probabilistic generative model is an important tool for statistical learning because it enables rich data to be explained in terms of simpler latent structure.
The Gaussian process (GP) is a popular nonparametric Bayesian method for nonlinear regression.
Datasets from a wide range of modern research areas are increasingly high dimensional, which presents a number of theoretical and practical challenges.
Let {x1 , x2 , .
Gaussian processes (GPs) [1] have been popular in the NIPS community for a number of years now, as one of the key non-parametric Bayesian inference approaches.
Many uses of graphical models either directly employ chains or tree structures—as in part-of-speech tagging—or employ them to enable inference in more complex models—as in junction trees and tree block coordinate descent [1].
Markov logic [1] is a language for statistical relational learning, which employs weighted first-order logic formulas to compactly represent a Markov random field (MRF) or a conditional random field (CRF).
This paper proposes a new algorithm for the following task: given a sparse undirected unweighted graph, partition the nodes into disjoint clusters so that the density of edges within clusters is higher than the edges across clusters.
In many applications, labeled data are expensive and time consuming to obtain while unlabeled data are abundant.
Networks have been powerful abstractions for modeling a variety of natural and artificial systems that consist of a large collection of interacting entities.
Regularizing with the `1 norm, when we expect a sparse solution to a regression problem, is often justified by kwk1 being the “convex envelope” of kwk0 (the number of non-zero coordinates of a vector w ∈ Rd ).
Cortical thickness measures the distance between the outer and inner cortical surfaces (see Fig.
In order to leverage the large amount of available unlabeled text, a lot of research has been devoted to developing good probabilistic models of documents.
Characterizing the statistical features of spike time sequences in the brain is important for understanding how the brain represents information about stimuli or actions in the sequences of spikes.
Traditional computer vision algorithms, particularly those that exploit various probabilistic and learning-based approaches, are often formulated in centralized settings.
Object recognition is one of the hardest problems in computer vision and important for making robots useful in home environments.
The performance of (semi-)supervised learning methods typically depends on the amount of labeled data.
Multi-task learning (MTL) exploits the relationships among multiple related tasks to improve the generalization performance.
There is a growing need for statistical models which can capture rich dependencies in structured data.
The accuracy of the items placed near the top is crucial for many information retrieval systems such as search engines or recommendation systems, since most users of these systems browse or consider only the first k items.
d Learning functions f : x → y based on training data (yi , xi )m i=1 : R × R is a fundamental problem with many scientific and engineering applications.
Statistical Relational Learning (SRL) [7] aims at modeling data consisting of relations between entities.
We consider optimization problems of the following form, p∗ = min x∈C, 1T x=1, x≥0 f (x) + λcard(x) where f is a convex function, C is a convex set, card(x) denotes the number of nonzero elements of x and λ ≥ 0 is a given tradeoff parameter for adjusting desired sparsity.
When estimating a quantity consisting of two elements, a two-stage approach of first estimating the two elements separately and then approximating the target quantity based on the estimates of the two elements often performs poorly, because the first stage is carried out without regard to the second stage and thus a small estimation error incurred in the first stage can cause a big error in the second stage.
Problem description.
Preference learning is concerned with making inference from data consisting of pairs of items and corresponding binary labels indicating user preferences.
There has been signiﬁcant interest and progress in recent years in understanding consistency of learning methods for various ﬁnite-output learning problems, such as binary classiﬁcation, multiclass 0-1 classiﬁcation, and various forms of ranking and multi-label prediction problems [1–15].
Brain Computer Interfaces interpret brain signals to allow direct man-machine communication [17].
Experimental findings from neuro- and cognitive sciences have led to the hypothesis that humans create and maintain an internal model of their environment in neuronal circuitry of the brain during learning and development [1, 2, 3, 4], and employ this model for Bayesian inference in everyday cognition [5, 6].
Reinforcement learning is focused on learning optimal policies from trajectories of data.
Inverse reinforcement learning (IRL) [14] consists in finding a reward function such that a demonstrated expert behavior is optimal.
Problems of learning with rank-based error metrics [16] and the adoption of learning for the purpose of rank aggregation in social choice [7, 8, 23, 25, 29, 30] are gaining in prominence in recent years.
Probabilistic graphical models are widely used in a variety of applications, from computer vision to natural language processing to computational biology.
Large-scale matrices emerging from stocks, genomes, web documents, web images and videos everyday bring new challenges in modern data analysis.
There has been increasing interest in count modeling using the Poisson process, geometric process [1, 2, 3, 4] and recently the negative binomial (NB) process [5, 6].
Network analysis methods such as MMSB [1], ERGMs [20], spectral clustering [17] and latent feature models [12] require the adjacency matrix A of the network as input, reflecting the natural assumption that networks are best represented as a set of edges taking on the values 0 (absent) or 1 (present).
Many machine learning algorithms use graphs as input, such as clustering [16, 14], manifold based dimensional reduction [2, 15], and graph-based semi-supervised learning [23, 22].
Receptor neurons in early sensory systems are more numerous than the projection neurons that transmit sensory information to higher brain areas, implying that sensory signals must be compressed to pass through a limited bandwidth channel known as “Barlow’s bottleneck” [1].
A key challenge in many time series applications is capturing long-range dependencies for which Markov-based models are insufficient.
Optimizing large-scale complex systems often requires the tuning of many parameters.
Directly specifying desired behaviors for automated agents is a difficult and time consuming process.
Computing the Shannon entropy in massive data have important applications in neural computation [17], graph estimation [5], query logs analysis in Web search [14], network anomaly detection [21], etc.
Undirected graphical models, also known as Markov random fields, are an important class of statistical models that have been extensively used in a wide variety of domains, including statistical physics, natural language processing, image analysis, and medicine.
SAT was originally shown to be a canonical NP-complete problem in Cook’s seminal work [5].
The expected return is often the objective function of choice in planning problems where outcomes not only depend on the actor’s decisions but also on random events.
Finding a subset of data points, called representatives or exemplars, which can efficiently describe the data collection, is an important problem in scientific data analysis with applications in machine learning, computer vision, information retrieval, etc.
The high volume and velocity of social media, such as blogs and Twitter, have propelled them to the forefront as sources of breaking news.
The analysis of legislative roll-call data provides an interesting setting for recent developments in the joint analysis of matrices and text [23, 8].
There is an increasing interest in using crowdsourcing to collect labels for machine learning [19, 6, 21, 17, 20, 10, 13, 12].
Clustering analysis as a discrete optimization problem is usually NP-hard.
State-space models (SSMs) are widely used to model time series and dynamical systems.
Graphical models are a common method to describe the dependencies of a joint probability distribution over a set of discrete random variables.
In this paper, we focus on scalable parallelization of Monte Carlo simulation, a problem motivated by the increasingly large inference problems occurring in a variety of fields in science and engineering.
Variable selection plays a fundamental role in statistical modeling for high-dimensional data sets, especially when the underlying model has a sparse representation.
The ability of neurons to adapt their responses to greatly varying sensory signal statistics is central to efficient neural coding [1, 2, 3, 4, 5, 6, 7].
One of the goals of multi-set data analysis is forming qualitative comparisons between datasets.
In a Bayesian setting, the Gaussian process (GP) is commonly used to define a prior probability distribution over functions.
Problems of dynamic optimization in the face of uncertainty are frequently posed as Markov decision processes (MDPs).
The question of similarity between two sets of examples is common to many ﬁelds, including statistics, data mining, machine learning and computer vision.
Fast clustering algorithms are a staple of exploratory data analysis.
A number of problems in Computer Vision, Natural Language Processing and Computational Biology involve predictions over complex but structured interdependent outputs, also known as structured-output prediction.
In this work we consider multiclass prediction: The problem of classifying objects into one of several possible target classes.
Data clustering is a data analysis methodology which aims to automatically reveal the underlying structure of data.
The problem of learning multiple classes from data with imprecise label information has attracted a recent attention in the literature.
Nonnegative matrix factorization (NMF) is a popular approach for selecting features in data [16–18, 23].
Partially-observable Markov decision processes (POMDPs) are a powerful modeling formalism for real-world sequential decision-making problems [3].
Discriminative learning algorithms are typically trained from large collections of vectorial training examples.
Given an infinite-horizon stationary γ-discounted Markov Decision Process [24, 4], we consider approximate versions of the standard Dynamic Programming algorithms, Policy and Value Iteration, that build sequences of value functions vk and policies πk as follows vk+1 ← T vk + k+1 Approximate Value Iteration (AVI):  Approximate Policy Iteration (API): vk πk+1 ← vπ k +  k ← any element of G(vk ) (1) (2) where v0 and π0 are arbitrary, T is the Bellman optimality operator, vπk is the value of policy πk and G(vk ) is the set of policies that are greedy with respect to vk .
Human activity understanding has been a research topic of substantial interest in computer vision [1].
Additive Models are a class of nonparametric regression methods which have been the subject of intensive theoretical research and found widespread applications in practice (see [1]).
Neuroimaging is a powerful tool for characterizing neurodegenerative process in the progression of Alzheimer’s disease (AD).
With the increasing amount of data that is available for training, it becomes an urgent task to devise efficient algorithms for optimization/learning problems with unprecedented sizes.
Normative theories of human choice behavior have long been based on how economic theory has postulated they should be made.
How many processors should we use and how often should they communicate for large-scale distributed optimization? We address these questions by studying the performance and limitations of a class of distributed algorithms that solve the general optimization problem m minimize F (x) = x∈X 1 X lj (x) m j=1 (1) where each function lj (x) is convex over a convex set X ⊆ Rd .
We address situations in which an informed choice between candidate predictive models—for instance, a baseline method and a challenger—has to be made.
Discrete undirected graphical models have seen wide use in natural language processing [11, 24] and computer vision [19].
Temporal-difference (TD) learning is a widely used method in reinforcement learning (RL).
Covariance selection, first described in [2], has come to refer to the problem of estimating a normal distribution that has a sparse inverse covariance matrix P, whose non-zero entries correspond to edges in an associated Gaussian Markov Random Field, [3].
Observed image signals are often corrupted by acquisition channel or artificial editing.
Information in the real world comes through multiple input channels.
Linear models are widely used for a variety of tasks including classification and regression.
Kernel methods [16], such as support vector machines, are among the most effective learning methods.
Ranking is a central problem in many applications, such as document retrieval, meta search, and collaborative filtering.
The considered problem has a simple formulation: Given are multiple similarities between the same set of n data points, each similarity can be represented as a weighted graph.
The nearest neighbor (NN) classifier is one of the simplest and most classical non-linear classification algorithms.
The motivating hypothesis behind multi-task learning (MTL) algorithms is that leveraging data from related tasks can yield superior performance over learning from each task independently.
Recent progress in voltage clamp techniques has enabled the recording of local membrane voltage in dendritic branches, and this greatly changed our view of the potential for single neuron computation.
Every year, more than 34,000 suicides occur and over 370,000 individuals are treated for selfinflicted injuries in emergency rooms in the U.
Learning a model of a high-dimensional environment can pose a significant challenge, but the ability to make predictions about future events is key to good decision making.
Multi-Agent Plan Recognition (MAPR) seeks an explanation of observed team-action traces.
Latent variable models have shown great success in various fields, including computational linguistics and machine learning.
We consider the problem of nonparametrically estimating the Shannon mutual information between two random variables.
Many risk minimization problems in machine learning can be formulated into a regularized stochastic optimization problem of the following form: minx∈X {φ(x) := f (x) + h(x)}.
An important statistical problem in the study of natural systems is to estimate the entropy of an unknown discrete distribution on the basis of an observed sample.
Machine learning algorithms are rarely parameter-free: parameters controlling the rate of learning or the capacity of the underlying model must often be specified.
With the tremendous growth of data, providing a multi-granularity conceptual view using hierarchical classification (HC) has become increasingly important.
How to compare examples is a fundamental question in machine learning.
Diffusion processes are a flexible and useful tool in stochastic modelling.
Bayesian nonparametric time series models, including various “infinite” Markov switching processes [1, 2, 3], provide a promising modeling framework for complex sequential data.
The rapidly growing number of products available online makes it increasingly difficult for users to choose the ones worth their attention.
In the multivariate regression problem the objective is to estimate the conditional mean E(Y ∣ X) = m(X) = (m1 (X), .
We are given a sequence x := X1 , X2 , .
The last few years have seen a tremendous interest in the study, understanding and statistical modeling of complex networks [14, 6].
The multi–armed bandit [13] elegantly formalizes the problem of on–line learning with partial feedback, which encompasses a large number of real–world applications, such as clinical trials, online advertisements, adaptive routing, and cognitive radio.
A plethora of the problems arising in machine learning involve computing an approximate minimizer of the sum of a loss function over a large number of training examples, where there is a large amount of redundancy between examples.
Consider sentiment analysis task for a set of reviews for different products.
The fitting of complex models to big data often requires computationally intractable integrals to be approximated.
Inverse reinforcement learning (IRL) aims to find the agent’s underlying reward function given the behaviour data and the model of environment [1].
Convex optimization has proved to be extremely useful to all quantitative disciplines of science.
Despite the anatomical and functional similarities between the primary auditory cortex (A1) and the primary visual cortex (V1), the computational modelling of A1 has proven to be less fruitful than V1, primarily because the responses of A1 cells are more disorganized.
The Bregman divergence first appeared in the context of relaxation techniques in convex programming ([4]), and has found numerous applications as a general framework in clustering ([2]), proximal minimization ([5]) and online learning ([27]).
Derivative-free optimization schemes have a long history in optimization (see, for example, the book by Spall [21]), and they have the clearly desirable property of never requiring explicit gradient calculations.
Recent approaches to collaborative filtering (CF) have concentrated on estimating an algebraic or statistical model, and using the model for predicting the missing rating of user u on item i.
Extracting a 3D representation from a single-view image depicting a 3D object has been a longstanding goal of computer vision [20].
Clustering is a fundamental problem in data analysis, and has extensive applications in statistics, data mining, computer vision and even in social sciences.
Magnetic Resonance Imaging (MRI) is widely used for observing the tissue changes of the patients within a non-invasive manner.
Behavioral evidence shows that animal behaviors are often influenced not only by the content of sensory information but also by its uncertainty.
Markov Decision Processes (MDP) provide a rich and elegant mathematical framework for solving sequential decision-making problems.
Over the past several years, online convex optimization has emerged as a fundamental tool for solving problems in machine learning (see, e.
Forecasting systems behavior with multiple responses has been a challenging issue in many contexts of applications such as collaborative filtering, financial markets, or bioinformatics, where responses can be, respectively, movie ratings, stock prices, or activity of genes within a cell.
Local image descriptors have long been explored in the context of machine learning and computer vision.
Many problems of relevance in machine learning, signal processing, and high dimensional statistics can be posed in composite form: minimize f (x) := g(x) + h(x), n x∈R (1) where g : Rn → R is a convex, continuously differentiable loss function, and h : Rn → R is a convex, continuous, but not necessarily differentiable penalty function.
Partitioning data points into sensible groups is a fundamental problem in machine learning.
The increasing availability of genetic data (for example, from the Thousand Genomes project [1]) and the importance of genetics in scientific and medical applications requires the development of scalable and accurate models for genetic sequences which are informed by genetic processes.
Data in the form of partial rankings, i.
Variational bounds provide a convenient approach to approximate inference in a range of intractable models [Ghahramani and Beal, 2001].
Scalp recorded electroencephalography (EEG) can be used for non-muscular control and communication systems, commonly called brain-computer interfaces (BCI).
The two-alternative forced-choice (2AFC) task is a standard experimental paradigm used in psychology and neuroscience to investigate various aspects of sensory, motor, and cognitive processing [5].
A Deep Boltzmann Machine (DBM) is a type of binary pairwise Markov Random Field with multiple layers of hidden random variables.
In many real-world classification problems, the output labels are organized in a hierarchy.
Consider the classical problem of Gaussian linear regression1 : Y = Xβ ∗ + σ ∗ ξ, ξ ∼ Nn (0, In ), (1) where Y ∈ Rn and X ∈ Rn×p are observed, in the neoclassical setting of very large dimensional unknown vector β ∗ .
Whilst Bayesian methods have played a significant role in machine learning and related areas (see [1] for an introduction), improving the class of distributions for which inference is either tractable or can be well approximated remains an ongoing challenge.
The use of Markov chain Monte Carlo methods can be extremely challenging in many modern day applications.
The two sample problem addresses the question of whether two independent samples are drawn from the same distribution.
Mesh segmentation methods decompose a three-dimensional (3D) mesh, or a collection of aligned meshes, into their constituent parts.
The term “reservoir computing” encompasses a range of similar machine learning techniques, independently introduced by H.
Probabilistic methods based on Gaussian densities have celebrated successes throughout machine learning.
Rank aggregation is an important task in a wide range of learning and social contexts arising in recommendation systems, information retrieval, and sports and competitions.
Linear programming is a fundamental mathematical model with numerous applications in both combinatorial and continuous optimization.
The development of successful methods for training deep architectures have influenced the development of representation learning algorithms either on top of SIFT descriptors [1, 2] or raw pixel input [3, 4, 5] for feature extraction of full-sized images.
Many problems in machine learning are based on a form of (regularized) empirical risk minimization.
Variational Bayesian (VB) approximation [1] was proposed as a computationally efficient alternative to rigorous Bayesian estimation.
The Lovász ϑ function [19] plays a fundamental role in modern combinatorial optimization and in various approximation algorithms on graphs, indeed Goemans was led to say It seems all roads lead to ϑ [10].
The Bag of Words (BoW) [7] is the de facto standard image feature for the image categorization.
In this paper we study the problem of learning from random samples a probability distribution supported on a manifold, when the learning error is measured using transportation metrics.
Anomaly detection is an important problem that has been studied in a variety of areas and used in diverse applications including intrusion detection, fraud detection, and image processing [1, 2].
Let A ∈ Rm×n be a rank-r matrix, where r ≪ m, n.
This paper aims at promoting an infrequent way to tackle multiclass prediction problems: we advocate for the use of the confusion matrix —the matrix which reports the probability of predicting class q for an instance of class p for all potential label pair (p, q)— as the objective ‘function’ to be optimized.
Time-series data are available in many different fields, including medicine, finance, information retrieval and weather prediction.
In reinforcement learning (RL), the agent interacts with a (partially) unknown environment, classically assumed to be a Markov decision process (MDP), with the goal of maximizing its expected long-term total reward.
Crowdsourcing has become an efficient and inexpensive way to label large datasets in many application domains, including computer vision and natural language processing.
Object recognition research has made impressive gains in recent years, with particular success in using discriminative learning algorithms to train classifiers tuned to each category of interest (e.
There has been growing interest in the machine learning community to model dynamical systems in continuous time.
Associated with any undirected graphical model [1] is the so-called density of states, a term borrowed from statistical physics indicating a distribution that, for any likelihood value, gives the number of configurations with that probability.
A central problem in network analysis is to identify communities, groups of related nodes with dense internal connections and few external connections [1, 2, 3].
Informative subset selection problems arise in many applications where a small number of items must be chosen to represent or cover a much larger set; for instance, text summarization [1, 2], document and image search [3, 4, 5], sensor placement [6], viral marketing [7], and many others.
Real world problems usually demand continuous state or action spaces, and one of the challenges for reinforcement learning is to deal with such continuous domains.
The Restricted Boltzmann Machine (RBM) [1, 2] is an important class of probabilistic graphical models.
Graphical models have proven to be a useful tool for performing approximate inference in a wide variety of application areas including computer vision, combinatorial optimization, statistical physics, and wireless networking.
A neuron represents sensory information via its spike train.
In the matrix reconstruction problem, we are given a matrix Y ∈ Rn×m whose entries are only partly observed, and would like to reconstruct the unobserved entries as accurately as possible.
A variety of problems in machine learning, from ranking to multi-object tracking, involve inference over permutations.
R Non-linear entropy functionals of a multivariate density f of the form g(f (x), x)f (x)dx arise in applications including machine learning, signal processing, mathematical statistics, and statistical communication theory.
Conditional Random Fields (CRF) are a widely popular class of discriminative models for the distribution of a set of hidden states conditional on a set of observable variables.
In this paper we consider the problem of numerical integration of a differentiable function f : [0, 1]d → R given a ﬁnite budget n of evaluations to the function that can be allocated sequentially.
In the field of theoretical signal processing, compressive sensing (CS) has arguably been one of the major developments of the past decade.
We consider the problem of learning high dimensional graphical models.
Models of disease progression are among the core tools of modern medicine for early disease diagnosis, treatment determination and for explaining symptoms to patients.
Perhaps the most striking property of biological visual systems is their ability to efficiently cope with the high bandwidth data streams received from the eyes.
Automatic speech recognition (ASR), the process of automatically translating spoken words into text, has been an important research topic for several decades owing to its wide array of potential applications in the area of human-computer interaction (HCI).
In regression problems over Rd , the unknown function f might vary more in some coordinates than in others, even though all coordinates might be relevant.
Figure 1: We describe two-stage models for detecting and analyzing the 3D shape of objects in unconstrained images.
Our focus in this paper is on unsupervised learning problems such as matrix factorization or latent subspace identification.
Feature selection is a key component in many machine learning settings.
Gaussian processes (GP) are a popular non-parametric prior for function estimation.
Topic models use latent variables to explain the observed (co-)occurrences of words in documents.
Ultimately, the main aim of a clinical trial is straightforward: it is to examine and quantify the effectiveness of a treatment of interest.
Consider a standard least squares regression problem.
The structures of organizational communication networks are critical to collaborative problem solving [1].
Many machine learning algorithms presuppose the existence of a pairwise similarity measure on the input space.
This paper focuses on modeling data matrices by simultaneously capturing the dependent structures among both rows and columns, which is especially useful for filling missing values.
Sensitive statistical data on individuals are ubiquitous, and publishable analysis of such private data is an important objective.
Following recent advances in learning algorithms and robust feature representations, tasks in video understanding have shifted from classifying simple motions and actions [3, 4] to detecting complex events and activities in Internet videos [1,5,6].
Representing salient image regions in a way that is invariant to unwanted image transformations is a crucial Computer Vision task.
Part-based and hierarchical representations have been widely studied in computer vision, and lead to some elegant frameworks for complex object detection and recognition.
Structured relational data arises in a variety of contexts, including graph-valued data [e.
Causal discovery aims to discover the underlying generating mechanism of the observed data, and consequently, the causal relations allow us to predict the effects of interventions on the system [15, 19].
Imitation learning has been successfully applied to a variety of applications [1, 2].
A fundamental problem in probability and statistics is to determine with overwhelming probability the rate of convergence of the empirical covariance (or inverse covariance) of an i.
The task of learning a policy for a sequential decision problem with continuous state space is a long-standing challenge that has attracted the attention of the reinforcement learning community for years.
Scene understanding approaches have largely focused on understanding the physical structure of a scene: “what is where?” [1].
Many real applications can be reduced to a ranking problem.
A fundamental challenge faced by the brain is to combine noisy sensory information with prior knowledge in order to perceive and act in the natural world.
During the past decades, a large number of algorithms have been proposed to deal with learning problems in the case of single-valued functions (e.
Here we study learning and inference in the overcomplete linear model given by Y x = As, p(s) = fi (si ), (1) i where A ∈ RM ×N , N ≥ M , and each marginal source distribution fi may depend on additional parameters.
Consider a book recommendation system.
Large scale multilabel classification problems arise in several practical applications and has recently generated a lot of interest with several efficient algorithms being proposed for different settings [1, 2].
Neurophysiology experiments are costly and time-consuming.
In the area of X-ray imaging, phase retrieval (PR) refers to the problem of recovering a complex multivariate signal from the squared magnitude of its Fourier transform.
The hidden Markov model (HMM) [1] is a probabilistic model that assumes a signal is generated by a double embedded stochastic process.
Latent structure analysis of sequence data is an important technique for many applications such as speech recognition, bioinformatics, and natural language processing.
Topic models, such as Latent Dirichlet Allocation (LDA) [3], have shown great promise in discovering latent semantic representations of large collections of text documents.
Probabilistic models play a crucial role in many scientific disciplines and real world applications.
Minwise hashing [4, 3] is a standard technique in the context of search, for efficiently computing set similarities.
In this paper, we focus on the learning of a general-purpose non-linear classifier applied to perceptual signals such as vision and speech.
Decision-making entities, whether they are businesses, governments, or individuals, usually interact in game-theoretic environments, in which the final outcome is intimately tied to the actions taken by others in the environment.
Resampling methods (e.
Neuromorphic systems try to replicate cognitive processing functions in integrated circuits.
When humans address a new learning problem, they often use knowledge acquired while learning different but related tasks in the past.
Deep learning and unsupervised feature learning have shown great promise in many practical applications.
Stochastic optimization algorithms have many desirable features for large-scale machine learning, and have been studied intensively in the last few years (e.
Advances in sensory neuroscience rely on the development of testable functional models for the encoding of sensory stimuli in neural responses.
Legislative behavior centers around the votes made by lawmakers.
In this paper we propose a simple but new model to learn informative linear projections of multivariate data.
We present an algorithm (with rigorous performance guarantees) for a basic statistical problem.
A key objective in the theory of Markov Decision Processes (MDPs) is to maximize the expected sum of discounted rewards when the dynamics of the MDP are (perhaps partially) unknown.
A surrogate loss is a loss function used as a substitute for the true quality measure during training in order to ease the optimization of the empirical risk.
As social animals, people constantly organise themselves into social groups.
The estimation of probability density functions over sets of random variables is a central problem in learning.
How is the brain structured? The recent field of connectomics [2] is developing high-throughput techniques for mapping connections in nervous systems, one of the most important and ambitious goals of neuroanatomy.
The framework of graphical models allows for parsimonious representation of high-dimensional data by encoding statistical relationships among the given set of variables through a graph, known as the Markov graph.
There are natural tensions between learning and privacy that arise whenever a learner must aggregate data across multiple individuals.
Images of three dimensional objects exhibit a great deal of variation due to viewpoint.
Data samples described in high-dimensional feature spaces are encountered in many important areas.
An enduring challenge for machine learning is in the development of algorithms that scale to truly large data sets.
Our study is broadly motivated by questions in high-dimensional learning.
Retrieving relevant content from massive databases containing high-dimensional data is becoming common in many applications involving images, videos, documents, etc.
The Kalman filter and its extensions [1], such as the extended and unscented Kalman filters [7], are principled statistical models that have been widely used for some of the most challenging and mission-critical applications in automatic control, robotics, machine learning, and economics.
Functional Magnetic Resonance Imaging (fMRI) is a technique used in psychological experiments to measure the blood oxygenation level throughout the brain, which is a proxy for neural activity; this measurement is called brain activation.
Sparse Coding (SC) is one of the most popular algorithms for feature learning and has become a standard approach in Machine Learning, Computational Neuroscience, Computer Vision, and other related ﬁelds.
The nominal goal of predictive inference is to achieve high accuracy.
Dynamical systems (DS) have proved to be a promising framework for encoding and generating complex motions.
We consider the undirected graph estimation problem for a d-dimensional random vector X = (X1 , .
Symmetric positive definite (spd) matrices1 are remarkably pervasive in a multitude of areas, especially machine learning and optimization.
MAP inference on graphical models is a central problem in machine learning, pattern recognition, and computer vision.
We consider sequential prediction of outcomes y1 , y2 , .
Consider the `1 -regularized loss minimization problem n min w 1X `(yi , (Xw)i ) + kwk1 , n i=1 (1) where X 2 IRn⇥p is the design matrix, w 2 IRp is a weight vector to be estimated, and the loss function ` is such that `(y, ·) is a convex differentiable function for each y.
Topic models (also known as mixed-membership models) are a useful method for analyzing large text collections [1, 2].
Structure learning for graphical models is a problem that arises in many contexts.
Dimensionality reduction is one of the most important forms of unsupervised learning, with roots dating to the origins of data analysis.
Locality-sensitive hashing (LSH) method aims to hash similar data samples to the same hash code with high probability [7, 9].
Many algorithms are now available to learn hierarchical features from unlabeled image data.
Reinforcement learning (RL) is a well known framework that formalizes decision making in unknown, uncertain environments.
It is well known that outliers have a detrimental effect on standard regression estimators.
We develop a novel approach for supervised learning based on adaptively partitioning the feature space into different regions and learning local region classifiers.
Current approaches to object recognition make essential use of machine learning methods.
In machine learning, the notion of “abstention” commonly refers to the possibility of refusing a prediction in cases of uncertainty.
Random walks have been widely used for graph-based learning, leading to a variety of models including PageRank [14] for web page ranking, hitting and commute times [8] for similarity measure between vertices, harmonic functions [20] for semi-supervised learning, diffusion maps [7] for dimensionality reduction, and normalized cuts [12] for clustering.
Automatic semantic analysis of multimedia content has been an active area of research due to potential implications for indexing and retrieval [1–7].
The Principal Component Analysis (PCA) is introduced as follows.
In the last decade, several direct policy search (DPS) methods have been developed in the field of reinforcement learning (RL) [1, 2, 3, 4, 5, 6, 7, 8, 9] and have been successfully applied to practical decision making applications [5, 7, 9].
Our visual systems are amazingly competent at recognizing patterns in images.
Some of the most influential machine learning tools are based on the hypothesis class of halfspaces with margin.
Information theory, machine learning, and statistics, are closely related disciplines.
In this paper, we focus on the setting of intrinsically motivated reinforcement learning (see Oudeyer and Kaplan [2007], Baranes and Oudeyer [2009], Schmidhuber [2010], Graziano et al.
Mixture distributions have been widely used for statistical modeling of complex data.
In this paper, we consider the well-studied non-stochastic expert problem in a distributed setting.
Multivariate response prediction, also known as multiple-output regression [3] when the responses are real-valued vectors, is an important problem in machine learning and statistics.
Modeling data using low-dimensional representations is a fundamental approach in data analysis, motivated by the inherent redundancy in many datasets and to increase the interpretability of data via dimensionality reduction.
We consider the problem of learning discriminative classification models for visual recognition.
Human long-term memory can store a remarkable amount of visual information and remember thousands of different pictures even after seeing each of them only once [25, 1].
Hierarchical clustering models aim to fit hierarchies to data, and enjoy the property that clusterings of varying size can be obtained by “pruning” the tree at particular levels.
Time delay is pervasive in neural information processing.
Linear classifiers, including SVM and boosting, play an important role in machine learning.
The Kolmogorov-Smirnov (KS) test is efficient, simple, and often considered the choice method for comparing distributions.
Recent value-based reinforcement learning applications have shown the benefit of exhaustively generating features, both in discrete and continuous state domains.
Many algorithms are now available to learn hierarchical features from unlabeled image data.
Arising from domains as diverse as bioinformatics and web mining, large-scale data exhibiting network structure are becoming increasingly available.
The information bottleneck method (IB) [1] considers the concept of relevant information in the data compression problem, and takes a new perspective to signal compression which was classically treated using rate distortion theory.
In statistical learning theory, one of the major concerns is to obtain the generalization bound of a learning process, which measures the probability that a function, chosen from a function class by an algorithm, has a sufficiently small error (cf.
This paper focuses on nonconvex composite objective problems having the form minimize Φ(x) := f (x) + h(x) x ∈ X, (1) where f : Rn → R is continuously differentiable, h : Rm → R ∪ {∞} is lower semi-continuous (lsc) and convex (possibly nonsmooth), and X is a compact convex set.
Binary classification is one of the most well-understood problems of machine learning and statistics: a wealth of efficient classification algorithms has been developed and applied to a wide range of applications.
The use of graphical models [1, 2] is ubiquitous in machine learning.
Consider the estimation of a random vector x ∈ Rn from a measurement vector y ∈ Rm .
The problem of learning from examples is in most circumstances ill-posed.
This work aims to generate useful probabilistic models of high dimensional trajectories in continuous spaces.
Blaschko and Lampert have recently shown that object localization can be approached as structured regression problem [2].
In reinforcement learning (RL), an agent autonomously learns how to make optimal sequential decisions by interacting with the world.
Bayesian nonparametric (BNP) models [1] have emerged as an important tool for building probability models with flexible latent structure and complexity.
Collaborative prediction is a task of predicting users’ potential preferences on currently unrated items (e.
One of the central problems in machine learning is prediction/inference, where given an input datum X, we would like to predict or infer the value of a target variable of interest, y, assuming X and y have some intrinsic relationship.
Latent variables models such as principal components analysis (Pearson, 1901; Hotelling, 1933; Tipping and Bishop, 1999; Roweis, 1998) and factor analysis (Young, 1941) are popular for summarising high dimensional data, and can be seen as modelling the covariance of the observed dimensions.
Given x1 , .
Bayesian nonparametric methods provide an increasingly important framework for unsupervised learning from structured data.
The essence of machine learning is to exploit what we observe in order to form accurate predictors of what we cannot.
Dimensionality reduction is a fundamental tool for understanding complex data sets that arise in contemporary machine learning and data mining applications.
Latent linear dynamical system (LDS) models, also known as Kalman-filter models or linearGaussian state-space models, provide an important framework for modelling shared temporal structure in multivariate time series.
By giving reward at the right times, animals like monkeys can be trained to perform complex tasks that require the mapping of sensory stimuli onto responses, the storage of information in working memory and the integration of uncertain sensory evidence.
Discrete mixture models characterize the density of y ∈ Y ⊂ <m as f (y) = k X ph φ(y; γh ) (1) h=1 where p = (p1 , .
Regularization [1] is a popular and well-studied methodology to address ill-posed estimation problems [2] and learning from examples [3].
In supervised classification, the goal is to learn a classifier from a collection of training instances, where each instance has a unique class label.
The recent interest in understanding human perception and behavior from the perspective of neuroscience and cognitive psychology has spurred a revival of interest in mathematical decision theory.
The goal of this paper is to develop an extended framework for supervised learning with similarity functions.
Reinforcement learning (RL) agents need to solve the exploitation-exploration tradeoff.
It is widely recognized that the process of fitting observed data to a statistical model needs to incorporate latent or hidden factors, which are not directly observed.
Historically, the fields of statistical inference and stochastic optimization have often developed their own specific methods and approaches.
Probabilistic graphical models (PGMs) have become useful tools for classical machine learning tasks, such as multi-label classification [1] or semi-supervised learning [2], as well for many realworld applications, for example image processing [3], natural language processing [4], bioinformatics [5], and computational neuroscience [6].
Protein structure prediction from amino acidic sequence is one of the grand challenges in Bioinformatics and Computational Biology.
Object class detection is a central problem in computer vision.
In this paper we address the problem of adaptive control of a high dimensional linear quadratic (LQ) system.
Natural sounds, such as speech and animal vocalizations, consist of complex acoustic events occurring at multiple scales.
Modeling large, complex, real-world domains requires the ability to handle both rich relational structure and large amount of uncertainty.
Human memory has a vast capacity, storing all the semantic knowledge, facts, and experiences that people accrue over a lifetime.
A large fraction of the machine learning community is concerned itself with the formulation of a learning problem as a single, well-defined optimization problem.
Probabilistic graphical models [Pearl, 1988] are widely use to model and reason about phenomena in a variety of domains such as medical diagnosis, communication, machine vision and bioinformatics.
Many applications involve simultaneous prediction of multiple variables.
Object recognition is a major focus of research in computer vision and machine learning.
One of the most challenging aspects of image recognition is the large amount of intra-class variability, due to factors such as lighting, background, pose, and perspective transformation.
The cocktail party problem, or the speech separation problem, is one of the central problems in speech processing.
Finding principles underlying learning in neural networks is an important problem for both artificial and biological networks.
There are many factors that contribute to a document’s word choice: topic, syntax, sentiment, author perspective, and others.
Online social networks allow users to follow streams of posts generated by hundreds of their friends and acquaintances.
Generative parsing models, which define joint distributions over sentences and their parse trees, are one of the core techniques in computational linguistics.
In addition to functional localization and integration, mapping the neural correlates of “mental states” or “brain states” (i.
A rapidly emerging theme in the analysis of networked data is the study of signed networks.
The problem of best arm(s) identification [6, 3, 1] in the stochastic multi-armed bandit setting has recently received much attention.
Robustness against uncertainties and sensitivity to risk are major issues that have been addressed in recent development of the Markov decision process (MDP).
As bigger and more complex datasets are available, multiclass learning is becoming increasingly important in machine learning.
This paper studies the online learning framework, where the goal of the player is to incur small regret while observing a sequence of data on which we place no distributional assumptions.
In statistical learning (also called batch learning) [1] one obtains a random sample (X1 , Y1 ), .
Biological vision systems have evolved sophisticated tracking mechanisms, capable of tracking complex objects, undergoing complex motion, in challenging environments.
Kernel learning methods (such as Support Vector Machines) are conceptually simple, strongly rooted in statistical learning theory, and can often be formulated as a convex optimization problem.
The modeling of temporal dependencies is an important and challenging task with applications in fields that use forecasting or retrospective analysis, such as finance, biomedicine, and anomaly detection.
Online convex optimization is a sequential prediction paradigm in which, at each time step, the learner chooses an element from a fixed convex set S and then is given access to a convex loss function defined on the same set.
Many tasks in text and speech processing, computational biology, or learning models of the environment in reinforcement learning, require estimating a function mapping variable-length sequences to real numbers.
We consider the problem of finding a set of locally-biased vectors that inherit many of the “nice” properties that the leading nontrivial global eigenvectors of a graph Laplacian have—for example, that capture “slowly varying” modes in the data, that are fairly-efficiently computable, that can be used for common machine learning and data analysis tasks such as kernel-based and semi-supervised learning, etc.
When models contain multiple output variables, an important potential source of structure is the number of variables that take on a particular value.
Contour detection is a fundamental problem in vision.
One often has to use function approximation to represent the near optimal value function of the reinforcement learning (RL) and planning problems with large state spaces.
Bipartite matching problems (BMPs), which involve mapping one set of items to another, are ubiquitous, with applications ranging from computational biology to information retrieval to computer vision.
An extensive-form game is a common formalism used to model sequential decision making problems containing multiple agents, imperfect information, and chance events.
One of the key challenges of computer vision is the robust representation of complex objects and so over the years, increasingly rich features have been proposed.
The number of parameters in a textbook probabilistic graphical model (PGM) is an exponential function of the number of parents of the nodes in the graph.
Finding genetic correlates of disease is a long-standing important problem with potential contributions to diagnostics and treatment of disease.
Graphical model inference is now prevalent in many fields, running the gamut from computer vision and civil engineering to political science and epidemiology.
The estimation of the inverse of the covariance matrix (also referred to as precision matrix or concentration matrix) is a very important problem with applications in a number of fields, from biology to social sciences, and is a fundamental step in the estimation of underlying data networks.
Collaborative Filtering (CF) is a method of making predictions about an individual’s preferences based on the preference information from many users.
Let Θ ∈ IRd1 ×d2 be a matrix of interest and Ω∗ = {1, .
Nonparametric mixture models allow us to bypass the issue of model selection, by modeling data using a random number of mixture components that can grow if we observe more data.
Variational inference algorithms pose probabilistic inference as an optimization over distributions.
We are interested in visual learning for recognition of objects and scenes embedded in physical space.
Crowdsourcing provides an easy and relatively inexpensive way to utilize human capabilities to solve difficult computational learning problems (e.
A central problem in systems neuroscience is to understand the probabilistic representation of information by neurons and neural populations.
Understanding dependencies within multivariate data is a central problem in the analysis of financial time series, underpinning common tasks such as portfolio construction and calculation of value-atrisk.
Generative latent variable models offer an intuitive way to explain data in terms of hidden structure, and are a cornerstone of exploratory data analysis.
While supervised training is an integral part of building visual, textual, or multi-modal category models, more recently, knowledge transfer between categories has been recognized as an important ingredient to scale to a large number of categories as well as to enable fine-grained categorization.
Continuous time stochastic processes provide a flexible and popular framework for data modelling in a broad spectrum of scientific and engineering disciplines.
We consider the classical reinforcement learning problem of an agent interacting with its environment while trying to maximize total reward accumulated over time [1, 2].
To find the best solution to a complex real-life search problem, e.
It is widely thought that our very ability to remember the past over long time scales depends crucially on our ability to modify synapses in our brain in an experience dependent manner.
In machine learning, we often encounter the following optimization problem.
Information theoretic quantities are popular tools in neuroscience, where they are used to study neural codes whose representation or function is unknown.
Ridge Regression, which penalizes the `2 norm of the weight vector and shrinks it towards zero, is the most widely used penalized regression method.
Standard Restricted Boltzmann Machines (RBMs) are a type of Markov Random Field (MRF) characterized by a bipartite dependency structure between a group of binary visible units x ∈ {0, 1}n and binary hidden units h ∈ {0, 1}m .
Problems related to graph isomorphisms have been an important and enjoyable challenge for the scientific community for a long time.
Decentralized partially observable Markov decision processes (DecPOMDPs) are a popular model for cooperative multi-agent decision problems; however, they are NEXP-complete to solve [15].
Given an m n matrix A, it is often desirable to find a sparser matrix B that is a good proxy for A.
Matrix completion has attracted a lot of attention over the past few years.
The visual world is populated with a vast number of objects, the most appropriate labeling of which is often ambiguous, task specific, or admits multiple equally correct answers.
Multi-relational data refers to directed graphs whose nodes correspond to entities and edges of the form (head, label, tail) (denoted (h, `, t)), each of which indicates that there exists a relationship of name label between the entities head and tail.
Stochastic gradient (SG) optimization [1, 2] is widely used for training machine learning models with very large-scale datasets.
The classification problem in machine learning and data mining is to predict an unobserved discrete output value y based on an observed input vector x.
Kernel-based algorithms are widely used in machine learning and have been shown to often provide very effective solutions.
Natural actor-critics form a class of policy search algorithms for ﬁnding locally optimal policies for Markov decision processes (MDPs) by approximating and ascending the natural gradient [1] of an objective function.
Motivated by applications in viral marketing [1], researchers have been studying the influence maximization problem: find a set of nodes whose initial adoptions of certain idea or product can trigger, in a time window, the largest expected number of follow-ups.
The principle of parsimony is used in many areas of science and engineering to promote “simple” models over more complex ones.
Ordinary Least Squares (OLS) is one of the oldest and most widely studied statistical estimation methods with its origins tracing back over two centuries.
In the real world, two images of the same object may only be related by a very complicated and highly nonlinear transformation.
The problem of acquiring an N -dimensional signal x through M linear measurements, y = F x, arises in many contexts.
Convex optimization has become a tool central to many areas of engineering and applied sciences, such as signal processing [20] and machine learning [24].
Gaussian process (GP) inference methods have been successfully applied to models for dynamical systems, see e.
Model-based machine learning and probabilistic programming offer the promise of a world where a probabilistic model can be specified independently of the inference routine that will operate on the model.
Multi-task learning (MTL) has been studied for learning multiple related tasks simultaneously.
Probability distributions over spike words form the fundamental building blocks of the neural code.
High-treewidth graphical models typically yield distributions where exact inference is intractable.
Massive datasets are becoming an ubiquitous by-product of modern scientific and industrial applications.
Search advertising, also known as sponsored search, has been formulated as a multi-armed bandit (MAB) problem [11], in which the search engine needs to choose one ad from a pool of candidate to maximize some objective (e.
Processing of signals on graphs is emerging as a fundamental problem in an increasing number of applications [22].
The computation performed by a neural circuit is a product of the properties of single neurons in the circuit and their connectivity.
In terms of sheer size, visual data is, by most accounts, the biggest “Big Data” out there.
Dropout is an algorithm for training neural networks that was described at NIPS 2012 [7].
Motivation and background In analyzing multivariate time series data, collected in financial applications, monitoring of influenza outbreaks and other fields, it is often of key importance to accurately characterize dynamic changes over time in not only the mean of the different elements (e.
For evolutionary reasons, biological tissue at all spatial scales is composed of repeating patterns.
During the recent years, there has been a growing interest on the problem of learning a tensor from a set of linear measurements, such as a subset of its entries, see [9, 17, 22, 23, 25, 26, 27] and references therein.
A collection of documents, each consisting of a disorganized bag of words is often modeled compactly using mixture or admixture models, such as Latent Semantic Analysis (LSA) [4] and Latent Dirichlet Allocation (LDA) [1].
Similarity search algorithms are essential to multimedia retrieval, computational biology, and statistical machine learning.
Consider a politician trying to elude a group of reporters.
Many applications of machine learning, ranging from computer vision to computational biology, require the analysis of large volumes of high-dimensional continuous-valued measurements.
As we move towards more complete image understanding, having more precise and detailed object recognition becomes crucial.
Bayesian networks have been popular tools for representing the probability distribution over a large number of variables.
Decision trees have a long history in machine learning and were one of the first models proposed for inductive learning [14].
Modern applications awaiting next generation machine intelligence systems have posed unprecedented scalability challenges.
Calcium imaging methods have revolutionized data acquisition in experimental neuroscience; we can now record from large neural populations to study the structure and function of neural circuits (see e.
In many situations, individuals wish to share their personal data for machine learning applications and other exploration purposes.
A set function f : 2V → R is said to be submodular [4] if for all subsets S, T ⊆ V , it holds that f (S) + f (T ) ≥ f (S ∪ T ) + f (S ∩ T ).
We live in the big data era – a world where an overwhelming amount of data is generated and collected every day, such that it is becoming increasingly impossible to process data in its raw form, even though computers are getting exponentially faster over time.
In the context of network analysis, a latent space refers to a space of unobserved latent representations of individual entities (i.
Machine vision methods have achieved considerable success in recent years, as evidenced by performance on major challenge problems [4, 7], where strong performance has been obtained for assigning one of a large number of labels to each of a large number of images.
Parsimony, preferring a simple explanation to a more complex one, is probably one of the most intuitive principles widely adopted in the modeling of nature.
Graph-based learning algorithms have received considerable attention in machine learning community.
Bayesian nonparametric mixture models [7] provide an important framework to describe complex data.
Policy evaluation, i.
Invariances are among the most useful prior information used in machine learning [1].
Multilabel classification (MLC) is a classification task where each input may be associated to several class labels, and the goal is to predict the label set given the input.
Spatial filtering is a crucial step in the reliable decoding of user intention in Brain-Computer Interfacing (BCI) [1, 2].
It is well-known that Dirichlet process mixtures (DPMs) of normals are consistent for the density — that is, given data from a sufficiently regular density p0 the posterior converges to the point mass at p0 (see [1] for details and references).
Low-rankedness of matrices has frequently been exploited when one reconstructs a matrix from its noisy observations.
The geometry of Hermitian positive definite (hpd) matrices is remarkably rich and forms a foundational pillar of modern convex optimisation [21] and of the rapidly evolving area of convex algebraic geometry [4].
EDML is a recently proposed algorithm for learning MAP parameters of a Bayesian network from incomplete data [5, 16].
Privacy is an important problem in data analysis.
Robot navigation relies on at least three sub-tasks: localization, mapping, and motion planning.
Cross language text classification is an important natural language processing task that exploits a large amount of labeled documents in an auxiliary source language to train a classification model for classifying documents in a target language where labeled data is scarce.
Recent years have seen an explosion in the availability of time series data related to virtually every human endeavor — data that demands to be analyzed and turned into valuable insights.
Graph-structured data appears in many application domains of machine learning, reaching from Social Network Analysis to Computational Biology.
One of the central problems studied in online learning is prediction with expert advice.
Maximization of submodular functions [14] has wide applications in machine learning and artificial intelligence, such as social network analysis [9], sensor placement [10], and recommender systems [7, 2].
In recent years, with the advances in sensorial and information technology, massive amounts of high-dimensional data are available to us.
In the modern digital period, we are facing a rapid growth of available datasets in science and technology.
Optimization of nonconvex functions is known to be computationally intractable in general [11, 12].
The Dirichlet process mixture model (DPMM) is a powerful tool for clustering data that enables the inference of an unbounded number of mixture components, and has been widely studied in the machine learning and statistics communities [1–4].
In recent years of machine learning applications, the size of data has been observed with an unprecedented growth.
The mixture model has been studied extensively from several directions.
In many practical applications of active learning, the cost to acquire a large batch of labels at once is significantly less than the cost of the same number of sequential rounds of individual label requests.
Scaling probabilistic inference algorithms to large datasets and parallel computing architectures is a challenge of great importance and considerable current research interest, and great strides have been made in designing parallelizeable algorithms.
Structured prediction models for action recognition and localization are emerging as prominent alternatives to more traditional holistic bag-of-words (BoW) representations.
Machine learning algorithms rely critically on the features used to represent data; the feature set provides the primary interface through which an algorithm can reason about the data at hand.
Learning from Demonstration (LfD) is a practical framework for learning complex behaviour policies from demonstration trajectories produced by an expert.
Most real-world applications are structured, i.
We consider agents that live for a long time in a sequential decision-making environment.
Learning a boosted classifier from a set of samples S = {X, Y }N ∈ RD × {−1, 1} is usually addressed in the context of two main frameworks.
A neuron’s linear receptive field (RF) is a filter that maps high-dimensional sensory stimuli to a one-dimensional variable underlying the neuron’s spike rate.
In many sequential decision problems, the transition dynamics can change with time.
One-class SVM (OCSVM) [14] is a kernel-based learning algorithm that is often considered to be the method of choice for set estimation in high-dimensional data due to its generalization power, efficiency, and nonparametric nature.
The optimization of non-linear functions whose evaluation may be noisy and expensive is a challenge that has important applications in sciences and engineering.
This work was stimulated by a concrete problem, namely the decomposition of state-of-the-art 2D + time calcium imaging sequences as shown in Fig.
Revealing hidden structures of a graph is the heart of many data analysis problems.
Ontologies and knowledge bases such as WordNet [1], Yago [2] or the Google Knowledge Graph are extremely useful resources for query expansion [3], coreference resolution [4], question answering (Siri), information retrieval or providing structured knowledge to users.
Principal components analysis (PCA) is a popular technique for unsupervised dimension reduction that has a wide range of application—science, engineering, and any place where multivariate data is abundant.
We first introduce the problem of causal inference on iid data, that is in the case with no time structure.
An option is a financial contract that allows the purchase or sale of a given asset, such as a stock, bond, or commodity, for a predetermined price on a predetermined date.
Recent work on scaling deep networks has led to the construction of the largest artificial neural networks to date.
Inference in complex models drives much of the research in machine learning applications, from computer vision, natural language processing, to computational biology.
In this paper we are interested in recovering a complex1 vector x∗ ∈ Cn from magnitudes of its linear measurements.
Dirichlet process mixture models (DPMMs) are widely used in the machine learning community (e.
Principal component regression (PCR) has been widely used in statistics for years (Kendall, 1968).
Various graph-based models, regardless of application, aim to learn a target function on graphs that well respects the graph topology.
We consider a stochastic optimal control problem in discrete time with continuous state and action spaces.
Low contrast stimuli are perceived to move slower than high contrast ones [17].
A major goal of neuroscience is the mapping of neural microcircuits at the scale of hundreds to thousands of neurons [1].
For decades, deep networks with broad hidden layers and full connectivity could not be trained to produce useful results, because of overfitting, slow convergence and other issues.
Functional brain imaging, in particular fMRI, is the workhorse of brain mapping, the systematic study of which areas of the brain are recruited during various experiments.
The problem.
p Consider a p-dimensional probability distribution with true covariance matrix Σ0 ∈ S++ and true p −1 p×n precision (or inverse covariance) matrix Ω0 = Σ0 ∈ S++ .
The last decade, machine learning has seen the rise of neural networks composed of multiple layers, which are often termed deep neural networks (DNN).
Suppose we have completed a placebo-controlled clinical trial of a promising new drug for a neurodegenerative disorder such as Alzheimer’s disease (AD) on a small sized cohort.
Visual tracking, also called object tracking, refers to automatic estimation of the trajectory of an object as it moves around in a video.
Many essential neural computations are implemented by large populations of neurons working in concert, and recent studies have sought both to monitor increasingly large groups of neurons [1, 2] and to characterise their collective behaviour [3, 4].
Inference in large-scale probabilistic models remains a challenge, particularly for modern “big data” problems.
State-space models (SSMs) constitute a popular and general class of models in the context of time series and dynamical systems.
Determinantal Point Process (DPP) [1] is a well-known framework for representing a probability distribution that models diversity.
Discriminatively trained deep convolutional neural networks (CNN) [18] have recently achieved impressive state of the art results over a number of areas, including, in particular, the visual recognition of categories in the ImageNet Large-Scale Visual Recognition Challenge [4].
Tetris is a popular video game created by Alexey Pajitnov in 1985.
Submodularity is a rich combinatorial concept that expresses widely occurring phenomena such as diminishing marginal costs and preferences for grouping.
Markov decision processes (MDPs) provide a general framework for planning and learning under uncertainty.
Gaussian mixture models provide a simple framework for several machine learning problems including clustering, density estimation and classification.
The firing rate of a neuron is arguably the most important characterisation of both neural network dynamics and neural computation, and has been ever since the seminal recordings of Adrian and Zotterman [1] in which the firing rate of a neuron was observed to increase with muscle tension.
The sequential posterior updates play a central role in many Bayesian inference procedures.
People can acquire a new concept from only the barest of experience – just one or a handful of examples in a high-dimensional space of raw perceptual input.
Samples from a determinantal point process (DPP) [15] are sets of points that tend to be spread out.
Many generative models are defined in terms of an unnormalized probability distribution, and computing the probability of a state requires computing the (usually intractable) partition function.
Recent advances in nanoscale devices and biomolecular synthesis have opened up new and exciting possibilities for constructing microscopic systems that can sense and autonomously manipulate the world.
K-armed bandit problems provide an elementary model for exploration-exploitation tradeoffs found at the heart of many online learning problems.
We consider the following generic optimization problem.
Clustering is a major task in machine learning and has been extensively studied over decades of research [11].
Learning dynamic models from observed data has been a central issue in many fields of study, scientific or engineering tasks.
Adaptive decision making algorithms have been used increasingly in the past years, and have attracted researchers from many application areas, like artificial intelligence [16], financial engineering [10], medicine [14] and robotics [15].
Sampling techniques are one of the most widely used approaches to approximate probabilistic reasoning for high-dimensional probability distributions where exact inference is intractable.
Online learning provides a scalable and flexible approach for solving a wide range of prediction problems, including classification, regression, ranking, and portfolio management.
In this paper, we investigate the problem of robust Principal Component Analysis (PCA) in an online fashion.
Although it is often useful for machine learning methods to consider how nature has arrived at a particular solution, it is perhaps more instructive to first understand the functional role of such biological constraints.
Factorized asymptotic Bayesian (FAB) inference is a recently-developed Bayesian approximation inference method for model selection of latent variable models [5, 6].
Privacy-preserving machine learning algorithms are increasingly essential for settings where sensitive and personal data are mined.
Unsupervised feature learning algorithms have recently attracted much attention, with the promise of letting the data guide the discovery of good representations.
Dropout training was introduced by Hinton et al.
Given two samples {xi }ni=1 where xi ∼ P i.
The explosive growth of web videos makes automatic video classification important for online video search and indexing.
Markov Decision Processes (MDPs) have been widely used to model and solve sequential decision making problems under uncertainty, in fields including artificial intelligence, control, finance and management (Puterman, 2009, Barber, 2011).
Submodularity is a pervasive and important property in the areas of combinatorial optimization, economics, operations research, and game theory.
Multilayer perceptrons (MLPs) are general purpose function approximators.
We focus on optimization problems written over the set of permutations.
Learning from prior tasks and transferring that experience to improve future performance is a key aspect of intelligence, and is critical for building lifelong learning agents.
In linear regression, we wish to estimate an unknown but fixed vector of parameters θ0 ∈ Rp from n pairs (Y1 , X1 ), (Y2 , X2 ), .
As the volume of data collected in the social and natural sciences increases, the computational cost of learning from large datasets has become an important consideration.
In many scientific areas, an important methodology that has withstood the test of time is the approximation of “complicated” functions by those that are easier to handle.
For single-task learning, besides global learning methods there are local learning methods [7], e.
Most classic clustering algorithms are designed for the centralized setting, but in recent years data has become distributed over different locations, such as distributed databases [21, 5], images and videos over networks [20], surveillance [11] and sensor networks [4, 12].
Graphical models (GMs) represent the backbone of the generic statistical toolbox for encoding dependence structures in multivariate distributions.
Principal Component Analysis (PCA) [19] is arguably the most widely used method for dimensionality reduction in data analysis.
The structured learning problem is to find a function F (x, y) to map from inputs x to outputs as y ∗ = arg maxy F (x, y).
Digital images are often corrupted with noise during acquisition and transmission, degrading performance in later tasks such as: image recognition and medical diagnosis.
What can one infer about an unknown distribution based on a random sample? If the distribution in question is relatively “simple” in comparison to the sample size—for example if our sample consists of 1000 independent draws from a distribution supported on 100 domain elements—then the empirical distribution given by the sample will likely be an accurate representation of the true distribution.
Humans recognize visually-presented objects rapidly and accurately even under image distortions and variations that make this a computationally challenging problem [27].
Eye movements provide a rich source of knowledge into the human visual information processing and result from the complex interplay between the visual stimulus, prior knowledge of the visual world, and the task.
In this paper, we study the problem of online learning in a class of finite non-stationary episodic Markov decision processes.
We study symbolic dynamic programming (SDP) for Markov Decision Processes (MDPs) with exponentially large factored state and action spaces.
Traditional nonparametric regression such as kernel or k-NN can be expensive to estimate given modern large training data sizes.
We study the problem of discovering the presence of latent variables in data and learning models involving them.
Graphical Models (GMs) provide a useful representation for reasoning in a range of scientific fields [1, 2, 3, 4].
Many machine learning algorithms follow the framework of empirical risk minimization, which often can be cast into the following generic optimization problem n 1∑ (1) gi (w), min G(w) := w∈W n i=1 where n is the number of training examples, gi (w) encodes the loss function related to the ith training example (xi , yi ), and W is a bounded convex domain that is introduced to regularize the solution w ∈ W (i.
The brain is able to integrate noisy and partial information from both sensory inputs and internal states to construct a consistent interpretation of the actual state of the environment.
Work on unsupervised feature selection has received considerable attention.
Learning how to measure similarity (or dissimilarity) is a fundamental problem in machine learning.
It is widely recognized that modern statistical problems are increasingly high-dimensional, i.
There has been much interest in recent years in understanding consistency properties of learning algorithms – particularly algorithms that minimize a surrogate loss – for a variety of ﬁnite-output learning problems, including binary classiﬁcation, multiclass classiﬁcation, multi-label classiﬁcation, subset ranking, and others [1–17].
Principal component analysis (PCA) is a widely-used classical technique for dimensionality reduction.
The estimation of covariance matrices is the basis of many machine learning algorithms and estimation procedures in statistics.
q(x) In this paper we address the problem of estimating the ratio of two functions, p(x) where p is given by a sample and q(x) is either a known function or another probability density function given by a sample.
For many optimization problems, the objective and constraint functions are not adequately modeled by linear or convex functions (e.
Sparsity is an important concept in high-dimensional statistics [1] and signal processing [2] that has led to important application successes by reducing model complexity and improving interpretability of the results.
Motivated by the difficulty in exact specification of reward and transition models, researchers have proposed the uncertain Markov Decision Process (MDP) model and robustness objectives in solving these models.
Gaussian processes (GPs) have found many applications in machine learning and statistics ranging from supervised learning tasks to unsupervised learning and reinforcement learning.
Statistical relational models are capable of representing both probabilistic dependencies and relational structure [1, 2].
Choosing a suitable distance to compare probabilities is a key problem in statistical machine learning.
In this paper we are interested in the Bayesian multi-armed bandit problem which can be described as follows.
Most organisms employ a mutlitude of sensory systems to create an internal representation of their environment.
Recently, no-regret algorithms have received increasing attention in a variety of communities, including theoretical computer science, optimization, and game theory [3, 1].
A deep Boltzmann machine (DBM) [18] is a structured probabilistic model consisting of many layers of random variables, most of which are latent.
High-dimensional statistical models have been the subject of considerable focus over the past decade, both theoretically as well as in practice.
In Gaussian random design model for the linear regression, we seek to reconstruct an unknown coefficient vector θ0 ∈ Rp from a vector of noisy linear measurements y ∈ Rn : y = Xθ0 + w, (1.
In recent years, distributed estimation, learning and prediction has attracted a considerable attention in wide variety of disciplines with applications ranging from sensor networks to social and economic networks [1–6].
Regularization has become an indispensable part of modern machine learning algorithms.
Principal Component Analysis (PCA) is a ubiquitous tool used in many data analysis, machine learning and information retrieval applications.
A tenet of mathematical modeling is to faithfully match the structural properties of the data; yet, on occasion, the available tools are inadequate to perform the task.
An important problem that arises in many applications is that of recovering a high-dimensional sparse (or approximately sparse) vector given a small number of linear measurements.
This paper addresses the problem of learning probability distributions over pairs of input-output sequences, also known as transduction problem.
Many clustering models rely on the minimization of an energy over possible partitions of the data set.
Many linear regression problems are characterized by a large number d of features or explaining attributes and by a reduced number n of training instances.
PAC-Bayesian analysis is a general and powerful tool for data-dependent analysis in machine learning.
The problem of characterizing the statistical properties of a spiking neuron is quite general, but two interesting questions one might ask are: (1) what kind of time dependencies are present? and (2) how much information is the neuron transmitting? With regard to the second question, information theory provides quantifications of the amount of information transmitted by a signal without reference to assumptions about how the information is represented or used.
Markov chain Monte Carlo (MCMC) methods [1] have been dominant tools for posterior analysis in Bayesian inference.
Partially observable Markov decision processes (POMDPs) provide a principled general framework for planning in partially observable stochastic environments.
Matrix Completion is the task to reconstruct low-rank matrices from a subset of its entries and occurs naturally in many practically relevant problems, such as missing feature imputation, multitask learning [2], transductive learning [4], or collaborative filtering and link prediction [1, 8, 9].
Social network analysis is vital to understanding and predicting interactions between network entities [6, 19, 21].
Large-scale machine learning problems are becoming ubiquitous in many areas of science and engineering.
Auto-associative memories have a venerable history in computational neuroscience.
Online display advertising inventory — e.
The recent heightened interest in understanding the brain calls for the development of technologies that will advance our understanding of neuroscience.
Complex dynamical systems can often be observed by monitoring time series of one or more variables.
In many applications, we need to aggregate the preferences of agents over a set of alternatives to produce a joint ranking.
Generative models of text have gained large popularity in analyzing a large collection of documents [3, 4, 17].
Learning prototype from a set of given or observed objects is a core problem in machine learning, and has numerous applications in computer vision, pattern recognition, data mining, bioinformatics, etc.
The Markov chain is a standard model for analyzing the dynamics of stochastic systems, including economic systems [29], traffic systems [11], social systems [12], and ecosystems [6].
Variable selection is a core inferential problem in a multitude of statistical analyses.
In undirected graphical models or Markov random fields, each node represents a random variable while the set of edges specifies the conditional independencies of the underlying distribution.
Natural language processing and information retrieval systems can often benefit from incorporating accurate word similarity information.
We consider the reinforcement learning problem in which one attempts to find an approximately optimal policy for controlling a stochastic nonlinear dynamical system.
A long–term goal of machine learning is to create systems that can be interactively trained or guided by non-expert end-users.
Large, streaming data sets are increasingly the norm in science and technology.
Learning classifiers that generalize well is a hard problem when only few training examples are available.
Natural scene statistics have been used to explain a variety of neural structures.
Multitask learning exploits the relationships between several learning tasks in order to improve performance, which is especially useful if a common subset of features are useful for all tasks at hand.
There is increasing interest in exploring connections between information and estimation theory.
Let us consider a fairly general linear inverse problem, where one wants to estimate a parameter vector z ∈ RD , from a noisy observation y ∈ Rn , such that y = Az + b, where A ∈ Rn×D is sometimes referred to as the observation or design matrix, and b ∈ Rn represents an additive Gaussian noise with a distribution PB ∼ N (0, Σ).
Probabilistic approaches to machine learning involve modeling the probability distributions over large collections of variables.
A probabilistic framework for incorporating features posits latent or hidden variables that can provide a good explanation to the observed data.
In Bayesian models, though conjugate priors normally result in easier inference problems, nonconjugate priors could be more expressive in capturing desired model properties.
The goal of nonnegative matrix factorization (NMF) is to approximate a nonnegative matrix V with the product of two nonnegative matrices, as V ≈ W1 W2 .
The recent rise of crowdsourcing has provided a fast and inexpensive way to collect human knowledge and intelligence, as illustrated by human intelligence marketplaces such as Amazon Mechanical Turk, games with purpose like ESP, reCAPTCHA, and crowd-based forecasting for politics and sports.
A growing body of work on efficient reinforcement learning provides algorithms with guarantees on sample and computational efficiency [13, 6, 2, 22, 4, 9].
The Indian buffet process [IBP, 11] is one of several distributions over matrices with exchangeable rows and infinitely many columns, only a finite (but random) number of which contain any non-zero entries.
People often organise themselves into groups or communities.
Movement Primitives (MPs) are commonly used for representing and learning basic movements in robotics, e.
Bayesian nonparametric methods provide a flexible framework for unsupervised modeling of structured data like text documents, time series, and images.
Multi-task Gaussian process (GP) models are widely used to couple related tasks or functions for joint regression.
Recent literature has advocated the use of randomization as a key algorithmic device with which to dramatically accelerate statistical learning with lp regression or low-rank matrix approximation techniques [12, 6, 8, 10].
Auto-encoders learn an encoder function from input to representation and a decoder function back from representation to input space, such that the reconstruction (composition of encoder and decoder) is good for training examples.
The “missing data” problem arises when values for one or more variables are missing from recorded observations.
The problem faced by the sensory system is to infer the underlying causes of a set of input spike trains.
Designing supervised learning algorithms that can learn from data sets with noisy labels is a problem of great practical importance.
Data with various structures and scales comes from almost every aspect of daily life.
Principal component analysis (PCA) [12] has been widely used to analyze high-dimensional data.
A domain refers to an underlying data distribution.
There has been much work investigating how information about stimulus variables is represented by a population of neurons in the brain [1].
Conditional random fields (CRFs) are a popular class of models that combine the advantages of discriminative modeling and undirected graphical models.
In our daily life, we sense the world through multiple sensory systems.
Deep architectures have strong representational power due to their hierarchical structures.
Sintel MPI KITTI Figure 1: Samples of frames and flows from new flow databases.
Numerous machine learning algorithms require selecting representative subsets of manageable size out of large data sets.
Deep learning has recently been enjoying a resurgence [1, 2] due to the discovery that stage-wise pre-training can significantly improve the results of classical training methods [3–5].
Learning and inference in complex models drives much of the research in machine learning applications ranging from computer vision, natural language processing, to computational biology [1, 18, 21].
The ability to classify instances of an unseen visual class, called zero-shot learning, is useful in several situations.
Dimensionality reduction, as an important form of unsupervised learning, has been widely explored for analyzing complex data such as images, video sequences, text documents, etc.
This paper addresses the problem of focused active inference: selecting a subset of observable random variables that is maximally informative with respect to a specified subset of latent random variables.
In this paper, we study a variant of online learning, called online probing, which is motivated by practical problems where there is a cost to observing the features that may help one’s predictions.
Non-cooperative game theory is a formal mathematical framework for describing behavior of interacting self-interested agents.
In pool-based active learning [1], we select training data from a finite set (called a pool) of unlabeled examples and aim to obtain good performance on the set by asking for as few labels as possible.
Statistical models of neural spike recordings have greatly facilitated the study of both intra-neuron spiking behavior and the interaction between populations of neurons.
The phrase “one-shot learning” has been used to describe our ability – as humans – to correctly recognize and understand objects (e.
Image blur is an undesirable degradation that often accompanies the image formation process and may arise, for example, because of camera shake during acquisition.
Prediction with expert advice —see, e.
Let {y1 , y2 , .
Over the past years, multi-armed bandit (MAB) algorithms have been employed in an increasing amount of large-scale applications.
Learning problems with binary labels, where one is given training examples consisting of objects with binary labels (such as emails labeled spam/non-spam or documents labeled relevant/irrelevant), are widespread in machine learning.
The subspace learning problem is that of finding the smallest linear space supporting data drawn from an unknown distribution.
The proper setting of high-level hyperparameters in machine learning algorithms – regularization weights, learning rates, etc.
Effective models in complex computer vision and natural language problems try to strike a favorable balance between accuracy and speed of prediction.
Markov decision processes (MDPs) [Puterman, 1994] have been widely used to model and solve sequential decision problems in stochastic environments.
In recent years, the music industry has shifted more and more towards digital distribution through online music stores and streaming services such as iTunes, Spotify, Grooveshark and Google Play.
Mapping a problem involving discrete variables into continuous variables often results in a more tractable formulation.
The usual optimization criteria for an infinite horizon Markov decision process (MDP) are the expected sum of discounted rewards and the average reward.
An outlier, which is “an observation which deviates so much from other observations as to arouse suspicions that it was generated by a different mechanism” (by Hawkins [10]), appears in many reallife situations.
Computing the stationary distribution of a Markov chain (MC) with a very large state space (finite, or countably infinite) has become central to statistical inference.
Suppose a set of k centers {pi }ki=1 is selected by approximate minimization of k-means cost; how does the fit over the sample compare with the fit over the distribution? Concretely: given m points sampled from a source distribution ρ, what can be said about the quantities   Z m 1 X    2 2 min kxj − pi k2 − min kx − pi k2 dρ(x)  i i m  j=1   ! ! Z m k k 1 X  X X   ln αi pθi (xj ) − ln αi pθi (x) dρ(x)  m  j=1 i=1 i=1 (k-means), (1.
An important instance of the framework of prediction with expert advice —see, e.
We study the precision matrix estimation problem: let X = (X1 , .
Matrix completion concerns the problem of recovering a low-rank matrix from a limited number of observed entries.
In this paper, we study the problem of estimating the cluster tree of a density when the density is supported on or near a manifold.
Many tasks of recommender systems can be formulated as recovering an unknown tensor (multiway array) from a few observations of its entries [17, 26, 25, 21].
Majorization-minimization [15] is a simple optimization principle for minimizing an objective function.
In the matrix completion problem we observe a subset of the entries of a target matrix Y , and our aim is to retrieve the rest of the matrix.
Consider the simple task of learning a threshold classifier in 1D (Figure 1).
Recently, the machine learning and signal processing communities have focused considerable attention toward understanding the benefits of adaptive sensing.
Principal component analysis (PCA) is a popular form of dimensionality reduction that projects a data set on the top eigenvector(s) of its covariance matrix.
Contemporary statistical procedures are making inroads into a diverse range of applications in the natural sciences and engineering.
Active learning algorithms seek to mitigate the cost of learning by using unlabeled data and sequentially selecting examples to query for their label to minimize total number of queries.
Mobile manipulator robots have arms with high degrees of freedom (DoF), enabling them to perform household chores (e.
Neural signals from electrodes implanted in cortex [1], electrocorticography (ECoG) [2], and electroencephalography (EEG) [3] all have been used to decode motor intentions and control motor prostheses.
A host of machine-learning problems can be solved effectively as approximations of such NP-hard combinatorial problems as set cover, set packing, and multiway-cuts [8, 11, 16, 22].
The movement between the specification of “local” marginals and models for complete joint distributions is ingrained in the language and methods of modern probabilistic inference.
Decomposition of tensors [10, 14] (or multi-way arrays) into low-rank components arises naturally in many real world data analysis problems.
The ability of a website to present personalized content recommendations is playing an increasingly crucial role in achieving user satisfaction.
The standard approach to learning models from data assumes that the data were generated by a certain model, and the goal of learning is to recover this generative model.
Most classic machine learning methods depend on the assumption that humans can annotate all the data available for training.
What makes a teacher effective? A critical factor is their instructional policy, which specifies the manner and content of instruction.
In recent years there has been increasing interest in probabilistic models where the latent variables or parameters of interest are discrete probability distributions over K items, i.
Real-world data are often presented as a graph where the nodes in the graph bear labels that vary smoothly along edges.
The desire to apply machine learning to increasingly larger datasets has pushed the machine learning community to address the challenges of distributed algorithm design: partitioning and coordinating computation across the processing resources.
Statistical analysis of social networks and other relational data is becoming an increasingly important problem as the scope and availability of network data increases.
How humans achieve long-term goals in an uncertain environment, via repeated trials and noisy observations, is an important problem in cognitive science.
Many machine learning problems can be interpreted as matching two objects, e.
When a trader enters a market, say a stock or commodity market, with the desire to buy or sell a certain quantity of an asset, how is this trader guaranteed to find a counterparty to agree to transact at a reasonable price? This is not a problem in a liquid market, with a deep pool of traders ready to buy or sell at any time, but in a thin market the lack of counterparties can be troublesome.
Object detection and segmentation approaches often assume that the training and test samples are drawn from the same distribution.
Although both stochastic optimization [17, 4, 18, 10, 26, 20, 22] and multiple objective optimization [9] are well studied subjects in Operational Research and Machine Learning [11, 12, 24], much less is developed for stochastic multiple objective optimization, which is the focus of this work.
A wide range of statistical models have been proposed for the discovery of hidden communities within observed networks.
The focus of this paper is energy minimization for Markov random fields.
Hippocampus, olfactory cortex, and other brain regions are thought to operate as associative memories [1,2], having the ability to learn patterns from presented inputs, store a large number of patterns, and retrieve them reliably in the face of noisy or corrupted queries [3–5].
Although sensory stimuli are high-dimensional, sensory neurons are typically sensitive to only a small number of stimulus features.
Encoding high-dimensional objects using short binary hashes can be useful for fast approximate similarity computations and nearest neighbor searches.
Statistical learning theory, especially the theory of “probably approximately correct” (PAC) learning, has mostly developed under the assumption that data are independent and identically distributed (IID) samples from a fixed, though perhaps adversarially-chosen, distribution.
Measuring statistical dependence between random variables is a fundamental problem in statistics.
Principal component analysis is a fundamental tool for dimensionality reduction, clustering, classification, and many more learning tasks.
Recovering scene properties (shape, illumination, reflectance) that led to the generation of an image has been one of the fundamental problems in computer vision.
Perceptrons are paradigmatic building blocks of neural networks [1].
Direct policy search methods have the potential to scale gracefully to complex, high-dimensional control tasks [12].
Policy gradient methods have established as the most effective reinforcement–learning techniques in robotic applications.
Motivation.
Our lives are embedded in networks–social, biological, communication, etc.
Undirected graphical models, or Markov random fields (MRFs), are a popular class of statistical models for representing distributions over a large number of variables.
Probabilistic generative models are used to mathematically formulate the generation process of observed data.
Low-rank matrix factorization techniques like the singular value decomposition (SVD) constitute an important tool in data analysis yielding a compact representation of data points as linear combinations of a comparatively small number of ’basis elements’ commonly referred to as factors, components or latent variables.
For humans and other primates, action recognition is an important ability that facilitates social interaction, as well as recognition of threats and intentions.
Rapid growth in the size and scale of datasets has fueled increasing interest in statistical estimation in distributed settings [see, e.
Large-scale classification of textual and visual data into a large number of target classes has been the focus of several studies, from researchers and developers in industry and academia alike.
Clustering aims to summarize observed data by grouping its elements according to their similarities.
An important aspect of deciphering the neural code is to determine those stimulus features populations of sensory neurons are most sensitive to.
Undirected graphical models, a.
Visual clutter, defined colloquially as a “confused collection” or a “crowded disorderly state”, is a dimension of image understanding that has implications for applications ranging from visualization and interface design to marketing and image aesthetics.
Distributed representations of words in a vector space help learning algorithms to achieve better performance in natural language processing tasks by grouping similar words.
Graphical models are used in a wide variety of domains, both to provide compact representations of probability distributions for rapid, efficient inference, and also to represent complex causal structures.
Online marketplaces such as Walmart, Netflix, and Amazon store information about their customers and the products they purchase in binary matrices.
Bayesian inference is computationally expensive.
Detecting anomalous activity refers to determining if we are observing merely noise (business as usual) or if there is some signal in the noise (anomalous activity).
Computer vision has historically been formulated as the problem of producing symbolic descriptions of scenes from input images [10].
Probabilistic logical modes (PLMs) combine elements of first-order logic with graphical models to succinctly model complex, uncertain, structured domains [5].
In the past several years, significant strides have been made in scaling up plan synthesis techniques.
Graph-based learning is by now well established in machine learning and is the standard way to deal with data that encode pairwise relationships.
Progress in neural recording technology has made it possible to record spikes from ever larger populations of neurons [1].
Two puzzles present themselves to language users: What do words mean in general, and what do they mean in context? Consider the utterances “it’s raining,” “I ate some of the cookies,” or “can you close the window?” In each, a listener must go beyond the literal meaning of the words to fill in contextual details (“it’s raining here and now”), infer that a stronger alternative is not true (“I ate some but not all of the cookies”), or more generally infer the speaker’s communicative goal (“I want you to close the window right now because I’m cold”), a process known as pragmatic reasoning.
High-dimensional representations have become very popular in modern applications of machine learning, computer vision, and information retrieval.
Stochastic grammars are traditionally used to represent natural language syntax and semantics, but they have also been extended to model other types of data like images [1, 2, 3] and events [4, 5, 6, 7].
We consider a discrete-time dynamic system whose state transition depends on a control.
This paper looks at the information leaked by online learning algorithms, and seeks to design accurate learning algorithms with rigorous privacy guarantees – that is, algorithms that provably leak very little about individual inputs.
The original motivation for providing privacy in statistical problems, first discussed by Warner [23], was that “for reasons of modesty, fear of being thought bigoted, or merely a reluctance to confide secrets to strangers,” respondents to surveys might prefer to be able to answer certain questions non-truthfully, or at least without the interviewer knowing their true response.
One core operation in computer vision involves evaluating a bank of templates at a set of sample locations in an image.
The problem of nonparametric testing of interaction between variables has been widely treated in the machine learning and statistics literature.
The comparison problem asks which of a number of objects has a higher value on an unobserved criterion.
Random utility models (RUM), which presume agent utility to be composed of a deterministic component and a stochastic unobserved error component, are frequently used to model choices by individuals over alternatives.
Both artificial and natural sensing systems face the challenge of making sense out of a continuous stream of noisy sensory inputs.
Gaussian Process (GP) learning and inference are computationally prohibitive with large datasets, having time complexities O(n3 ) and O(n2 ), where n is the number of training points.
Minimax analysis has recently been shown to be a powerful tool for the construction of online learning algorithms [Rakhlin et al.
Finding the correct bijection between two sets of objects X = {x1 , x2 , .
The problem of finding a low–rank matrix that (approximately) satisfies a given set of conditions has recently generated a lot of interest in many communities.
Despite a long history of prior work, human body pose estimation, or specifically the localization of human joints in monocular RGB images, remains a very challenging task in computer vision.
Several methods have been proposed to solve the image denoising problem including anisotropic diffusion [15], frequency-based methods [26], Bayesian and Markov Random Fields methods [20], locally adaptive kernel-based methods [17] and sparse representation [10].
In this paper, we consider the minimization of block-seperable convex functions subject to linear constraints, with a canonical form: J J X X min f (x) = fj (xj ) , s.
Neuroscience has made significant progress in learning how activity in specific neurons or brain areas correlates with behavior.
Matching two potentially heterogenous language objects is central to many natural language applications [28, 2].
Recommendation systems have become ubiquitous in our lives, helping us filter the vast expanse of information we encounter into small selections tailored to our personal tastes.
Modern statistical estimation is routinely faced with real world problems where the number of parameters p handily outnumbers the number of observations n.
Kernel machines have become widely used in many machine learning problems, including classification, regression, and clustering.
A permutation-valued function, also called a ranking function, outputs a ranking over a set of objects given features corresponding to the objects, and learning such ranking functions given data is becoming an increasingly key machine learning task.
Deep connectionist architectures involve many layers of nonlinear information processing [1].
1.
Stimulus information is encoded in neuronal responses.
In this paper, we deal with binary prediction in metric spaces.
Recognition of human actions in videos is a challenging task which has received a significant amount of attention in the research community [11, 14, 17, 26].
Covariance matrices are a key ingredient in many algorithms in signal processing, machine learning and statistics.
We consider problems where the goal is to detect outstanding events or extreme values in domains such as outlier detection [1], security [18], or medicine [17].
Data in scientific and commercial disciplines are increasingly characterized by high dimensions and relatively few samples.
It has long been recognised that the firing properties of cortical neurons are not constant over time, but that neural systems can exhibit multiple distinct firing regimes.
Many object recognition schemes, inspired from biological vision, are based on feed-forward hierarchical architectures, e.
Canonical Correlation Analysis (CCA) is a widely used spectrum method for finding correlation structures in multi-view datasets introduced by [15].
A triumph of machine learning is the ability to predict many human aspects: is certain mail spam or not, is a news-item of interest or not, does a movie meet one’s taste or not, and so on.
To cope with the rich variety of transformations in natural images, recognition systems require a representative sample of possible variations.
Learning in animals involves the active gathering of sensor data, presumably selecting those sensor inputs that are most useful for learning a model of the world.
Many learning problems can be formulated in terms of inference on predictive stochastic models.
Conventional methods for real-time abstract planning over options in reinforcement learning require a single pre-specified reward function, and these methods are not efficient in settings with multiple reward functions that can be specified at any time.
Nonlinear inversion problems, where we wish to infer the latent inputs to a system given observations of its output and the system’s forward-model, have a long history in the natural sciences, dynamical modeling and estimation.
On-line learning has received much attention in recent years.
Given a design matrix X 2 Rn⇥d and a response matrix Y 2 Rn⇥m , we consider a multivariate linear model Y = XB0 + Z, where B0 2 Rd⇥m is an unknown regression coefficient matrix and Z 2 Rn⇥m is a noise matrix [1].
Neural network-based architectures have recently had great success in significantly advancing the state of the art on challenging image classification and object detection datasets [8, 12, 19].
Besides accuracy and sample efficiency, computational cost is a crucial design criterion for machine learning algorithms in real-time settings, such as control problems.
In the spiked covariance model proposed by [JL04], we are given data x1 , x2 , .
Many application domains of machine learning use massive data sets in dense medium-dimensional or sparse high-dimensional spaces.
One of the most significant recent developments in machine learning has been the resurgence of “deep learning”, usually in the form of artificial neural networks.
A boosting algorithm can be seen as a meta-algorithm that maintains a distribution over the sample space.
Consider a linear inverse problem of the form: y = Ax + e, N ×D (1) N where A ∈ R is the measurement matrix, y ∈ R is the measurement vector, x ∈ RD is the desired solution and e ∈ RN is a vector of corruptive noise.
The general perception is that kernel methods are not scalable.
With the advent of online crowdsourcing services such as Amazon Mechanical Turk, crowdsourcing has become an appealing way to collect labels for large-scale data.
We consider the reinforcement learning (RL) problem of optimizing rewards in an unknown Markov decision process (MDP) [1].
The generic viewpoint assumption (GVA) [5, 9, 21, 22] postulates that what we see in the world is not seen from a special viewpoint, or lighting condition.
Consider the following fundamental statistical task: Given independent draws from an unknown probability distribution, what is the minimum sample size needed to obtain an accurate estimate of the distribution? This is the question of density estimation, a classical problem in statistics with a rich history and an extensive literature (see e.
In the era of big data, a natural idea is to select a small subset of m samples Ce = {xe1 , .
A number of real world applications model data as being sampled from a union of independent subspaces.
Judging a person as a friend or foe, a mushroom as edible or poisonous, or a sound as an \l\ or \r\ are examples of categorization tasks.
Graphical models in computer vision Optimization of undirected graphical models such as Markov Random Fields, MRF, or Conditional Random Fields, CRF, is of fundamental importance in computer vision.
Machine learning has recently experienced a proliferation of problem settings that, to some extent, enrich the classical dichotomy between supervised and unsupervised learning.
A detailed understanding of brain function is a still-elusive grand challenge.
Principal Component Analysis (PCA) aims at recovering the top k leading eigenvectors u1 , .
Musical rhythm occurs in all human societies and is related to many phenomena, such as the perception of a regular emphasis (i.
Determining the topology of macro-scale functional networks in the brain and micro-scale neural networks has important applications to disease diagnosis and is an important step in understanding brain function in general [11, 19].
Kernel methods and methods based on integral operators have become one of the central areas of machine learning and learning theory.
One typical vision problem usually comprises several subproblems, which tend to be tackled jointly to achieve superior capability.
Given a data matrix X, Principal Component Analysis (PCA) can be regarded as a ‘denoising’ technique that replaces X by its closest rank-one approximation.
Graphical models have had tremendous impact in a variety of application domains.
Imagine independently consulting a small set of medical experts for the purpose of reaching a binary decision (e.
Many important problems including sensor placement [3], image co-segmentation [4], MAP inference for determinantal point processes [5], influence maximization in social networks [6], and document summarization [7] may be expressed as the maximization of a submodular function.
The kernel mean or the mean element, which corresponds to the mean of the kernel function in a reproducing kernel Hilbert space (RKHS) computed w.
Markov Logic Networks (MLNs) [5] are powerful template models that define Markov networks by instantiating first-order formulas with objects from its domain.
Exact inference in Markov Random Fields (MRFs) is generally intractable, motivating approximate algorithms.
Reinforcement learning (RL) and approximate dynamic programming (ADP) [24, 2] are effective approaches to solve the problem of decision-making under uncertainty.
Learning sparse polynomials over the Boolean domain is one of the fundamental problems from computational learning theory and has been studied extensively over the last twenty-five years [1– 6].
Matrix factorization methods currently enjoy a large popularity in machine learning and signal processing.
The stochastic multi-armed bandit problem (MAB) [16] offers a simple formalization for the study of sequential design of experiments.
The focus of this paper is on the problem of Maximum Inner Product Search (MIPS).
In [1], Nesterov introduced a primal-dual technique, called the excessive gap, for constructing and analyzing first-order methods for nonsmooth and unconstrained convex optimization problems.
The emergence of social graphs of the World Wide Web has had a considerable effect on propagation of ideas or information.
Graphical models provide a mechanism for expressing the relationships among a collection of variables.
With the advent of massively open online courses (MOOCs) and online learning platforms such as Khan Academy and Reasoning Mind, large volumes of data are collected from students as they solve exercises, acquire cognitive skills, and achieve a conceptual understanding.
Online social platforms routinely track and record a large volume of event data, which may correspond to the usage of a service (e.
The accuracy of Natural Language Processing (NLP) tools for a given language depend heavily on the availability of annotated resources in that language.
Marginal inference and estimating the partition function for undirected graphical models, also called Markov random fields (MRFs), are fundamental problems in machine learning.
Spectral decomposition of large-scale graphs is one of the most informative and fundamental matrix approximations.
Classical statistical theory studies the rate at which the error in an estimation problem decreases as the sample size increases.
We would like to live in a world where we can define a probabilistic model, press a button, and get accurate inference results within a matter of seconds or minutes.
The last decade has witnessed a tremendous growth in the amount of data involved in machine learning tasks.
1.
Time series of financial returns often exhibit heteroscedasticity, that is the standard deviation or volatility of the returns is time-dependent.
While value/utility is a useful abstraction for macroeconomic applications, it has little psychological validity [1].
We are interested in general strategies for sequential prediction and decision making (a.
Background and motivation.
Bipartite ranking aims to learn a real-valued ranking function that places positive instances above negative instances.
State aggregation is one of the simplest approximate methods for reinforcement learning with very large state spaces; it is a special case of linear value function approximation with binary features.
Faces of the same identity could look much different when presented in different poses, illuminations, expressions, ages, and occlusions.
In many machine learning applications, different datasets may reside on different but highly correlated manifolds.
Finding an hyperplane that minimizes the number of misclassifications is N P-hard.
Many domains today—vision, speech, biology, and others—are flush with data.
The multi-armed bandit problem is a reinforcement learning problem with K actions.
We consider Partial Monitoring, a repeated game where in every time step a learner chooses an action while, simultaneously, an opponent chooses an outcome.
Object recognition is a central problem in vision.
Since 2006 there has been a boost in machine learning due to improvements in the field of unsupervised learning of representations.
This paper concerns two algorithms which until now have remained somewhat disjoint in the literature: the randomized Kaczmarz algorithm for solving linear systems and the stochastic gradient descent (SGD) method for optimizing a convex objective using unbiased gradient estimates.
Many real-world Reinforcement Learning (RL) problems combine the challenges of closed-loop action (or policy) selection with the already significant challenges of high-dimensional perception (shared with many Supervised Learning problems).
Recent work in statistics has focused on high-dimensional inference problems where the number of parameters p equals or exceeds the number of samples n.
We study the problem of ranking a set of n items given pairwise comparisons between these items.
Active learning has been the subject of significant theoretical and experimental study in machine learning, due to its potential to greatly reduce the amount of labeling effort needed to learn a given target function.
Tensor data analysis have witnessed increasing applications in machine learning, data mining and computer vision.
Suppose we are given a linear subspace S of a high-dimensional space Rp , which contains a sparse vector x0 6= 0.
Graphical models are a powerful framework for succinct representation of complex highdimensional distributions.
In recent years, the alternating direction method of multipliers (ADMM) [4] has been successfully used in a broad spectrum of applications, ranging from image processing [11, 14] to applied statistics and machine learning [26, 25, 12].
Social choice studies the design and evaluation of voting rules (or rank aggregation rules).
Traditional building blocks for deep learning have some unsatisfactory properties.
Recently, Multilayer1 Neural Networks (MNNs) with deep architecture have achieved state-of-theart performance in various supervised learning tasks [11, 14, 8].
The problem of aligning temporal sequences is ubiquitous in applications ranging from bioinformatics [5, 1, 23] to audio processing [4, 6].
Consider the following optimization problem min x h(x) , f (x1 , .
The point process generalized linear model (GLM) has provided a useful and highly tractable tool for characterizing neural encoding in a variety of sensory, cognitive, and motor brain areas [1–5].
Learning Bayesian network parameters is the problem of estimating the parameters of a known structure given a dataset.
In numerous learning problems the decision maker is provided with vast amounts of different types of information which it can utilize to learn how to select actions that lead to high rewards.
People like to look at examples.
Graphical models are a very useful tool to capture the dependencies between the variables of interest.
As data sets and problems are ever increasing in size, accelerating first-order methods is both of practical and theoretical interest.
Logistic regression (LR) is a standard probabilistic statistical classification model that has been extensively used across disciplines such as computer vision, marketing, social sciences, to name a few.
Statistical tests based on distribution embeddings into reproducing kernel Hilbert spaces have been applied in many contexts, including two sample testing [18, 15, 32], tests of independence [17, 33, 4], tests of conditional independence [14, 33], and tests for higher order (Lancaster) interactions [24].
We denote by x1 , .
Our ability to effortlessly extrapolate patterns is a hallmark of intelligent systems: even with large missing regions in our field of view, we can see patterns and textures, and we can visualise in our mind how they generalise across space.
In many statistical estimation problems, observations can be modeled as noisy quadratic functions of an unknown vector v0 = (v0,1 , v0,2 , .
With the immense growth of available data, developing distributed algorithms for machine learning is increasingly important, and yet remains a challenging topic both theoretically and in practice.
Clustering is an important problem in unsupervised learning that deals with grouping observations (data points) appropriately based on their similarities or distances [20].
From the considerable amount of recent research on high-dimensional statistical estimation, it has now become well understood that it is vital to impose structural constraints upon the statistical model parameters for their statistically consistent estimation.
Bayesian networks are graphical models widely used to represent joint probability distributions on complex multivariate domains.
The motivation for this paper started with a question: Why are the number of samples needed for Reinforcement Learning (RL) in practice so much smaller than those given by theory? Can we improve this? In Markov Decision Processes (MDPs, Puterman (1994)), when the performance is measured by (1) the sample complexity (Kearns and Singh, 2002; Kakade, 2003; Strehl and Littman, 2008; Szita and Szepesvári, 2010) or (2) the regret (Bartlett and Tewari, 2009; Jaksch, 2010; Ortner, 2012), algorithms have been developed that achieve provably near-optimal performance.
Both classification and detection are key visual recognition challenges, though historically very different architectures have been deployed for each.
Devising ensembles of base predictors is a standard approach in machine learning which often helps improve performance in practice.
Visual recognition research has achieved major successes in recent years using large datasets and discriminative learning algorithms.
The efficient harnessing of renewable energy has become paramount in an era characterized by decreasing natural resources and increasing pollution.
Matrix completion has attracted a lot of contributions over the past decade.
A range of machine learning problems such as link prediction in graphs containing community structure [16], phase retrieval [5], subspace clustering [18] or dictionary learning [12] amount to solve sparse matrix factorization problems, i.
During the past few years, hashing has become a popular tool for tackling a variety of large-scale computer vision and machine learning problems including object detection [6], object recognition [35], image retrieval [22], linear classiﬁer training [19], active learning [24], kernel matrix approximation [34], multi-task learning [36], etc.
Let p∗ (x) be a probability density on Rd corresponding to a random vector X = (X1 , .
Several problems in science and engineering require estimating a real-valued, non-linear (and often non-convex) function f defined on a compact subset of Rd in high dimensions.
Synaptic plasticity is believed to be the fundamental building block of learning and memory in the brain.
Big Data challenge modern data analysis in terms of large dimension, insufficient sample and the inhomogeneity.
Binary classification performance is often measured using metrics designed to address the shortcomings of classification accuracy.
Pairwise clustering methods partition the data into a set of self-similar clusters based on the pairwise similarity between the data points.
Computing the dominant singular vectors of a matrix is one of the most important algorithmic tasks underlying many applications including low-rank approximation, PCA, spectral clustering, dimensionality reduction, matrix completion and topic modeling.
Conditional random fields [24] are used to model structure in numerous problem domains, including natural language processing (NLP), computational biology, and computer vision.
Among the many probabilistic models over permutations, models based on penalizing inversions with respect to a reference permutation have proved particularly elegant, intuitive, and useful.
There has been an increasing interest in generative models for unsupervised learning, with many applications in Image processing [1, 2], natural language processing [3, 4], vision [5] and audio [6].
A substantial body of work has examined the optimality of neural population codes [1–19].
One of the most detailed and widely accepted models of the neuron is the Hodgkin Huxley (HH) model [1].
The hidden Markov model (HMM) [1] is one of the most widely and successfully applied statistical models for the description of discrete time series data.
Different types of multiple data modalities can be used to describe the same event.
Inferring the distributions of latent variables is a key tool in statistical modeling.
In the big data era, many applications require solving optimization problems with billions of variables on a huge amount of training data.
Consider the model yi = µ(xi ) + i , i ∼ N (0, σ 2 I), (1) p where µ(x) is an arbitrary function, and xi ∈ R .
Nowadays our data are often high-dimensional, massive and full of gross errors (e.
Action recognition in real-world videos has many potential applications in multimedia retrieval, video surveillance and human computer interaction.
Undirected graphical models are a familiar framework in diverse application domains such as computer vision, statistical physics, coding theory, social science, and epidemiology.
Relational and graph-structured data has become ubiquitous in many fields of application such as social network analysis, bioinformatics, and artificial intelligence.
Signals associated with nodes or edges of a graph arise in a number of applications including sensor network intrusion, disease outbreak detection and virus detection in communication networks.
There are numerous applications of higher-order tensors in machine learning [22, 29], signal processing [10, 9], computer vision [16, 17], data mining [1, 2], and numerical linear algebra [14, 21].
Probabilistic modeling of ranking data is an extensively studied problem with a rich body of past work [1, 2, 3, 4, 5, 6, 7, 8, 9].
This paper establishes the asymptotic normality of a nonparametric estimator of the f -divergence between two distributions from a finite number of samples.
Graphical models provide compact representations of multivariate distributions using graphs that represent Markov conditional independencies in the distribution.
Modern deep neural networks exhibit a curious phenomenon: when trained on images, they all tend to learn first-layer features that resemble either Gabor filters or color blobs.
Coverage functions are a special class of the more general submodular functions which play important role in combinatorial optimization with many interesting applications in social network analysis [1], machine learning [2], economics and algorithmic game theory [3], etc.
Clustering is a fundamental form of data analysis that is applied in a wide variety of domains, from astronomy to zoology.
The US Coast Guard, the Federal Air Marshal Service, the Los Angeles Airport Police, and other major security agencies are currently using game-theoretic algorithms, developed in the last decade, to deploy their resources on a regular basis [13].
Bayesian approaches to machine learning problems inevitably call for the frequent approximation of computationally intractable integrals of the form Z Z = h`i = `(x) π(x) dx, (1) where both the likelihood `(x) and prior π(x) are non-negative.
A fundamental problem in sequential decision making is controlling an agent when the environmental dynamics are only partially known.
The multivariate Gaussian (Normal) distribution is ubiquitous in statistical applications in machine learning, signal processing, computational biology, and others.
Advancements in sensory technologies and digital storage media have led to a prevalence of “Big Data” collections that have inspired an avalanche of recent efforts on “scalable” machine learning (ML).
Most tasks in natural language processing and understanding involve looking at words, and could benefit from word representations that do not treat individual words as unique symbols, but instead reflect similarities and dissimilarities between them.
Event time data is often modeled as an inhomogeneous Poisson process, whose rate λ(t) as a function of time t has to be learned from the data.
Many important quantities in machine learning and statistics can be viewed as integral functionals of one of more continuous probability densities; that is, quanitities of the form Z F (p1 , · · · , pk ) = f (p1 (x1 ), .
Articulated pose estimation is one of the fundamental challenges in computer vision.
In this paper we study the problem of graph transduction on a simple, undirected graph G = (V, E), with vertex set V = [n] and edge set E ⊆ V ×V .
State-space models (SSMs) are a widely used class of models that have found success in applications as diverse as robotics, ecology, finance and neuroscience (see, e.
Sparse-Group Lasso (SGL) [5, 16] is a powerful regression technique in identifying important groups and features simultaneously.
In recent years there has been a great deal of interest in the problem of learning a low rank matrix from a set of linear measurements.
A growing fraction of Internet advertising is sold through automated real-time ad exchanges.
In recent years object detectors have undergone an impressive transformation [11, 32, 14].
The Dantzig Selector (DS) [3, 5] provides an alternative to regularized regression approaches such as Lasso [19, 22] for sparse estimation.
Memories are thought to be encoded in the joint, persistent activity of groups of neurons.
Since it was raised in 2009, Curriculum Learning (CL) [1] has been attracting increasing attention in the field of machine learning and computer vision [2].
In realistic industrial machine learning applications the datasets range from 1TB to 1PB.
In many classification problems, the input is represented as a set of features.
Despite being introduced over a decade ago, random forests remain one of the most popular machine learning tools due in part to their accuracy, scalability, and robustness in real-world classification tasks [3].
Multitask learning (MTL) captures and exploits the relationship among multiple related tasks and has been empirically and theoretically shown to be more effective than learning each task independently.
We consider a general class of online decision-making problems, where a learner sequentially decides which actions to take from a given decision set and suffers some loss associated with the decision and the state of the environment.
Information retrieval systems require us to rank a set of samples according to their relevance to a query.
Gaussian processes have been shown to be flexible models that are able to capture complicated structure, without succumbing to over-fitting.
Learning to rank is a problem of ordering a set of items according to their relevances to a given context [8].
Stochastic optimization techniques have been extensively employed for online machine learning on data which is uncertain, noisy or missing.
Maximum a posteriori (MAP) inference in Markov random fields (MRFs) is an important problem with abundant applications in computer vision [1], computational biology [2], natural language processing [3], and others.
Modern learning applications frequently require a level of fine-grained control over prediction performance that is not offered by traditional “per-point” performance measures such as hinge loss.
There is significant value in the ability to associate natural language descriptions with images.
There are many learning problems where classifiers must make accurate decisions quickly.
Recent years have unveiled central contact points between the areas of statistical and online learning.
Symmetric Positive Definite (SPD) matrices, in particular covariance matrices, have been playing an increasingly important role in many areas of machine learning, statistics, and computer vision, with applications ranging from kernel learning [12], brain imaging [9], to object detection [24, 23].
Distributed word representations have enjoyed success in several NLP tasks [1, 2].
Prior knowledge is a crucial component of any learning system as without a form of prior knowledge learning is provably impossible [1].
Probabilistic Graphical Models (PGMs) provide a principled approach to approximate constraint optimization for NP-hard problems.
Background.
Large-scale clustering of data points in metric spaces is an important problem in mining big data sets.
Pairwise classification aims to determine if two examples belong to the same class.
Many planning problems from diverse areas such as urban planning, social networks, and transportation can be cast as stochastic network design, where the goal is to take actions to enhance connectivity in a network with some stochastic element [1–8].
In many applications, one obtains measurements (xi , yi ) for which the response y is related to x via some mixture of known kernel functions fθ (x), and the goal is to recover the mixture parameters θk and their associated weights: yi = K X wk fθk (x) + i (1) k=1 where fθ (x) is a known kernel function parameterized by θ, and θ = (θ1 , .
There are multiple scenarios where we might wish to reconstruct a symmetric positive semidefinite (SPSD) matrix from a sampling of its entries.
Many domains in AI and machine learning (e.
Probabilistic graphical models have been extensively studied as a powerful tool for modeling a set of conditional independencies in a probability distribution [12].
We prove finite sample bounds for k-nearest neighbor (k-NN) density estimation, and subsequently apply these bounds to the related problem of mode estimation.
Spatio-temporal data provide unique information regarding “where” and “when”, which is essential to answer many important questions in scientific studies from geology, climatology to sociology.
The number of photographs being uploaded online is growing at an unprecedented rate.
Graph-based techniques for clustering have become very popular in machine learning as they allow for an easy integration of pairwise relationships in data.
Since data is often partitioned across multiple servers [20, 7, 18], there is an increased interest in computing on it in the distributed model.
Hierarchical Dirichlet Process (HDP) mixture models were first introduced by Teh et al.
Let us consider the problem of learning a classifier from positive and unlabeled data (PU classification), which is aimed at assigning labels to the unlabeled dataset [1].
Factored multi-agent MDPs [4] offer a powerful mathematical framework for studying multi-agent sequential decision problems in the presence of uncertainty.
It is often the case that an observed signal is a linear combination of some other target signals that one wishes to resolve from each other and from background noise.
Stochastic Gradient Descent (SGD) algorithms are gaining more and more importance in the Machine Learning community as efficient and scalable machine learning tools.
Undirected graphical models, also known as Markov random fields (MRFs), are a powerful class of statistical models, that represent distributions over a large number of variables using graphs, and where the structure of the graph encodes Markov conditional independence assumptions among the variables.
One of the most fundamental challenges in neuroscience is the “large-scale integration problem”: how does distributed neural activity lead to precise, unified cognitive moments [1].
Structure-from-motion is the ability to perceive the 3D shape of objects solely from motion cues.
Choice is a fundamental behavior of humans and has been studied extensively in Artificial Intelligence and related areas.
Dropout training [1] is an increasingly popular method for regularizing learning algorithms.
Sequential Monte Carlo (SMC) inference techniques require blocking barrier synchronizations at resampling steps which limit parallel throughput and are costly in terms of memory.
Topic models [1] assume that each document in a text corpus is generated from an ad-mixture of topics, where, each topic is a distribution over words in a Vocabulary.
Many brain circuits are known to maintain information over short periods of time in the firing of their neurons [15].
Markov random fields are widely used as priors for solving a variety of vision problems such as image restoration and stereo [5, 8].
The power of Online Convex Optimization (OCO) framework is in its ability to generalize many problems from the realm of online and statistical learning, and supply universal tools to solving them.
Object localization is often formulated as a binary classification problem, where a learned classifier determines the existence or absence of a target object within a candidate window of every location, size, and aspect ratio.
Structure from motion (SfM) is the task of jointly reconstructing 3D scenes and camera poses from a set of images.
The Higgs boson was observed for the first time in 2011-2012 and will be the central object of study when the Large Hadron Collider (LHC) comes back online to collect new data in 2015.
Remarkably, recent advances [1, 2] have shown that it is possible to minimise strongly convex finite sums provably faster in expectation than is possible without the finite sum structure.
In online learning problems [4] we aim to sequentially select actions from a given set in order to optimize some performance measure.
The main motivation behind current sparse estimation methods and regularized empirical risk minimization is the principle of parsimony, which states that simple explanations should be preferred over complex ones.
Logistic regression (LR) is a popular and well established classification method that has been widely used in many domains such as machine learning [4, 7], text mining [3, 8], image processing [9, 15], bioinformatics [1, 13, 19, 27, 28], medical and social sciences [2, 17] etc.
Spectral tensor methods and tensor decomposition are emerging themes in machine learning, but they remain global rather than “online.
Gaussian mixture models (GMMs) [1, 2, 3] have become a popular signal model for compressive sensing [4, 5] of imagery and video, partly because the information domain in these problems can be decomposed into subdomains known as pixel/voxel patches [3, 6].
Differential privacy [17] is a cryptographically motivated definition of privacy that has recently gained significant attention in the data mining and machine learning communities.
Blind deconvolution is an important inverse problem that gains increasing attentions from various fields, such as neural signal analysis [3, 10] and computational imaging [6, 8].
Recent work in computer vision relies heavily on manually labeled datasets to achieve satisfactory performance.
Neurons in sensory cortices organize into highly-interconnected circuits that share common input, dynamics, and function.
The growing amount of sparsely and noisily labeled image data demands robust detection methods that can cope with a minimal amount of supervision.
The term biclustering has been used to describe several distinct problems variants.
Olfaction is perhaps the most widespread sensory modality in the animal kingdom, often crucial for basic survival behaviours such as foraging, navigation, kin recognition, and mating.
Gaussian Processes (GPs) provide a flexible nonparametric prior over functions which can be used as a probabilistic module in both supervised and unsupervised machine learning problems.
Principal component analysis (PCA) is a common procedure for preprocessing and denoising, where a low rank approximation to the input matrix (such as the covariance matrix) is carried out.
Although the history of microfinance systems takes us back to as early as the 18th century, the foundation of the modern microfinance movement was laid in the 1970s by Muhammad Yunus, a then-young Economics professor in Bangladesh.
Sparse inverse covariance estimation has received tremendous attention in the machine learning, statistics and optimization communities.
Tensor data appears naturally in a number of applications [1, 2].
A full 90% of all the data in the world has been generated over the last two years and this expansion rate will not diminish in the years to come [17].
Fisher vector coding is a coding method derived from the Fisher kernel [1] which was originally proposed to compare two samples induced by a generative model.
Hierarchically structured clustering models offer a natural representation for many forms of data.
Clustering [1] broadly refers to the problem of identifying data points that are similar to each other.
Submodular functions [1] are a rich class of set functions F : 2V → R, investigated originally in game theory and combinatorial optimization.
For mixture modeling, there is a wide selection of nonparametric Bayesian priors, such as the Dirichlet process [1] and the more general family of normalized random measures with independent increments (NRMIs) [2, 3].
Nearest neighbor classifiers are among the oldest and the most widely used tools in machine learning.
Large-scale Bayesian inference remains intractable for many models, such as logistic regression, sparse linear models, or dynamical systems with non-Gaussian observations.
The geometric structure of a data domain can be described with a graph [11], where neighbor data points are represented by vertices related by an edge.
As we enter the age of “big data”, datasets are growing to ever increasing sizes and there is an urgent need for scalable machine learning algorithms.
A large body of recent work demonstrates that many discrete problems in machine learning can be phrased as the optimization of a submodular set function [2].
There is an increasing demand for systems that learn long-term, high-dimensional data streams.
The growing popularity of online crowdsourcing services (e.
In the last decade many algorithms for numerical linear algebra problems have been proposed, often providing substantial gains over more traditional algorithms based on the singular value decomposition (SVD).
The promise of deep learning is to discover rich, hierarchical models [2] that represent probability distributions over the kinds of data encountered in artificial intelligence applications, such as natural images, audio waveforms containing speech, and symbols in natural language corpora.
You are given a training set with 1M labeled points.
Machine learning, and especially probabilistic modeling, can be difficult to apply.
Estimating depth is an important component of understanding geometric relations within a scene.
Semi-supervised learning considers the problem of classification when only a small subset of the observations have corresponding class labels.
Recent advances in convolutional neural nets [2] dramatically improved the state-of-the-art in image classification.
Clustering and outlier detection are often studied as separate problems [1].
Optimal decision-making constitutes making optimal use of sensory information to maximize one’s overall reward, given the current task contingencies.
A typical workflow for converting a discrete optimization problem over the set of permutations of n objects into a continuous relaxation is as follows: (1) use permutation matrices to represent permutations; (2) relax to the convex hull of the set of permutation matrices — the Birkhoff polytope; (3) relax other constraints to ensure convexity/continuity.
Ensembles of models have long been used as a way to obtain robust performance in the presence of noise.
Segregating a speaker of interest in a multi-speaker environment is an effortless task we routinely perform.
To improve scalability of the widely used ordinary least squares algorithm, a number of randomized approximation algorithms have recently been proposed.
In recent years, developments in neural recording technologies have enabled the recording of populations of neurons from multiple brain areas simultaneously [1–7].
Stochastic variational inference (SVI) lets us scale up Bayesian computation to massive data [1].
Many practical tasks involve finding models that are both simple and capable of explaining noisy observations.
Latent Dirichlet allocation (LDA) [5] is a generative model successfully used in various applications such as text analysis [5], image analysis [15], genometrics [6, 4], human activity analysis [12], and collaborative filtering [14, 20]1 .
Feature selection (FS) is a fundamental and classic problem in machine learning [10, 4, 12].
Differential equations are a basic feature of dynamical systems.
Auctions have long been an active area of research in Economics and Game Theory [Vickrey, 2012, Milgrom and Weber, 1982, Ostrovsky and Schwarz, 2011].
In the last decade, estimating low rank matrices has attracted increasing attention in the machine learning community owing to its successful applications in a wide range of domains including subspace clustering [13], collaborative filtering [9] and visual texture analysis [25], to name a few.
In this paper, we study active learning of classiﬁers in an agnostic setting, where no assumptions are made on the true function that generates the labels.
Expectation-maximization (EM) [10], sampling methods [13], and matrix factorization [20, 25] are three algorithms commonly used to produce maximum likelihood (or maximum a posteriori (MAP)) estimates of models with latent variables/factors, and thus are used in a wide range of applications such as clustering, topic modeling, collaborative filtering, structured prediction, feature engineering, and time series analysis.
It is an impressive yet alarming fact that there is far more video being captured—by consumers, scientists, defense analysts, and others—than can ever be watched or browsed efficiently.
Interdependent Security (IDS) games [1] model the interaction among multiple agents where each agent chooses whether to invest in some form of security to prevent a potential loss based on both direct and indirect (transfer) risks.
Graphical models are a convenient tool to illustrate the dependencies among a collection of random variables with potentially complex interactions.
A sensible criterion for integration of partially redundant information from multiple senses is that no information about the underlying cause be lost.
Machine learning approaches have proven highly effective for statistical pattern recognition problems, such as those encountered in speech or vision.
Multivariate longitudinal ordinal/count data arise in many areas, including economics, opinion polls, text mining, and social science research.
Markov logic [4] uses weighted first order formulas to compactly encode uncertainty in large, relational domains such as those occurring in natural language understanding and computer vision.
Subset selection is a core task in many real-world applications.
Probabilistic graphical models (PGMs) such as Markov random fields (MRFs) and Bayesian networks (BNs) are widely used as a knowledge representation tool for reasoning under uncertainty.
There has been significant recent interest in extending multi-armed bandit techniques to address problems with more complex information structures, in which sampling one action can inform the decision-maker’s assessment of other actions.
We consider supervised multitask learning problems [1, 6, 7] in which the tasks are indexed by a pair of indices known as multilinear multitask learning (MLMTL) [17, 19].
Sparse representation represents a signal as the linear combination of a small number of atoms chosen out of a dictionary, and it has achieved a big success in various image processing and computer vision applications [1, 2].
The generation of random samples from a posterior distribution is a pervasive problem in Bayesian statistics which has many important applications in machine learning.
We are given n random variables V = {V1 , V2 , .
Subspace clustering is a classic problem where one is given points in a high-dimensional ambient space and would like to approximate them by a union of lower-dimensional linear subspaces.
Bayesian statistics provides a natural way to manage model complexity and control overfitting, with modern problems involving complicated models with a large number of parameters.
The estimation of a probability density function (pdf) from a random sample is a ubiquitous problem in statistics.
Data clustering is a classic unsupervised learning technique, whose goal is dividing input data into disjoint sets.
The central theme in approaches like kernel machines [1] and spectral clustering [2, 3] is the use of symmetric matrices that encode certain similarity relations between pairs of data instances.
Clustering algorithms aim to find a meaningful grouping of the samples at hand in an unsupervised manner for exploratory data analysis.
We define a set of simple linear learning problems described by an n dimensional square matrix M with ±1 entries.
To solve complex problems in real-time, intelligent agents have to make efficient use of their finite computational resources.
Kernel methods have become standard for building non-linear models from simple feature representations, and have proven successful in problems ranging across classification, regression, structured prediction and feature extraction [16, 20].
We consider a reinforcement learning agent that takes sequential actions within an uncertain environment with an aim to maximize cumulative reward [1].
Information constraints play a key role in machine learning.
Existing clustering methods fall roughly into two categories.
In an online combinatorial decision problem, the decision space is a set of combinatorial structures, such as subsets, trees, paths, permutations, etc.
Large neural networks have recently demonstrated impressive performance on a range of speech and vision tasks.
Multi-armed bandit (MAB) is a predominant model for characterizing the tradeoff between exploration and exploitation in decision-making problems.
Semantic labeling aims at getting pixel-wise dense labeling of an image in terms of semantic concepts such as tree, road, sky, water, foreground objects etc.
In linear regression, the goal is to predict the real-valued labels of data points in Euclidean space using a linear function.
Big data analysis challenges both statistics and computation.
Deep learning has recently achieved significant advances in several areas of perceptual computing, including speech recognition [1], image analysis and object detection [2, 3], and natural language processing [4].
Bayesian optimization techniques form a successful approach for optimizing black-box functions [5].
Several real-world applications routinely encounter multi-way data with structure which can be modeled as low-rank tensors.
A Markov random field (MRF) is a graph whose vertices are random variables, and whose edges specify a neighborhood over the random variables.
As vision techniques like segmentation and object recognition begin to mature, there has been an increasing interest in broadening the scope of research to full scene understanding.
Automatic music transcription is the task of transcribing a musical audio signal into a symbolic representation (for example MIDI or sheet music).
The nearest neighbor classifier for non-parametric classification is perhaps the most intuitive learning algorithm.
As the number of digital images which are available online is constantly increasing due to rapid advances in digital camera technology, image processing tools and photo sharing platforms, similaritypreserving binary codes have received significant attention for image search and retrieval in largescale image collections [1, 2].
Object categorization is a challenging problem that requires drawing boundaries between groups of objects in a seemingly continuous space.
`1 -regularized M -estimators have attracted considerable interest in recent years due to their ability to fit large-scale statistical models, where the underlying model parameters are sparse.
Among pre-processing methods, data partitioning is one of the most fundamental.
All branches of experimental science are plagued by missing data.
There has been significant interest recently in developing discriminative feature-learning models, in which the labels are utilized within a max-margin classifier.
In a classical transfer learning setting, we have sufficient fully labeled data from the source domain (or the training domain) where we fully observe the data points X tr , and all corresponding labels Y tr are known.
Many problems in Computer Vision, Natural Language Processing and Computational Biology involve mappings from an input space X to an exponentially large space Y of structured outputs.
Consider the problem of determining the connectivity structure of subsurface aquifers in a large ground-water system from time-series measurements of the concentration of tracers injected and measured at multiple spatial locations.
Sketching has emerged as a powerful dimensionality reduction technique for accelerating statistical learning techniques such as `p -regression, low rank approximation, and principal component analysis (PCA) [12, 5, 14].
Undirected probabilistic graphical models, also known as Markov Random Fields (MRFs), are a natural framework for modelling in networks, such as sensor networks and social networks [24, 11, 20].
Stochastic and online gradient descent methods have proved to be extremely useful for solving largescale machine learning problems [1, 2, 3, 4].
In statistical analyses involving data from individuals, there is an increasing tension between the need to share the data and the need to protect sensitive information about the individuals.
An integer-valued1 function f : 2X → Z defined over subsets of some finite ground set X of n elements is submodular if it satisfies the following diminishing marginal returns property: for every S ⊆ T ⊆ X and i ∈ X \ T , f (S ∪ {i}) − f (S) ≥ f (T ∪ {i}) − f (T ).
Over the past decades, our knowledge of how neural systems process static information has advanced considerably, as is well documented by the receptive field properties of neurons.
Many learning tasks require separating a time series into a linear combination of a larger number of “source” signals.
When faced with large datasets, it is commonly observed that using all the data with a simpler algorithm is superior to using a small fraction of the data with a more computationally intense but possibly more effective algorithm.
The adversarial multi-armed bandit problem [4] is a T -round prediction game played by a randomized player in an adversarial environment.
Given a set of individual preferences from multiple decision makers or judges, we address the problem of computing a consensus ranking that best represents the preference of the population collectively.
Recurrent Neural Networks (RNN) constitute a powerful computational tool for sequences modelling and prediction [1].
It is often the case that our geometric intuition, derived from experience within a low dimensional physical world, is inadequate for thinking about the geometry of typical error surfaces in high-dimensional spaces.
A cognitive map, as originally conceived by Tolman [46], is a geometric representation of the environment that can support sophisticated navigational behavior.
Artificial neural networks with several hidden layers, called deep neural networks, have become popular due to their unprecedented success in a variety of machine learning tasks (see, e.
Over the past decade, progress has been made in developing non-asymptotic bounds on the estimation error of structured parameters based on norm regularized regression.
Policy search methods can be divided into model-based algorithms, which use a model of the system dynamics, and model-free techniques, which rely only on real-world experience without learning a model [10].
Differential Dynamic Programming (DDP) is a powerful trajectory optimization approach.
Consider the problem of sequentially recommending content for a set of users.
Matching local visual features is a core problem in computer vision with a vast range of applications such as image registration [28], image alignment and stitching [6] and structure-from-motion [1].
When statistical predictors are deployed in a live production environment, feedback loops can become a concern.
Consider a learner who in each round t = 1, 2, .
Branch-and-bound (B&B) [1] is a systematic enumerative method for global optimization of nonconvex and combinatorial problems.
Structured prediction models are popularly used to solve structure dependent problems in a wide variety of application domains including natural language processing, bioinformatics, speech recognition, and computer vision.
The F1 -measure, defined as the harmonic mean of the precision and recall of a binary decision rule [20], is a traditional way of assessing the performance of classifiers.
This paper addresses the problem of solving large state-space Markov Decision Processes (MDPs)[16] in an infinite time horizon and discounted reward setting.
There has been a growing interest in high-dimensional statistical problems, where the number of parameters d is comparable to or even larger than the sample size n, spurred in part by many modern science and engineering applications.
Building rich generative models that are capable of extracting useful, high-level latent representations from high-dimensional sensory input lies at the core of solving many AI-related tasks, including object recognition, speech perception and language understanding.
Stochastic variational inference (SVI) is a powerful method for scaling up Bayesian computation to massive data sets [1].
The high speed of human sensory perception [1] is puzzling given its inherent computational complexity: sensory inputs are noisy and ambiguous, and therefore do not uniquely determine the state of the environment for the observer, which makes perception akin to a statistical inference problem.
Until recently, much of the emphasis in the theory of high-dimensional statistics has been on “first order” problems, such as estimation and prediction.
The performance of face recognition systems depends heavily on facial representation, which is naturally coupled with many types of face variations, such as view, illumination, and expression.
Recent years have seen a surge of work at the intersection of social choice and machine learning.
1.
Models of natural language need the ability to compose the meaning of words and phrases in order to understand complex utterances such as facts, multi-word entities, sentences or stories.
In this paper we introduce the Translation-invariant Matrix-T process (TiMT) for estimating Gaussian graphical models (GGMs) from pairwise distances.
The learnablity of regular languages is a classic topic in computational learning theory.
A standard optimization criterion for an infinite horizon Markov decision process (MDP) is the expected sum of (discounted) costs (i.
Recommender systems exploit fragmented information available from each user.
Biological systems face the difficult task of devising effective control strategies based on partial information communicated between sensors and actuators across multiple distributed networks.
It once seemed obvious that the running time of an algorithm should increase with the size of the input.
Extracting clusters or communities in networks have numerous applications and constitutes a fundamental task in many disciplines, including social science, biology, and physics.
In this paper we develop a probabilistic model of articles and reader behavior data.
Modern data-science applications increasingly require distributed learning algorithms to extract information from many data repositories stored at different locations with minimal interaction.
Bipartite ranking (scoring) amounts to rank (score) data from binary labels.
The explosion in both size and velocity of data has brought new challenges to the design of statistical algorithms.
Method of Moments (MoM) based algorithms [1, 2, 3] for learning latent variable models have recently become popular in the machine learning community.
Gaussian processes (GPs, [1]) are a popular choice in practical Bayesian non-parametric modeling.
Humans are able to routinely estimate unknown world states from ambiguous and noisy stimuli, and anticipate upcoming events by learning the temporal dynamics of relevant states of the world from incomplete knowledge of the environment.
In many important applications, we are faced with the problem of sampling from high dimensional probability measures [19].
In this paper, we study online learning problems within a drifting-games framework, with the aim of developing a general methodology for designing learning algorithms based on a minimax analysis.
Deep Neural Networks (DNNs) are extremely powerful machine learning models that achieve excellent performance on difficult problems such as speech recognition [13, 7] and visual object recognition [19, 6, 21, 20].
The predominant paradigm of modeling time series is based on state-space models, in which a hidden state evolves according to some predefined dynamical law, and an observation model maps the state to the dataspace.
Bayesian inference in statistical models involving a large number of latent random variables is in general a difficult problem.
The traditional approach to fitting a Gaussian mixture model onto the data involves using the wellknown expectation-maximization algorithm to estimate component parameters [7].
How our visual system achieves robust performance against corruptions is a mystery.
The goal of supervised machine learning is to use available source data to make predictions with the smallest possible error (loss) on unlabeled target data.
Recent years have witnessed the emergence of big graphs in a large variety of real applications, such as the web and social network services.
Determining connectivity in populations of neurons is fundamental to understanding neural computation and function.
In recent years, many computer vision and natural language processing (NLP) tasks have benefited from the use of dense representations of inputs by allowing superficially different inputs to be related to one another [26, 9, 7, 4].
We have recently seen a revival of attention given to convolutional neural networks (CNNs) [22] due to their high performance for large-scale visual recognition tasks [15, 21, 30].
Recent progress in large-scale techniques for recording neural activity has made it possible to study the joint firing statistics of 102 up to 105 cells at single-neuron resolution.
Many image and video degradation processes can be modeled as translation-invariant convolution.
Topic modeling offers a suite of useful tools that automatically learn the latent semantic structure of a large collection of documents.
This paper consider the following optimization problem: def minimize f (x) = g(x) + h(x), x∈Rd (1) d where Pn g is the average dof the smooth convex functions g1 , .
The success of machine learning has led to its widespread use as a workhorse in a wide variety of domains, from text and language recognition to trading agent design.
In the standard formulation of graph clustering, we are given an unweighted graph and seek a partitioning of the nodes into disjoint groups such that members of the same group are more densely connected than those in different groups.
Structure sparsity induced regularization terms [1, 8] have been widely used recently for feature learning purpose, due to the inherent sparse structures of the real world data.
In many real-world applications, the performance measure used to evaluate a learning model is non-decomposable and cannot be expressed as a summation or expectation of losses on individual data points; this includes, for example, the F-measure used in information retrieval [1], and several combinations of the true positive rate (TPR) and true negative rate (TNR) used in class imbalanced classification settings [2–5] (see Table 1).
Structure learning in Markov networks, also known as undirected graphical models or Markov random fields, has attracted considerable interest in computational statistics, machine learning, and artificial intelligence.
Without any prior knowledge, what can be automatically learned from high-dimensional data? If the variables are uncorrelated then the system is not really high-dimensional but should be viewed as a collection of unrelated univariate systems.
Deep convolutional neural networks (CNNs) [1] with max-pooling layers [2] trained by backprop [3] on GPUs [4] have become the state-of-the-art in object recognition [5, 6, 7, 8], segmentation/detection [9, 10], and scene parsing [11, 12] (for an extensive review see [13]).
Graphical models [1, 2, 3] are a popular and important means of representing certain conditional independence relations between random variables.
Modern data analysis has seen an explosion in the size of the datasets available to analyze.
Drawing samples from arbitrary probability distributions is a core problem in statistics and machine learning.
Understanding the world in a single glance is one of the most accomplished feats of the human brain: it takes only a few tens of milliseconds to recognize the category of an object or environment, emphasizing an important role of feedforward processing in visual recognition.
Coordinate descent methods have received extensive attention in recent years due to their potential for solving large-scale optimization problems arising from machine learning and other applications.
Convolutional neural networks (CNNs) trained via backpropagation were recently shown to perform well on image classification tasks with millions of training images and thousands of categories [1, 2].
Given a matrix X ∈ Rm×n , biclustering or submatrix localization is the problem of identifying a subset of the rows and columns of X such that the bicluster or submatrix consisting of the selected rows and columns are “significant” compared to the rest of X.
One fundamental goal of any learning algorithm is to strike a right balance between underfitting and overfitting.
Progress on the path from shallow bag-of-words information retrieval algorithms to machines capable of reading and understanding documents has been slow.
Statistical relational models such as Markov logic [5] have the power to represent the rich relational structure as well as the underlying uncertainty, both of which are the characteristics of several real world application domains.
In this paper, we develop a latent variable model and efﬁcient spectral algorithm motivated by the recent emergence of very large data sets of chromatin marks from multiple human cell types [7, 9].
Not only are our data growing in volume and dimensionality, but the understanding that we wish to gain from them is increasingly sophisticated.
While early human case studies revealed the importance of the hippocampus in episodic memory [1, 2], the discovery of “place cells” in rats [3] established its role for spatial representation.
Interactive real-time controllers that are capable of generating complex, stable and realistic movements have many potential applications including robotic control, animation and gaming.
Gaussian Mixture Models (GMMs) are a mainstay in a variety of areas, including machine learning and signal processing [4, 10, 16, 19, 21].
Tensors are useful representational objects to model a variety of problems such as graphical models with latent variables [1], audio classification [20], psychometrics [8], and neuroscience [3].
Bayesian networks are probabilistic graphical models representing joint probability distributions of random variables.
Understanding differences between populations is a common task across disciplines, from biomedical data analysis to demographic or textual analysis.
Entropies, divergences, and mutual informations are classical information-theoretic quantities that play fundamental roles in statistics, machine learning, and across the mathematical sciences.
The task of selecting a set of items subject to constraints on the size or the cost of the set is versatile in machine learning problems.
Bayesian inference is a powerful framework for analyzing data.
Many problems in machine learning can be written as a stochastic optimization problem minimize E[f˜(x)] over x ∈ Rn , where f˜ is a random objective function.
This work tackles the challenge of constructing fully empirical bounds on the mixing time of Markov chains based on a single sample path.
Since large numbers of high-definition displays have sprung up, generating high-resolution videos from previous low-resolution contents, namely video super-resolution (SR), is under great demand.
Testing whether two random variables are identically distributed without imposing any parametric assumptions on their distributions is important in a variety of scientific applications.
The last few years have seen tremendous progress in learning useful image representations [6].
Probabilistic graphical models such as Bayesian networks and Markov random fields provide a useful framework and powerful tools for machine learning.
Computer vision researchers often go through great lengths to remove dataset biases from their models [32, 20].
Recently, supervised learning has been developed and used successfully to produce representations that have enabled leaps forward in classification accuracy for several tasks [1].
Being rooted in information retrieval [16], the so-called F-measure is nowadays routinely used as a b = (b performance metric in various prediction tasks.
Let M ⇤ 2 Rm⇥n be a rank k matrix with k much smaller than m and n.
It is often desirable to model discrete data in terms of continuous latent structure.
Enormous amounts of astronomical data are collected by a range of instruments at multiple spectral resolutions, providing information about billions of sources of light in the observable universe [1, 10].
Topic models have emerged as flexible and important tools for the modelisation of text corpora.
Methods for neuroimaging research can be grouped by discovering neurobiological structure or assessing the neural correlates associated with mental tasks.
Variational methods are a popular alternative to Markov chain Monte Carlo (MCMC) methods for Bayesian inference.
Symmetry-breaking is an approach to speeding up satisfiability testing by adding constraints, called symmetry-breaking predicates (SBPs), to a theory [7, 1, 16].
Any matrix A ∈ Rn×d with rank r can be written using a singular value decomposition (SVD) as A = UΣVT .
Many models of visual saliency have been proposed in the last decade with differences in defining principles and also divergent objectives.
Applications such as speech recognition [1], medical diagnosis [2], optical character recognition [3], machine translation [4], and scene labeling [5] have two properties: (i) they are instances of structured prediction, where the predicted output is a complex structured object; and (ii) they are user-facing applications for which it is important to provide accurate estimates of confidence.
Most interactive systems (e.
Log-linear models, a general class that includes conditional random fields (CRFs) and generalized linear models (GLMs), offer a flexible yet tractable approach modeling conditional probability distributions p(x|y) [1, 2].
The brain is faced with the persistent challenge of decision making under uncertainty due to noise in the sensory inputs and perceptual ambiguity .
In many machine learning tasks, data is represented in a high-dimensional Euclidean space.
Consider the following simple game.
We follow the standard mathematical abstraction of this problem (Candes & Fernandez-Granda [4, 3]): consider a d-dimensional signal x(t) modeled as a weighted sum of k Dirac measures in Rd : x(t) = k X wj µ(j) , (1) j=1 where the point sources, the µ(j) ’s, are in Rd .
Graphical models are a popular modeling tool for both discrete and continuous distributions.
Several variants of learning-to-rank problems have recently been studied in an online setting, with preferences over alternatives given in the form of stochastic pairwise comparisons [6].
A critical task in data analysis is to determine how similar two data samples are.
A hallmark of an intelligent agent is to learn new information as the world unfolds and to improvise by fusing the new information with prior knowledge.
Clustering is a fundamental machine learning problem with widespread applications.
Recurrent neural networks (RNN) have recently been trained successfully for time series modeling, and have been used to achieve state-of-the-art results in supervised tasks including handwriting recognition [12] and speech recognition [13].
When faced with a complex target distribution, one often turns to RMarkov chain Monte Carlo (MCMC) [1] to approximate intractable expectations EP [h(Z)] = X p(x)h(x)dx with asympPn totically exact sample estimates EQ [h(X)] = i=1 q(xi )h(xi ).
The task of completing the missing entries of a matrix from an incomplete subset of (potentially noisy) entries is encountered in many applications including recommendation systems, data imputation, covariance matrix estimation, and sensor localization among others.
The contextual bandit problem [1, 2, 3] is an important extension of the classic multi-armed bandit (MAB) problem [4], where the agent can observe a set of features, referred to as context, before making a decision.
One of the most basic problems in statistical hypothesis testing is the question of distinguishing whether two unknown distributions are very similar, or significantly different.
Likelihood-based approaches to learning dynamical systems, such as EM [1] and MCMC [2], can be slow and suffer from local optima.
A good distance metric is often the key to an effective machine learning algorithm.
High-dimensional tensor-valued data are prevalent in many fields such as personalized recommendation systems and brain imaging research [1, 2].
We consider a generalization of the one-bit quantized regression problem, where we seek to recover the regression coefficient β ∗ ∈ Rp from one-bit measurements.
A central challenge in machine learning is to extract useful information from massive data.
Background Many learning applications, ranging from language-processing staples such as speech recognition and machine translation to biological studies in virology and bioinformatics, call for estimating large discrete distributions from their samples.
A wide range of machine learning and signal processing problems can be formulated as the minimization of a composite objective: min F (x) := f (x) + kBxk x∈X (1) where X is closed and convex, f is convex and can be either smooth, or nonsmooth yet enjoys a particular structure.
Tensor is a natural way to express higher order interactions for a variety of data and tensor decomposition has been successfully applied to wide areas ranging from chemometrics, signal processing, to neuroimaging; see [15, 18] for a survey.
There has been significant recent interest in deep learning.
Complex machine learning tools such as deep learning are gaining increasing popularity and are being applied to a wide variety of problems.
In the primate and human retina, roughly 20 distinct classes of retinal ganglion cells (RGCs) send distinct visual information to diverse targets in the brain [18, 7, 6].
We study the problem of monitoring several time series so as to maintain a precise belief while minimising the cost of sensing.
Over the last decade, graph kernels have become a popular approach to graph comparison [4, 5, 7, 9, 12, 13, 14], which is at the heart of many machine learning applications in bioinformatics, imaging, and social-network analysis.
Discovering causal effects is a fundamentally important yet very challenging task in various disciplines, from public health research and sociological studies, economics to many applications in the life sciences.
Our visual system is designed to perceive a physical world that is full of dynamic content.
Analogy is the task of mapping information from a source to a target.
The great success of neural networks is due in part to the simplicity of the backpropagation algorithm, which allows one to efficiently compute the gradient of any loss function defined as a composition of differentiable functions.
We consider the problem of distributed convex learning and optimization, where a set of m machines, each with access to a different local convex function Fi : Rd 7→ R and a convex domain W ⊆ Rd , attempt to solve the optimization problem m min F (w) where F (w) = w∈W 1 X Fi (w).
Over recent years, the landscape of computer vision has been drastically altered and pushed forward through the adoption of a fast, scalable, end-to-end learning framework, the Convolutional Neural Network (CNN) [18].
Low rank matrix completion approaches are among the most widely used collaborative filtering methods, where a partially observed matrix is available to the practitioner, who needs to impute the missing entries.
Consider the following convex optimization problem min f (x) = g(x1 , · · · , xK ) + K  hk (xk ), s.
Deep neural networks currently demonstrate state-of-the-art performance in many domains of largescale machine learning, such as computer vision, speech recognition, text processing, etc.
Considerable research has been devoted to developing probabilistic models for high-dimensional time-series data, such as video and music sequences, motion capture data, and text streams.
Many machine learning tasks can be framed as learning a function given noisy information about its inputs and outputs.
Consider the problem of regret minimization in non-stochastic multi-armed bandits, as defined in the classic paper of Auer, Cesa-Bianchi, Freund, and Schapire [5].
The Multi-Armed Bandit (MAB) problem is one of the most popular settings encountered in the sequential decision-making literature [Rob52, LR85, EDMM06, Sco10, BCB12] with applications across multiple disciplines.
We consider the following problem of high dimensional linear regression: y = Xθ∗ + ω , (1) where y ∈ Rn is the response vector, X ∈ Rn×p has independent isotropic sub-exponential random rows, ω ∈ Rn has i.
Time series forecasting plays a crucial role in a number of domains ranging from weather forecasting and earthquake prediction to applications in economics and finance.
Many learning tasks require labeling large datasets.
Many modern fMRI studies of the human brain use data from multiple subjects.
The central idea behind Bayesian nonparametrics (BNPs) is the replacement of classical finitedimensional prior distributions with general stochastic processes, allowing for an open-ended number of degrees of freedom in a model [8].
Aggregating pairwise comparisons and partial rankings are important problems with applications in econometrics [1], psychometrics [2, 3], sports ranking [4, 5] and multiclass classification [6].
The goal of a metric learning algorithm is to capture the idiosyncrasies in the data mainly by defining a new space of representation where some semantic constraints between examples are fulfilled.
Partial monitoring is a general framework for sequential decision making problems with imperfect feedback.
There have been many recent advances in the recovery of communities in networks, under “blockmodel” assumptions [19, 18, 9].
Graphical models provide a powerful framework for reasoning with probabilistic information.
Probabilistic modeling has emerged as a powerful tool for data analysis.
Tree structured group Lasso (TGL) [13, 30] is a powerful regression technique in uncovering the hierarchical sparse patterns among the features.
A recent confluence of results from game theory and learning theory gives a simple explanation for why good outcomes in large families of strategically-complex games can be expected.
Matrix factorization (MF) techniques have emerged as a powerful tool to perform collaborative filtering in large datasets [1].
Online social networks, such as Twitter or Weibo, have become large information networks where people share, discuss and search for information of personal interest as well as breaking news [1].
Many machine learning tasks involve careful tuning of a regularization parameter that controls the balance between an empirical loss term and a regularization term.
Recently, attention-based recurrent networks have been successfully applied to a wide variety of tasks, such as handwriting synthesis [1], machine translation [2], image caption generation [3] and visual object classification [4].
Given a label budget, what is the best way to learn a classifier? Active learning approaches to this question are known to yield exponential improvements over supervised learning under strong assumptions [7].
The central problem of this paper is computational complexity in a setting where the number of classes k for multiclass prediction is very large.
Deep neural networks (DNNs) have recently been achieving state of the art results in many fields.
Building a good generative model of natural images has been a fundamental problem within computer vision.
We consider the problem of robust Principal Component Analysis (PCA).
In structured output prediction, it is important to learn a model that can perform probabilistic inference and make diverse predictions.
Markov random fields (MRFs) are used in many areas of computer science such as vision and speech.
In this paper we develop SLEEC (Sparse Local Embeddings for Extreme Classification), an extreme multi-label classifier that can make significantly more accurate and faster predictions, as well as scale to larger problems, as compared to state-of-the-art embedding based approaches.
In interactive submodular set cover (ISSC) [10, 11, 9], the goal is to interactively satisfy all plausible submodular functions in as few actions as possible.
We formulate hierarchical image segmentation from the perspective of estimating an ultrametric distance over the set of image pixels that agrees closely with an input set of noisy pairwise distances.
This paper constructs an algorithmic framework for the following convex optimization template: f ‹ :“ min tf pxq : Ax ´ b P Ku , xPX (1) where f : Rp Ñ R Y t`8u is a convex function, A P Rnˆp , b P Rn , and X and K are nonempty, closed and convex sets in Rp and Rn respectively.
Stochastic optimal control (SOC) is a general and powerful framework with applications in many areas of science and engineering.
The dueling bandit problem [1] arises naturally in domains where feedback is more reliable when given as a pairwise preference (e.
Multi-task learning (MTL) advocates sharing relevant information among several related tasks during the training stage.
Finding the global maximizer of a non-concave objective function based on sequential, noisy observations is a fundamental problem in various real world domains e.
Learning problems on graph-structured data have received significant attention in recent years [11, 17, 20].
In matrix completion, one has access to a matrix with only a few observed entries, and the task is to estimate the entire matrix using the observed entries.
We consider a convex optimization problem, minimizex∈X f (x), where X ⊆ Rn is convex and closed, f is a C 1 convex function, and ∇f is assumed to be Lf -Lipschitz.
Problem setting The problem of Compressive Phase Retrieval (CPR) is generally stated as the problem of estimating a k-sparse vector x? ∈ Rd from noisy measurements of the form 2 yi = |hai , x? i| + zi (1) for i = 1, 2, .
Discriminative methods pursue a direct mapping from the input to the output space for a classification or a regression task.
Scene labeling (or scene parsing) is an important step towards high-level image interpretation.
The rapidly growing data dimension has brought new challenges to statistical variable selection, a crucial technique for identifying important variables to facilitate interpretation and improve prediction accuracy.
Dynamic causal systems are a major focus of scientific investigation in diverse domains, including neuroscience, economics, meteorology, and education.
Machine learning has recently made great strides in many application areas, fueling a growing demand for machine learning systems that can be used effectively by novices in machine learning.
It is surely no surprise to the reader that modern machine learning algorithms thrive on large amounts of data — preferably labeled.
Undirected Graphical Models (also known as Markov Random Fields) provide a flexible framework to represent networks of random variables and have been used in a large variety of applications in machine learning, statistics, signal processing and related fields [2].
The greedy algorithm is simple and easy-to-implement, and can be applied to solve a wide range of complex optimization problems, either with exact solutions (e.
In the traditional economic approach to identifying a revenue-maximizing auction, one first posits a prior distribution over all unknown information, and then solves for the auction that maximizes expected revenue with respect to this distribution.
Recurrent neural networks can be used to process sequences, either as input, output or both.
Modeling notions such as coverage, representativeness, or diversity is an important challenge in many machine learning problems.
Modern classification problems often involve the prediction of multiple labels simultaneously associated with a single instance e.
One of the challenging aspects of deep learning is the optimization of the training criterion over millions of parameters: the difficulty comes from both the size of these neural networks and because the training objective is non-convex in the parameters.
This paper studies the problem of recovering communities in the general stochastic block model with linear size communities, for constant and logarithmic degree regimes.
The goal of machine learning is to produce hypotheses or models that generalize well to the unseen instances of the problem.
Training deep networks is a challenging problem [16, 2] and various heuristics and optimization algorithms have been suggested in order to improve the efficiency of the training [5, 9, 4].
An active learner is given a hypothesis class, a large set of unlabeled examples and the ability to interactively make label queries to an oracle on a subset of these examples; the goal of the learner is to learn a hypothesis in the class that ﬁts the data well by making as few oracle queries as possible.
There has been a steep rise in recent work [6, 7, 9–12, 25, 27, 29] on “variance reduced” stochastic gradient algorithms for convex problems of the finite-sum form: Xn min f (x) := n1 fi (x).
Gaussian process models are attractive for machine learning because of their flexible nonparametric nature.
Economist Thomas Sowell remarked that “The first lesson of economics is scarcity: There is never enough of anything to fully satisfy all those who want it.
We investigate the problem of constructing, in a memory and computationally efficient manner, an accurate estimate of the optimal rank k approximation M (k) of a large (m × n) matrix M ∈ [0, 1]m×n .
Delivering personalized user experiences is believed to play a crucial role in the long-term engagement of users to modern web-services [26].
We consider the low-rank approximation of symmetric positive semi-definite (SPSD) matrices that arise in machine learning and data analysis, with an emphasis on obtaining good statistical guarantees.
We consider the problem of optimizing the average of a finite but large sum of smooth functions, n min f (x) = x∈Rd 1X fi (x).
A restricted Boltzmann machine (RBM) [1, 2] is a type of undirected neural network with surprisingly many applications.
Kernel methods [17] have enjoyed tremendous success in solving several fundamental problems of machine learning ranging from classification, regression, feature extraction, dependency estimation, causal discovery, Bayesian inference and hypothesis testing.
Robust Least Squares Regression (RLSR) addresses the problem of learning a reliable set of regression coefficients in the presence of several arbitrary corruptions in the response vector.
Modern statistical inference demands scalability to massive datasets and high-dimensional models.
Determinantal point processes (DPPs) are point processes [1] that encode repulsiveness using algebraic arguments.
Neural networks have become ubiquitous in applications ranging from computer vision [1] to speech recognition [2] and natural language processing [3].
Graphical models are a flexible and widely used tool for modeling and inference in high dimensional settings.
Our brains analyze high-dimensional datasets streamed by our sensory organs with efficiency and speed rivaling modern computers.
With increasingly efﬁcient data collection methods, scientists are interested in quickly analyzing ever larger data sets.
Hidden Markov Models (HMM’s) are among the most widely adopted latent-variable models used to model time-series datasets in the statistics and machine learning communities.
In complex, chronic diseases such as autism, lupus, and Parkinson’s, the way the disease manifests may vary greatly across individuals [1].
Semi-supervised learning is now a standard methodology in machine learning.
Most reinforcement learning (RL) algorithms learn a value function—a function that estimates the expected return obtained by following a given policy from a given state.
We focus on the following minimization problem, n minimize f (✓) := 1X fi (✓), n i=1 (1.
Multi-Armed Bandit (MAB) problems [1] constitute the most fundamental sequential decision problems with an exploration vs.
Kernel methods such as nonlinear support vector machines (SVMs) [1] provide a powerful framework for nonlinear learning, but they often come with significant computational cost.
Causality is a fundamental concept in sciences and philosophy.
For several decades there has been much interest in understanding the manner in which ideas, language, and information cascades spread through society.
Consider the problem of minimizing a convex function over some convex domain.
Humans are good at considering “what-if?” questions about objects in their environment.
Markowitz’s mean-variance analysis sets the basis for modern portfolio optimization theory [1].
Ensemble-based learning is a very successful approach to learning classifiers, including well-known methods like boosting [1], bagging [2], and random forests [3].
In active learning, we are given a sample space X , a label space Y, a class of models that map X to Y, and a large set U of unlabelled samples.
A function f : 2S → R+ is called submodular if f (X) + f (Y ) ≥ f (X ∪ Y ) + f (X ∩ Y ) for all X, Y ⊆ S, where S is a finite ground set.
We consider a general global optimization problem: maximize f (x) subject to x ∈ Ω ⊂ RD where f : Ω → R is a non-convex black-box deterministic function.
Many high-dimensional datasets comprise points derived from a smooth, lower-dimensional manifold embedded within the high-dimensional space of measurements and possibly corrupted by noise.
Nowadays data of huge scale are prevalent in many applications of statistical learning and data mining.
Sequential Monte Carlo (SMC) is a class of algorithms that draw samples from a target distribution of interest by sampling from a series of simpler intermediate distributions.
As the number of classes increases, two important issues emerge: class overlap and multilabel nature of examples [9].
In the late 1940s, Wald and colleagues developed a sequential test called the sequential probability ratio test (SPRT; [7]).
Expectation Propagation (EP, 1) is an efficient approximate inference algorithm that is known to give good approximations, to the point of being almost exact in certain applications [2, 3].
We study a natural asynchronous stochastic gradient method for the solution of minimization problems of the form Z minimize f (x) := EP [F (x; W )] = F (x; ω)dP (ω), (1) Ω where x 7→ F (x; ω) is convex for each ω ∈ Ω, P is a probability distribution on Ω, and the vector x ∈ Rd .
In many machine learning problems, the statistical risk functional is an expectation over d-tuples (d ≥ 2) of observations, rather than over individual points.
Machine learning aims to find regularities in data to perform various tasks.
Numerous graphics algorithms have been established to synthesize photorealistic images from 3D models and environmental variables (lighting and viewpoints), commonly known as rendering.
We consider the estimation of generalized linear models (GLMs) [1], under high-dimensional settings where the number of variables p may greatly exceed the number of observations n.
Stochastic search algorithms [1, 2, 3, 4] are black box optimizers of an objective function that is either unknown or too complex to be modeled explicitly.
You step out of your house and notice a group of people looking up.
The multi-armed bandit is the simplest class of problems that exhibit the exploration/exploitation dilemma.
The expectation-maximization (EM) algorithm [12] is the most popular approach for calculating the maximum likelihood estimator of latent variable models.
Computing integrals is a core challenge in machine learning and numerical methods play a central role in this area.
Estimating expectations using Markov Chain Monte Carlo (MCMC) is a fundamental approximate inference technique in Bayesian statistics.
Learning deep structured models has attracted considerable research attention recently.
Combining image understanding and natural language interaction is one of the grand dreams of artificial intelligence.
This paper studies a class of problems that share a common high-level objective: from a number n of probabilistic distributions, find the k ones whose means are the greatest by a certain metric.
The spectral distribution of light reflected off a surface is a function of an intrinsic material property of the surface—its reflectance—and also of the spectral distribution of the light illuminating the surface.
Recently, there is increasing interest in the field of multimodal learning for both natural language and vision.
Text classification is a classic topic for natural language processing, in which one needs to assign predefined categories to free-text documents.
According to Ghahramani [9], models that have a nonparametric component give us more flexiblity that could lead to better predictive performance.
A broad class of learning problems fits into the framework of obtaining a sequence of independent random samples from a unknown distribution, and then (approximately) recovering this distribution using as few samples as possible.
Consider the following detection problem.
We consider a general class of stochastic optimization problems formulated as ξ ∗ = arg min Eτ ∼p(·|ξ) [J(τ )], ξ (1) where ξ defines a vector of decision variables, τ represents the system response defined through the density p(τ |ξ), and J(τ ) defines a positive cost function which can be non-smooth and nonconvex.
Linear Programming (LP) has been studied since the early 19th century and has become one of the representative tools of numerical optimization with wide applications in machine learning such as `1 -regularized SVM [1], MAP inference [2], nonnegative matrix factorization [3], exemplarbased clustering [4, 5], sparse inverse covariance estimation [6], and Markov Decision Process [7].
Networks are the simplest representation of relationships between entities, and as such have attracted significant attention recently.
Figure 1 shows an example of an image restoration problem.
Multi-label learning refers to the problem setting in which the goal is to assign to an object (e.
Age-related brain diseases, such as Parkinson’s or Alzheimer’s disease (AD) are complex diseases with multiple effects on the metabolism, structure and function of the brain.
The problem of measuring and harnessing dependence between random variables is an inescapable statistical problem that forms the basis of a large number of applications in machine learning, including rate distortion theory [4], information bottleneck methods [28], population coding [1], curiositydriven exploration [26, 21], model selection [3], and intrinsically-motivated reinforcement learning [22].
We treat the problem of optimizing a function f : X → R given a finite budget of n noisy evaluations.
In this paper, we introduce an unsupervised learning method that fits well with supervised learning.
Semidefinite programming has become a key optimization tool in many areas of applied mathematics, signal processing and machine learning.
Recent advances in object detection are driven by the success of region proposal methods (e.
Principal Component Analysis (PCA) reduces data dimensionality by projecting it onto principal subspaces spanned by the leading eigenvectors of the sample covariance matrix.
Gaussian graphical models (GGMs) form a powerful class of statistical models for representing distributions over a set of variables [1].
Consider test preparation software that tutors students for a national advanced placement exam taken at the end of a year, or maximizing business revenue by the end of each quarter.
Latent Dirichlet Allocation (LDA) [5], among various forms of topic models, is an important probabilistic generative model for analyzing large collections of text corpora.
Normalized random measures (NRMs) form a broad class of discrete random measures, including Dirichlet proccess (DP) [1] normalized inverse Gaussian process [2], and normalized generalized Gamma process [3, 4].
Directed generative models are naturally interpreted as specifying sequential procedures for generating data.
Modeling large-scale multivariate count data is an important challenge that arises in numerous applications such as neuroscience, systems biology and amny others.
Gaussian process (GP) models have become an important component of the machine learning literature.
Nowcasting convective precipitation has long been an important problem in the field of weather forecasting.
Recently a number of methods have been developed for applying Bayesian learning to large datasets.
In numerous machine learning and data analysis applications, the input data are modelled as a matrix A ∈ Rm×n , where m is the number of objects (data points) and n is the number of features.
There is broad interest in learning and exploiting lower-dimensional structure in high-dimensional data.
Some of the recent progress on the theoretical foundations of online learning has been motivated by the parallel developments in the realm of statistical learning.
Neural networks today are achieving state-of-the-art performance in competitions across a range of fields [1][2][3].
Low rank matrix completion is an important topic in machine learning and has been successfully applied to many practical applications [22, 12, 11].
Many network metrics have been introduced to measure the similarity between any two vertices.
Typical multi-class application domains such as natural language processing [1], information retrieval [2], image annotation [3] and web advertising [4] involve tens or hundreds of thousands of classes, and yet these datasets are still growing [5].
Learning features that are able to discriminate is a classical problem in data analysis.
Stochastic gradient descent (SGD) [1] is currently the standard in machine learning for the optimization of highly multivariate functions if their gradient is corrupted by noise.
Online advertisement is currently the fastest growing form of advertising.
A large number of machine learning and signal processing problems are formulated as the minimization of a composite objective function F : Rp → R: n o minp F (x) , f (x) + ψ(x) , (1) x∈R where f is convex and has Lipschitz continuous derivatives with constant L and ψ is convex but may not be differentiable.
You may remember that, on January 15, 2009, in New York City, a commercial passenger plane struck a flock of geese within two minutes of taking off from LaGuardia Airport.
One of the central problems in computational learning theory is the efficient learning of polynomials f (x) : x ∈ {−1, 1}n → R.
Probabilistic graphical models are an elegant framework for reasoning about multiple variables with structured dependencies.
Finding optimal or close-to-optimal policies in large Markov Decision Processes (MDPs) requires the use of approximation.
The last decade has witnessed fast growing attention in research of high-dimensional data: images, videos, DNA microarray data and data from many other applications all have the property that the dimensionality can be comparable or even much larger than the number of samples.
Optimal transport distances (Villani, 2008), a.
Boosting algorithms [21] are ensemble methods that convert a learning algorithm for a base class of models with weak predictive power, such as decision trees, into a learning algorithm for a class of models with stronger predictive power, such as a weighted majority vote over base models in the case of classification, or a linear combination of base models in the case of regression.
Over the past years, advances in adopting methods from algebraic topology to study the “shape” of data (e.
Neural spiking activity recorded from populations of cortical neurons can exhibit substantial variability in response to repeated presentations of a sensory stimulus [1].
Despite the tremendous growth of available data over the past decade, the lack of fully annotated data, which is an essential part of success of any traditional supervised learning algorithm, demands methods that allow good generalization from limited amounts of training data.
Nearest neighbor search is a key algorithmic problem with applications in several fields including computer vision, information retrieval, and machine learning [4].
The problem of data partitioning is of great importance to many machine learning (ML) and data science applications as is evidenced by the wealth of clustering procedures that have been and continue to be developed and used.
Deep neural networks are a flexible family of models that easily scale to millions of parameters and datapoints, but are still tractable to optimize using minibatch-based stochastic gradient ascent.
Good statistical models of populations are often very different from good models of individuals.
Community detection in graphs, also known as graph clustering, is a problem where one wishes to identify subsets of the vertices of a graph such that the connectivity inside the subset is in some way denser than the connectivity of the subset with the rest of the graph.
Many scenarios involve classification systems constrained by measurement acquisition budget.
How should we combine opinions (or beliefs) about hidden truths (or uncertain future events) furnished by several individuals with potentially diverse information sets into a single group judgment for decision or policy-making purposes? This has been a fundamental question across disciplines for a long time (Surowiecki [2005]).
In spite of the many great successes of deep learning, efficient optimization of deep networks remains a challenging open problem due to the complexity of the model calculations, the non-convex nature of the implied objective functions, and their inhomogeneous curvature [6].
Summarizing large data sets using pairwise co-occurrence frequencies is a powerful tool for data mining.
For many problems in information retrieval and learning to rank, the performance of a predictor is evaluated based on the combination of predictions it makes for multiple variables.
Combinatorial optimization [16] has many real-world applications.
Two grand challenges in artificial intelligence research have been to build models that can make multiple computational steps in the service of answering a question or completing a task, and models that can describe long term dependencies in sequential data.
Unsupervised learning seeks to induce good latent representations of a data set.
Subspace clustering was originally proposed to solve very specific computer vision problems having a union-of-subspace structure in the data, e.
Risk-sensitive optimization considers problems in which the objective involves a risk measure of the random cost, in contrast to the typical expected cost objective.
A variety of tasks in machine learning can be formulated in the form of an energy minimization problem, known also as maximum a posteriori (MAP) or maximum likelihood estimation (MLE) inference in an undirected graphical models (related to Markov or conditional random fields).
Statistical classification, a core task in many modern data processing and prediction problems, is the problem of predicting labels for a given feature vector based on a set of training data instances containing feature vectors and their corresponding labels.
In many data-rich domains such as computer vision, neuroscience and social networks consisting of multi-modal and multi-relational data, tensors have emerged as a powerful paradigm for handling the data deluge.
In perceptual decision making participants have to identify a noisy stimulus.
In many applications we are interested in computing similarities between structured objects such as graphs.
Non-linear Measurements.
The success of deep learning is to a large part based on advanced and efficient input representations [1, 2, 3, 4].
In large-scale Bayesian learning, diffusion based sampling methods have become increasingly popular.
The asynchronous parallel optimization recently received many successes and broad attention in machine learning and optimization [Niu et al.
Markov chain Monte Carlo sampling is among the most general methods for probabilistic inference.
Sparse learning with convex regularization has been successfully applied to a wide range of applications including marker genes identification [19], face recognition [22], image restoration [2], text corpora understanding [9] and radar imaging [20].
Prediction algorithms studied in this paper belong to the class of Venn–Abers predictors, introduced in [1].
Parameter estimation of probabilistic models on discrete space is a popular and important issue in the fields of machine learning and pattern recognition.
Many studies and theories in neuroscience posit that high-dimensional populations of neural spike trains are a noisy observation of some underlying, low-dimensional, and time-varying signal of interest.
Variational inference is a computationally efficient approach for approximating posterior distributions.
Causal discovery is the process to identify the causal relationships among a set of random variables.
The majority of available data in modern machine learning applications come in a raw and unlabeled form.
Over the last few years, heuristics for non-convex optimization have emerged as one of the most fascinating phenomena for theoretical study in machine learning.
We give general conditions for the convergence of the EM method for high-dimensional estimation.
Scaling up nonlinear component analysis has been challenging due to prohibitive computation and memory requirements.
Consider the common compressed sensing (CS) model yi = hai , x∗ i + σεi , i = 1, .
Stochastic variational inference has emerged as a promising and flexible framework for performing large scale approximate inference in complex probabilistic models.
The classic multi-armed bandit (MAB) problem, generally attributed to the early work of Robbins (1952), poses a generic online decision scenario in which an agent must make a sequence of choices from a fixed set of options.
Undirected graphical models, or Markov random fields [13], have been extensively studied and applied in fields ranging from computational biology [15, 28], to natural language processing [16, 20] and computer vision [9, 17].
Control of non-linear dynamical systems with continuous state and action spaces is one of the key problems in robotics and, in a broader context, in reinforcement learning for autonomous agents.
Convolutional neural networks (CNNs) (LeCun et al.
The reinforcement learning problem in Markov decision processes (MDPs) involves an agent using its observed rewards to learn an optimal policy that maximizes its expected total reward for a given task.
Given a high-dimensional dataset YD×N = (y1 , .
A firm that relies on the ability to make difficult predictions can gain a lot from a large collection of data.
Gaussian process (GP) regression models have become a standard tool in Bayesian signal estimation due to their expressiveness, robustness to overfitting and tractability [1].
Bandit with mixing arms.
We consider the problem of classification of a binary response given p covariates.
Learning generative models of sequences is a long-standing machine learning challenge and historically the domain of dynamic Bayesian networks (DBNs) such as hidden Markov models (HMMs) and Kalman filters.
Independent Component Analysis refers to a class of methods aiming at recovering statistically independent signals by observing their unknown linear combination.
Inverse optimal control (IOC) [13], also known as inverse reinforcement learning [18, 1] and inverse planning [3], has become a powerful technique for learning to control or make decisions based on expert demonstrations [1, 20].
Markov chain Monte Carlo (MCMC) has become a defacto tool for Bayesian posterior inference.
Neural circuits can be reconstructed by analyzing 3D brain images from electron microscopy (EM).
Encoding signals or building similarity kernels that are invariant to the action of a group is a key problem in unsupervised learning, as it reduces the complexity of the learning task and mimics how our brain represents information invariantly to symmetries and various nuisance factors (change in lighting in image classification and pitch variation in speech recognition) [1, 2, 3, 4].
In undirected graphical models, maximum likelihood learning is intractable in general.
A fundamental primitive in Bayesian learning is the ability to sample from the posterior distribution.
Generative models have become ubiquitous in machine learning and statistics and are now widely used in fields such as bioinformatics, computer vision, or natural language processing.
We consider a general problem that is pervasive in machine learning, namely optimization of an empirical or regularized convex risk function.
The discovery of matched instances in different domains is an important task, which appears in natural language processing, information retrieval and data mining tasks such as finding the alignment of cross-lingual sentences [1], attaching tags to images [2] or text documents [3], and matching user identifications in different databases [4].
In this paper we consider a primal-dual pair of structured convex optimization problems which has in several variants of varying degrees of generality attracted a lot of attention in the past few years in the machine learning and optimization communities [4, 22, 20, 23, 21, 27].
A number of problems in Computer Vision and Machine Learning involve searching for a set of bounding boxes or rectangular windows.
Deep Neural Networks (DNN) have substantially pushed the state-of-the-art in a wide range of tasks, especially in speech recognition [1, 2] and computer vision, notably object recognition from images [3, 4].
Object detection is one of the most foundational tasks in computer vision [21].
There are two roads to an accurate AI system today: (i) gather a huge amount of labeled training data [1] and do supervised learning [2]; or (ii) use crowdsourcing to directly perform the task [3, 4].
As a simple and intuitive representation, the Euclidean space <d has been widely used in various learning tasks.
Neural associative memories with exponential storage capacity and large (potentially linear) fraction of error-correction guarantee have been the topic of extensive research for the past three decades.
Mixture models play a central role in machine learning and statistics, with diverse applications including bioinformatics, speech, natural language, and computer vision.
Learning the structure of a Bayesian network from data is NP-hard [2].
In recent years, sparse and low rank learning has been a hot research topic and leads to a wide variety of applications in signal/image processing, statistics and machine learning.
Bandit convex optimization [11, 5] is the following online learning problem.
Decentralized computation and estimation have many applications in sensor and peer-to-peer networks as well as for extracting knowledge from massive information graphs such as interlinked Web documents and on-line social media.
Statistical model criticism or checking1 is an important part of a complete statistical analysis.
Long Short-Term Memory (LSTM) networks [1, 2] are recurrent neural networks (RNNs) initially designed for sequence processing.
A directed acyclic graph (DAG) G(V, E) defines a partial order on V where u precedes v if there is a directed path from u to v.
Bayesian methods are popular for their success in analyzing complex data sets.
Recurrent Neural Networks (RNNs) have been used for learning functions over sequences from examples for more than three decades [3].
Recently there has been a hike of interest in automatically generating natural language descriptions for images in the research of computer vision, natural language processing, and machine learning (e.
The benefits of using the Stochastic Gradient Descent (SGD) scheme for learning could not be stressed enough.
Deep networks have proven extremely successful across a broad range of applications.
A common task in supervised learning is to select the model that best fits the data.
The goal of disease progression modeling is to learn a model for the temporal evolution of a disease from sequences of clinical measurements obtained from a longitudinal sample of patients.
The recent success of deep feature learning in the supervised setting has inspired renewed interest in feature learning in weakly supervised and unsupervised settings.
Kernel methods provide an elegant and effective framework to develop nonparametric statistical approaches to learning [1].
In a variety of applications, one needs to process data of rich structure that can be conveniently represented by a hypergraph, where associations of the data items, represented by vertices, are represented by hyperedges, i.
Truly intelligent systems can learn and make decisions without human intervention.
Density ridges [10, 22, 15, 6] are one-dimensional curve-like structures that characterize high density regions.
Developing learning algorithms for distributed compositional semantics of words has been a longstanding open problem at the intersection of language understanding and machine learning.
We use the term “active learning” to refer to algorithms that employ adaptive data collection in order to accelerate machine learning.
Online learning in stochastic environments is a sequential decision problem where in each time step a learner chooses an action from a given finite set, observes some random feedback and receives a random payoff.
The quintessential scientific question is whether an unknown object has some property, i.
Trace regression models of the form yi = tr(Xi⊤ Σ∗ ) + εi , i = 1, .
Tractable learning [1] is a promising new machine learning paradigm that focuses on learning probability distributions that support efficient querying.
Orthogonal NMF The success of Nonnegative Matrix Factorization (NMF) in a range of disciplines spanning data mining, chemometrics, signal processing and more, has driven an extensive practical and theoretical study [1, 2, 3, 4, 5, 6, 7, 8].
Empirical risk minimization (ERM) is a domininant framework for supervised machine learning, and a key component of many learning algorithms.
Embedding structured data, such as graphs, in geometric spaces, is a central problem in machine learning.
Online learning is a well-established learning paradigm which has both theoretical and practical appeals.
Over the years, deep learning approaches (see [5, 26] for survey) have shown great success in many visual perception problems (e.
Computationally demanding simulators are used across the full spectrum of scientific and industrial applications, whether one studies embryonic morphogenesis in biology, tumor growth in cancer research, colliding galaxies in astronomy, weather forecasting in meteorology, climate changes in the environmental science, earthquakes in seismology, market movement in economics, turbulence in physics, brain functioning in neuroscience, or fabrication processes in industry.
We start with a general discussion of the tension between sample size and computational efficiency in statistical and learning problems.
Convolutional neural networks (CNNs) [15] are neural networks that can make use of the internal structure of data such as the 2D structure of image data through convolution layers, where each computation unit responds to a small region of input data (e.
Suppose we are given a response vector y = [yi ]1≤i≤m generated from a quadratic transformation of an unknown object x ∈ Rn /Cn , i.
Many modern applications of neural networks have to deal with data represented, or representable, as very large sparse vectors.
Markov chains are one of the workhorses of stochastic modeling, finding use across a variety of applications – MCMC algorithms for simulation and statistical inference; to compute network centrality metrics for data mining applications; statistical physics; operations management models for reliability, inventory and supply chains, etc.
As the machine learning (ML) community continues to accumulate years of experience with live systems, a wide-spread and uncomfortable trend has emerged: developing and deploying ML systems is relatively fast and cheap, but maintaining them over time is difficult and expensive.
One of the most challenging problems in large-scale machine learning is how to parallelize the training of large models that use a form of stochastic gradient descent (SGD) [1].
Principal component analysis (PCA) is a tool for providing a low-rank approximation to a data matrix D 2 Rn⇥d , with the aim of reducing dimension or capturing the main directions of variation in the data.
The goal of visual texture synthesis is to infer a generating process from an example texture, which then allows to produce arbitrarily many new samples of that texture.
Detecting the emergence of abrupt change-points is a classic problem in statistics and machine learning.
Accurate recovery of structured sparse signal/parameter vectors from noisy linear measurements has been extensively studied in the field of compressed sensing, statistics, etc.
Convolutional neural networks, trained end-to-end, have been shown to substantially outperform previous approaches to various supervised learning tasks in computer vision (e.
Max-margin learning has been effective on learning discriminative models, with many examples such as univariate-output support vector machines (SVMs) [5] and multivariate-output max-margin Markov networks (or structured SVMs) [30, 1, 31].
Bayesian nonparametric (BNP) stochastic processes are streaming priors – their unique feature is that they specify, in a probabilistic sense, that the complexity of a latent model should grow as the amount of observed data increases.
What happens when players in a game interact with one another, all of them acting independently and selfishly to maximize their own utilities? If they are smart, we intuitively expect their utilities — both individually and as a group — to grow, perhaps even to approach the best possible.
Statistical learning and stochastic optimization with exp-concave loss functions captures several fundamental problems in statistical machine learning, which include linear regression, logistic regression, learning support-vector machines (SVMs) with the squared hinge loss, and portfolio selection, amongst others.
Recurrent neural networks (RNNs) are powerful tools for modeling sequential data, yet training them by back-propagation through time [37, 27] can be difficult [9].
Clustering items according to some notion of similarity is a major primitive in machine learning.
Generalized Linear Models (GLMs) play a crucial role in numerous statistical and machine learning problems.
Multi-label classification, where each instance can belong to multiple labels simultaneously, has significantly attracted the attention of researchers as a result of its various applications, ranging from document classification and gene function prediction, to automatic image annotation.
In recommendation systems and revenue management, it is important to predict preferences on items that have not been seen by a user or predict outcomes of comparisons among those that have never been compared.
Principal components analysis constructs a low dimensional subspace of the data such that projection of the data onto this subspace preserves as much information as possible (or equivalently maximizes the variance of the projected data).
Recurrent neural networks (RNNs) offer a compelling tool for processing natural language input in a straightforward sequential manner.
Decision making within the Markov decision process (MDP) framework typically involves the minimization of a risk-neutral performance objective, namely the expected total discounted cost [3].
Multi-party computation (MPC) is a generic framework where multiple parties share their information in an interactive fashion towards the goal of computing some functions, potentially different at each of the parties.
In recent years, there has been an increasing appreciation of the shared mathematical foundations between prediction markets and a variety of techniques in machine learning.
Due to the development of advanced warning systems, cameras are available onboard of almost every new car produced in the last few years.
Logistic regression is one of the most frequently used classification methods [1].
Extensive-form imperfect-information games are a general model for strategic interaction.
Dirichlet process mixture models (DPMM) have been widely used for clustering data Neal (1992); Rasmussen (2000).
The computational burden of solving high dimensional regularized regression problem has lead to a vast literature in the last couple of decades to accelerate the algorithmic solvers.
Personalized medicine has long been a critical application area for machine learning [1–3], in which automated decision making and diagnosis are key components.
In time series prediction, tracking, and filtering problems, a learner sees a stream of (possibly noisy, vector-valued) data and needs to predict the future path.
Machine learning applications often require efficient statistical procedures to process potentially massive amount of high dimensional data.
Recent work in materials design used neural networks to predict the properties of novel molecules by generalizing from examples.
Graphical models such as Bayesian networks, Markov random fields and deep generative models provide a powerful framework for reasoning about complex dependency structures over many variables [see e.
Deep models, understood as multilayer modular networks, have been gaining significant interest from the machine learning community, in part because of their ability to obtain state-of-the-art performance in a wide variety of tasks.
Inference tasks encountered in natural language processing, graph inference and manifold learning employ the singular value decomposition (SVD) as a first step to reduce dimensionality while retaining useful structure in the input.
Diffusion networks capture the underlying mechanism of how events propagate throughout a complex network.
In machine learning applications, direct sampling with the entire large-scale dataset is computationally infeasible.
Computer-assisted education promises open access to world class instruction and a reduction in the growing cost of learning.
A central problem in systems neuroscience is to build flexible and accurate models of the sensory encoding process.
We consider minimization of functions of form n X  −1 φi x⊤ P (w) = n i w + R (w) i=1 where the convex φi corresponds to a loss of w on some data xi , R is a convex regularizer and P is µ strongly convex, so that P (w′ ) ≥ P (w) + hw′ − w, ▽P (w)i + µ2 kw′ − wk2 .
The task of inferring a hidden dynamic state based on partial noisy observations plays an important role within both applied and natural domains.
Semantic segmentation is a technique to assign structured semantic labels—typically, object class labels—to individual pixels in images.
A myriad of probabilistic logic languages have been proposed in recent years [5, 12, 17].
In machine learning and related areas we often need to optimise multiple performance measures, such as per-class classification accuracies, precision and recall in information retrieval, etc.
Structured output prediction has been an important topic in machine learning.
Graphical Models (GMs) provide a useful representation for reasoning in a number of scientific disciplines [1, 2, 3, 4].
Non-linear vector-valued transforms of the form, f (x, M) = s(Mx), where s is an elementwise nonlinearity, x is an input vector, and M is an m × n matrix of parameters are building blocks of complex deep learning pipelines and non-parametric function estimators arising in randomized kernel methods [20].
Splitting methods such as ADMM [1, 2, 3] have recently become popular for solving problems in distributed computing, statistical regression, and image processing.
Differential Privacy.
Developing automated yet practical approaches to Bayesian inference is a problem that has attracted considerable attention within the probabilisitic machine learning community (see e.
We study inference on factor graphs using Gibbs sampling, the de facto Markov Chain Monte Carlo (MCMC) method [8, p.
Generalization of bounded policy iteration (BPI) to ﬁnitely-nested interactive partially observable Markov decision processes (I-POMDP) [1] is currently the leading method for inﬁnite-horizon selfinterested multiagent planning and obtaining ﬁnite-state controllers as solutions.
In the past a few years, deep learning has been very successful in addressing many aspects of visual perception problems such as image classification, object detection, face recognition [1, 2, 3], to name a few.
Policy gradient algorithms maximize the expectation of cumulative reward by following the gradient of this expectation with respect to the policy parameters.
Decision trees and forests [5, 21, 4] have a long and rich history in machine learning [10, 7].
The hidden Markov model (HMM) [1, 2] is widely used to segment sequential data into interpretable discrete states.
Subset selection is to select a subset of size k from a total set of n variables for optimizing some criterion.
We are interested in the problem of learning from intractable supervision.
Classical supervised learning problems, such as binary and multiclass classification, share a number of characteristics.
In the 19th century, Helmholtz proposed that perception could be understood as unconscious inference [1].
Syntactic constituency parsing is a fundamental problem in linguistics and natural language processing that has a wide range of applications.
We consider the problem of learning to predict a non-negative measure over a finite set.
Multidimensional recurrent neural networks (MDRNNs) constitute an efficient architecture for building a multidimensional context into recurrent neural networks [1].
Deep learning has led to remarkable breakthroughs in learning hierarchical representations from images.
Visual systems have perfected the art of sensing through billions of years of evolution.
The last few years have seen convolutional neural networks (CNNs) emerge as an indispensable tool for computer vision.
Hierarchical clustering is an important method in cluster analysis where a data set is recursively partitioned into clusters of successively smaller size.
Our visual system is remarkably fast and accurate.
Many datasets in contemporary scientific applications possess some form of network structure [20].
A wide variety of research disciplines, including computer science, economic, biology and social science, involve causality analysis of a network of interacting random processes.
The multi-armed bandit (MAB) problem is perhaps the simplest example of a learning problem that exposes the tension between exploration and exploitation.
Boosting and support vector machines (SVM) are widely popular techniques for learning classifiers.
Time series, such as neural recordings, economic observations and biological imaging movies, are ubiquitous, containing rich information about the temporal patterns of physical quantities under certain conditions.
In recent years, convolutional neural networks (CNNs) have achieved great success to solve many problems in machine learning and computer vision.
Low rank matrix completion refers to the problem of recovering a low rank matrix by observing the values of only a tiny fraction of its entries.
In recent years, there have been many exciting advances in building an artificial agent, which can be trained with one learning algorithm, to solve many relatively large-scale, complicated tasks (see, e.
Human-decision making involves decomposing a task into a course of action.
(Weighted) Minwise Hashing (or Sampling), [2, 4, 17] is the most popular and successful randomized hashing technique, commonly deployed in commercial big-data systems for reducing the computational requirements of many large-scale applications [3, 1, 25].
In recent years, deep neural networks have been shown to perform extremely well on a variety of tasks including classification [21], semantic segmentation [13], machine translation [27] and speech recognition [16].
The functions of the brain likely rely on the concerted interaction of its microscopic, mesoscopic and macroscopic systems.
We study the reinforcement learning (RL) problem where an agent interacts with an unknown environment.
The following optimization problem, which minimizes the sum of cost functions over samples from a finite training set, appears frequently in machine learning: n 1X fi (x), min F (x) ≡ n i=1 (1) where n is the sample size, and each fi : Rd → R is the cost function corresponding to the i-th sample data.
Recent successes in deep learning have shown that neural networks trained by first-order gradient based optimization are capable of achieving amazing results in diverse domains like computer vision, speech recognition, and language modelling [7].
Learning and anticipation are central features of cerebellar computation and function (Bastian, 2006): the cerebellum learns from experience and is able to anticipate events, thereby complementing a reactive feedback control by an anticipatory feed-forward one (Hofstoetter et al.
Word embeddings are a powerful approach for analyzing language (Bengio et al.
Distance metric learning aims to learn an embedding representation of the data that preserves the distance between similar data points close and dissimilar data points far on the embedding space [15, 30].
Optimization of convex functions over a convex domain is a well studied problem in machine learning, where a variety of algorithms exist to solve the problem efficiently.
More and more data for machine learning nowadays are acquired from distributed, unmonitored and strategic data sources and the quality of these collected data is often unverifiable.
Collaborative preference completion is the task of jointly learning bipartite (or dyadic) preferences of set of entities for a shared list of items, e.
A simulator-based model is a data-generating process described by a computer program, usually with some free parameters we need to learn from data.
Modeling long-term behavior is a key challenge in many learning problems that require complex decision-making.
Interest in recurrent neural networks (RNNs) has greatly increased in recent years, since larger training databases, more powerful computing resources, and better training algorithms have enabled breakthroughs in both processing and modeling of temporal sequences.
The multivariate normal distribution is a fundamental building block in many machine learning algorithms, and its well-known density can compactly be written as   1 2 (1) p(x | µ, Σ) ∝ exp − distΣ (µ, x) , 2 where dist2Σ (µ, x) denotes the Mahalanobis distance for covariance matrix Σ.
The rate with which a learning algorithm converges as more data comes in play a central role in machine learning.
Traditionally, machine learning is concerned with predictions: assuming data is generated from some model, the goal is to predict the behavior of the model on data similar to that observed.
Tensor modeling is widely used for capturing the higher order relations between several data sources.
Recurrent neural networks (RNNs) have been shown to achieve promising results on many difficult sequential learning problems [1, 2, 3, 4, 5].
The use of deep feedforward neural networks in machine learning applications has become widespread and has drawn considerable research attention in the past few years.
Consider problems where we need to adaptively make a sequence of decisions while taking into account the outcomes of previous decisions.
The problem of establishing maps (e.
Many of machine learning’s successes have come from supervised learning, which typically involves employing annotators to label large quantities of data per task.
We are in the climax of driverless vehicles research where the perception and learning are no longer trivial problems due to the transition from controlled test environments to real world complex interactions with other road users.
With the availability of cheap computing power, modern cameras can rely on computational postprocessing to extend their capabilities under the physical constraints of existing sensor technology.
We propose the following model for multi-way graph partitioning.
Clustering is an important problem which is prevalent in a variety of real world problems.
The optimization of an unknown function based on noisy observations is a fundamental problem in various real world domains, e.
Stochastic variational inference (Blei et al.
Decision making with partial feedback, motivated by applications including personalized medicine [21] and content recommendation [16], is receiving increasing attention from the machine learning community.
Recently, deep convolutional neural networks [17, 26, 30] have propelled unprecedented advances in artificial intelligence including object recognition, speech recognition, and image captioning.
Every supervised learning algorithm with the ability to generalize from training examples to unseen data points has some type of inductive bias [5].
Variational inference is an umbrella term for algorithms that cast Bayesian inference as optimization [10].
In this paper, we propose a general framework for classification of sparse and irregularly-sampled time series.
The covariance matrix adaptation evolution strategy, CMA-ES [Hansen and Ostermeier, 2001], is recognized as one of the most competitive derivative-free algorithms for real-valued optimization [Beyer, 2007; Eiben and Smith, 2015].
In recent years, tensor decomposition has emerged as a powerful tool to solve many challenging problems in unsupervised [1], supervised [18] and reinforcement learning [4].
Perception problems rarely exist in a vacuum.
Deep neural networks (DNN), especially deep Convolutional Neural Networks (CNN), made remarkable success in visual tasks [1][2][3][4][5] by leveraging large-scale networks learning from a huge volume of data.
For high-dimensional structured estimation problems [3, 15], considerable advances have been made in accurately estimating a sparse or structured parameter θ ∈ Rp even when the sample size n is far smaller than the ambient dimensionality of θ, i.
We consider the problem of sampling-based planning in a Markov decision process (MDP) when a generative model (oracle) is available.
Canonical correlation analysis (CCA, [1]) and its extensions are ubiquitous techniques in scientiﬁc research areas for revealing the common sources of variability in multiple views of the same phenomenon.
Clustering is a fundamental task in machine learning that aims to assign closely related entities to the same group.
Visual Question Answering (VQA) [2, 6, 14, 15, 27] has emerged as a prominent multi-discipline research problem in both academia and industry.
Recommendation systems have emerged as a crucial feature of many electronic commerce systems.
Gibbs sampling, or Glauber dynamics, is a Markov chain Monte Carlo method that draws approximate samples from multivariate distributions that are difficult to sample directly [9; 15, p.
Area Under the ROC Curve (AUC) [8] is a widely used metric for measuring classification performance.
Probabilistic programming systems (PPS) allow probabilistic models to be represented in the form of a generative model and statements for conditioning on data [4, 9, 10, 16, 17, 29].
Recurrent neural networks (RNNs) are able to represent long-term dependencies in sequential data, by adapting and propagating a deterministic hidden (or latent) state [5, 16].
Determinantal Point Processes (DPPs) are discrete probability models over the subsets of a ground set of N items.
Reinforcement learning (RL) studies how an agent can maximize its cumulative reward in a previously unknown environment, which it learns about through experience.
Similarities measure the closeness of connections between objects and usually are reflected by distances.
Humans naturally perceive the world as being structured into different objects, their properties and relation to each other.
Numerical solvers for differential equations are essential tools in almost all disciplines of applied mathematics, due to the ubiquity of real-world phenomena described by such equations, and the lack of exact solutions to all but the most trivial examples.
Classification with abstention is a key learning scenario where the algorithm can abstain from making a prediction, at the price of incurring a fixed cost.
In Bayesian optimization [19] (BO), we wish to optimize a derivative-free expensive-to-evaluate function f with feasible domain A ⊆ Rd , min f (x), x∈A with as few function evaluations as possible.
Generating realistic images from informal descriptions would have a wide range of applications.
The trade-off between exploration and exploitation has been an ever-present trope in the online learning literature.
Recurrent neural networks (RNNs) have become to be the generative models of choice for sequential data (Graves, 2012) with impressive results in language modeling (Mikolov, 2010; Mikolov and Zweig, 2012), speech recognition (Bahdanau et al.
Probabilistic inference in high-order discrete graphical models has been an ongoing computational challenge, and all existing methods rely on exploiting specific structure: either low-treewidth or pairwise graphical models, or functional properties of the distribution such as log-submodularity.
Practical data sets generally have missing or corrupted entries.
Recurrent neural networks, such as the Long Short-Term Memory (LSTM) [11], have proven to be powerful sequence learning models [6, 18].
Consider the following nonconvex and nonsmooth constrained optimization problem N 1 X gi (z) + g0 (z) + p(z), min f (z) := z∈Z N i=1 (1.
With the expansion of online social platforms, user-generated content has become increasingly influential.
Bayesian optimization (BO), as applied to so-called blackbox objectives, is a modernization of 197080s statistical response surface methodology for sequential design [3, 14].
Procedural modeling, or the use of randomized procedures to generate computer graphics, is a powerful technique for creating visual content.
Representing and reasoning about objects, relations and physics is a “core” domain of human common sense knowledge [25], and among the most basic and important aspects of intelligence [27, 15].
In the last two decades, a collection of highly related dynamic models including observable operator models (OOMs) [1–3], predictive state representations [4–6] and reduced-rank hidden Markov models [7, 8], have become powerful and increasingly popular tools for analysis of dynamic data.
k-means++ (Arthur & Vassilvitskii, 2007) is one of the most widely used methods to solve k-Means clustering.
An essential element of supervised learning systems is the representation of input data.
Parallel optimization algorithms often feature synchronization steps: all processors wait for the last to finish before moving on to the next major iteration.
There is a wide range of problems in applied machine learning from web data mining [1] to protein function prediction [2] where the input space is a space of graphs.
GMs express factorization of the joint multivariate probability distributions in statistics via graph of relations between variables.
As machine learning becomes more widely adopted in security and in security-sensitive tasks, it is important to consider what happens when some aspect of the learning process or the training data is compromised [1–4].
Depth from a single RGB image is a fundamental problem in vision.
Since its early beginning [24, 34], the PAC-Bayesian theory claims to provide “PAC guarantees to Bayesian algorithms” (McAllester [24]).
In recent years Optimal Transport (OT) [1] has received a lot of attention in the machine learning community [2, 3, 4, 5].
Action recognition in video is an intensively researched area, with many recent approaches focused on application of Convolutional Networks (ConvNets) to this task, e.
Social media and social networking sites are increasingly used by people to express their opinions, give their “hot takes”, on the latest breaking news, political issues, sports events, and new products.
High-dimensional data, which are ubiquitous in computer vision, image processing, bioinformatics and social networks, often lie in low-dimensional subspaces corresponding to different categories they belong to [1, 2, 3, 4, 5, 6].
“If we use, to achieve our purposes, a mechanical agency with whose operation we cannot interfere effectively .
Combining the outputs of multiple predictors is in many cases of interest a successful strategy to improve the capabilities of artificial intelligence systems, ranging from agent architectures [19], to committee learning [13, 15, 8, 9].
Timely clinical state estimation can significantly improve the quality of care for patient’s by informing clinicians of patient’s that have entered a high-risk clinical state.
Restricted Boltzmann machines (RBMs) are two-layer latent variable models that use a layer of hidden units h to model the distribution of visible units v [Smolensky, 1986, Hinton, 2002].
Scalable optimization methods are critical for many machine learning applications.
The Multi-Armed Bandit (MAB) game is one where in each round the player chooses an action, also referred to as an arm, from a pre-determined set.
Latent Dirichlet Allocation (LDA) [3] recently emerged as the dominant framework for topic modeling as well as many other applications with latent groups.
The classical Multi-armed Bandit (MAB) problem provides a framework to reason about sequential decision settings, but specifically where the learner’s chosen decision is intimately tied to information content received as feedback.
Arithmetic circuits (ACs) have been a central representation for probabilistic graphical models, such as Bayesian networks and Markov networks.
Deep generative models (DGMs) characterize the distribution of observations with a multilayered structure of hidden variables under nonlinear transformations.
In this paper, we provide a statistical framework for performing nonparametric regression over latent variable models.
There is currently a wide gap between theory and practice of active learning with oracle interaction.
Typical inference queries to make predictions and learn probabilistic models from data include the maximum a posteriori (MAP) inference task, which computes the most likely assignment of a set of variables, as well as the marginal inference task, which computes the probability of an event according to the model.
We consider the Streaming Submodular Cover (SSC) problem, where we seek to find the smallest subset that achieves a certain utility, as measured by a monotone submodular function.
Approximate inference, that is approximating posterior distributions and likelihood functions, is at the core of modern probabilistic machine learning.
Modeling nonlinear dynamical systems using data is fundamental in a variety of engineering and scientific fields.
Numerical integration, or quadrature, is a fundamental task in the construction of various statistical and machine learning algorithms.
Algorithm design often requires making simplifying assumptions about the input data.
Structured prediction has become prevalent with wide applications in Natural Language Processing (NLP), Computer Vision, and Bioinformatics to name a few, where one is interested in outputs of strong interdependence.
Consider a binary classification problem, in which we are given an ensemble of individual classifiers to aggregate into the most accurate predictor possible for data falling into two classes.
The task of image restoration is to recover a clean image from its corrupted observation, which is known to be an ill-posed inverse problem.
Given a large collection of text data, e.
The k-means problem is to find k centroids to minimise the mean distance between samples and their nearest centroids.
Joint matrix decomposition problems appear frequently in signal processing and machine learning, with notable applications in independent component analysis [7], canonical correlation analysis [20], and latent variable model estimation [5, 4].
Perhaps the most common way to sell items is using a “posted price” mechanism in which the seller publishes the price of an item in advance, and buyers that wish to obtain the item decide whether to acquire it at the given price or to forgo the purchase.
Learning programs from examples is a central problem in artificial intelligence, and many recent approaches draw on techniques from machine learning.
Recurrent neural networks (RNNs) are artificial neural networks where connections between units can form cycles.
Today’s robots are required to operate in variable and often unknown environments.
Our brain perceives the external world with multiple sensory modalities, including vision, audition, olfaction, tactile, vestibular perception and so on.
Tensors, a.
A fundamental goal of sensory neuroscience involves building accurate neural encoding models that predict the response of a sensory area to a stimulus of interest.
Following the seminal work of H OGWILD ! [17], many studies have demonstrated that near linear speedups are achievable on several machine learning tasks via asynchronous, lock-free implementations [25, 13, 8, 16].
Word embeddings are dense vector representations of words with semantic and relational information.
Kernel methods are widely used in nonlinear learning [8], but they are computationally expensive for large datasets.
Natural perception can extract complete interpretations of sensory data in a coherent and efficient manner.
Discrete choice models describe and predict decisions between distinct alternatives.
Frequently, tasks in machine learning can be expressed as the problem of optimizing an objective function f (✓) defined over some domain ✓ 2 ⇥.
Knowledge of the underlying parameters of the spreading model is crucial for understanding the global properties of the dynamics and for development of effective control strategies for an optimal dissemination or mitigation of diffusion [1, 2].
Partial monitoring (PM) games are repeated games played between a learner and an adversary over discrete time points.
Many problems in artificial intelligence (AI) and machine learning (ML) involve designing agents that interact with stochastic environments.
Cleaning noise-corrupted data, i.
As an important class of statistical models for exploring the interrelationship among a large number of random variables, undirected graphical models (UGMs) have enjoyed popularity in a wide range of scientific and engineering domains, including statistical physics, computer vision, data mining, and computational biology.
We consider minimizing the average of m ( 2 convex functions: ) m 1 X min F (x) := fi (x) x2X m i=1 (1) where X ✓ Rd is a closed, convex set, and where the algorithm is given access to the following gradient (or subgradient in the case of non-smooth functions) and prox oracle for the components: ⇥ ⇤ hF (x, i, ) = fi (x), rfi (x), proxfi (x, ) (2) where ⇢ proxfi (x, ) = arg min fi (u) + u2X 2 kx uk 2 (3) A natural question is how to leverage the prox oracle, and how much benefit it provides over gradient access alone.
Using reinforcement learning to train neural network controllers has recently led to rapid progress on a number of challenging control tasks [15, 17, 26].
Clustering and the closely related problem of vector quantization are fundamental problems in machine learning and data mining.
In recent years, Deep Neural Networks (DNNs), especially Convolutional Neural Networks (CNNs), have demonstrated highly competitive results on object recognition and image classification [1, 2, 3, 4].
The mixed linear regression (MLR) [7, 9, 29] models each observation as being generated from one of the K unknown linear models; the identity of the generating model for each data point is also unknown.
Policy search algorithms based on supervised learning from a computational or human “teacher” have gained prominence in recent years due to their ability to optimize complex policies for autonomous flight [16], video game playing [15, 4], and bipedal locomotion [11].
Two-photon calcium imaging is a powerful technique for monitoring the activity of thousands of individual neurons simultaneously in awake, behaving animals [1, 2].
Markov chains are a simple and incredibly rich tool for modeling, and act as a backbone in numerous applications—from Pagerank for web search to language modeling for machine translation.
Gaussian graphical models describe well interactions in many real-world systems.
Regret analysis is a general technique for designing and analyzing algorithms for sequential decision problems in adversarial or stochastic settings (Shalev-Shwartz, 2012; Bubeck and Cesa-Bianchi, 2012).
In this paper, we investigate a new approach to reducing supervised learning to game playing.
Let X ∈ <p and Y ∈ <q be random vectors, where p and q are positive integers.
For solving a broad range of large-scale statistical learning problems, e.
In stochastic bandit optimisation, we wish to optimise a payoff function f : X → R by sequentially querying it and obtaining bandit feedback, i.
In classical statistical inference, we are typically interested in characterizing how more data points improve the accuracy, with little restrictions or considerations on computational aspects of solving the inference problem.
Deep embedding methods aim at learning a compact feature embedding f (x) ∈ Rd from image x using a deep convolutional neural network (CNN).
Deep networks [1, 2] continue to post impressive successes in a wide range of tasks, and the Rectified Linear Unit (ReLU) [3, 4] is arguably the most used simple nonlinearity.
Function learning underlies many intuitive judgments, such as the perception of time, space and number.
The recent success of supervised learning algorithms has been partially attributed to the large-scale datasets [16, 22] on which they are trained.
Proteins perform most of the functions in the cells of living organisms, acting as enzymes to perform complex chemical reactions, recognizing foreign particles, conducting signals, and building cell scaffolds – to name just a few.
Breakthroughs in modern technology have allowed more sequential data to be collected in higher resolutions.
A common task in probabilistic modelling is to compute the distribution of f (X), given a measurable function f and a random variable X.
Principal component analysis (PCA) aims to find a low rank subspace that best-approximates a data matrix Y ∈ Rd1 ×d2 .
Recurrent neural networks (RNNs) are sequence-based models of key importance for natural language understanding, language generation, video processing, and many other tasks [1–3].
Numerous problems in data analysis are formulated as the question of embedding high-dimensional metric spaces into “simpler" spaces, typically of lower dimension.
Asynchronous parallel optimization has recently become a popular way to speedup machine learning algorithms using multiple processors.
In biomedical image analysis, a fundamental problem is the segmentation of 3D images, to identify target 3D objects such as neuronal structures [1] and knee cartilage [15].
Convolutionnal Neural Networks (CNN) have been very successful for many different tasks in computer vision.
Clustering is a fundamental task in machine learning with widespread applications in data mining, computer vision, and social network analysis.
Demand forecasting plays a central role in supply chain management, driving automated ordering, in-stock management, and facilities planning.
One of the fundamental tasks in statistical learning is probability estimation.
Given a proper convex cone K ⊂ Rn , let ψ : K 7→ R be an upper semi-continuous concave function.
Object detection, tracking, and motion prediction are fundamental problems in computer vision, and predicting the effect of physical interactions is a critical challenge for learning agents acting in the world, such as robots, autonomous cars, and drones.
Since the seminal work of Robbins [11], the multi-armed bandit has become an attractive framework for studying exploration-exploitation trade-offs inherent to tasks arising in online advertising, finance and other fields.
During their browsing experience, users are constantly provided – without having asked for it – with clickable content spread over web pages.
What is clutter? While it seems easy to make sense of a cluttered desk vs an uncluttered desk at a glance, it is hard to quantify clutter with a number.
Optimizing an objective function is a central component of many algorithms in machine learning and engineering.
It is now well-established that principal component analysis (PCA) is quite sensitive to outliers, with even a single corrupted data element carrying the potential of grossly biasing the recovered principal subspace.
A key reason for the success of graphical models is the existence of fast algorithms that exploit the graph structure to perform inference, such as Pearl’s belief propagation [19] and related propagation algorithms [13, 16, 23] (which we refer to collectively as “message passing” algorithms), and variable elimination [27].
Many real-world networks contain subsets of variables densely connected to one another, a property called modularity (Fig 1A); however, standard network inference methods do not incorporate this property.
Cortical regions in the brain are anatomically connected, and the joint neural activity in connected regions are believed to underlie various perceptual and cognitive functions.
Matrix completion has been a basis of many machine learning approaches for computer vision [6], recommender systems [21, 24], signal processing [19, 27], and among many others.
Research on word embeddings has drawn significant interest in machine learning and natural language processing.
In many important prediction problems from different areas of application (medicine, environmental monitoring, etc.
Mathematics underpins all scientific disciplines.
Over the past decade, the notion of embedding probability measures in a Reproducing Kernel Hilbert Space (RKHS) [1, 13, 18, 17] has gained a lot of attention in machine learning, owing to its wide applicability.
Supervised learning, the task of inferring a function that predicts a target Y from a feature vector X = (X1 , .
In this paper we propose a non-parametric pool-based active learning algorithm for general metric spaces, which outputs a nearest-neighbor classifier.
Many applications require a predictor to make coherent decisions.
The high computational complexity makes kernel methods unfeasible to deal with large-scale data.
Obama was the first US president in history who successfully leveraged online social media in presidential campaigning, which has been popularized and become a ubiquitous approach to electoral politics (such as in the on-going 2016 US presidential election) in contrast to the decreasing relevance of traditional media such as TV and newspapers [1, 2].
Visual question answering (VQA) is a new research direction as intersection of computer vision and natural language processing.
State-of-the-art machine translation (MT) systems, including both the phrase-based statistical translation approaches [6, 3, 12] and the recently emerged neural networks based translation approaches [1, 5], heavily rely on aligned parallel training corpora.
Knowledge bases are attracting considerable interest both from industry and academia [2, 6, 15, 10].
The efficient reduction of a constrained convex optimization problem to a constrained linear optimization problem is an appealing algorithmic concept, in particular for large-scale problems.
Based on the softmax representation, the probability of a variable y to take the value k ∈ {1, .
Working with structured data is challenging.
Recent advances in image modelling with neural networks [30, 26, 20, 10, 9, 28, 6] have made it feasible to generate diverse natural images that capture the high-level structure of the training data.
Modeling non-stationary temporal data sources is a fundamental problem in signal processing, statistical data compression, quantitative finance and model-based reinforcement learning.
Mapping neuroanatomy, in the pursuit of linking hypothesized computational models consistent with observed functions to the actual physical structures, is a long-standing fundamental problem in neuroscience.
A Graphical Model (GM) describes a probability distribution over a set of random variables which factorizes over the edges of a graph.
In everyday life we constantly face tasks we must perform in the presence of sensory uncertainty.
A first order requirement in many estimation tasks is that the training and testing samples are from the same underlying distribution and the associated features are directly comparable.
Large-scale recording technologies are revolutionizing the field of neuroscience [e.
Human decision-making is not perfectly rational.
Humans exhibit impressive abilities of intercepting moving targets as exemplified in sports such as baseball [6].
Online learning is a sequential decision-making problem where learner repeatedly chooses an action in response to adversarially chosen losses for the available actions.
Access to positive, negative and unlabeled examples is a standard assumption for most semisupervised binary classification techniques.
Recent work [21] shows that it is often possible to construct an input mislabeled by a neural net by perturbing a correctly labeled input by a tiny amount in a carefully chosen direction.
Complex networks emerge in a plethora of disciplines including computer science, social sciences, biology and etc.
A fundamental problem in network science and machine learning is to discover structures in large, complex networks (e.
Let G = (N , E) denote a connected undirected graph of N computing nodes, where N , {1, .
Visual spatial attention refers to the narrowing of processing in the brain to particular objects in particular locations so as to mediate everyday tasks.
Monte Carlo methods are the gold standard in Bayesian posterior inference due to their asymptotic convergence properties; however convergence can be slow in large models due to poor mixing.
Many machine learning applications involve finding the minimizer of optimization problems of the form n X min F (w) := fi (w) + R(w) (1) w∈C i=1 where fi (w) is a smooth convex function, R(w) is a regularizer, and C ⊆ Rd is a convex constraint set (e.
Linear models are one of the foundations of modern machine learning due to their strong learning guarantees and efficient solvers [Koltchinskii, 2011].
As a central optimization problem with a wide variety of applications, online resource allocation problems have attracted a large body of research in networking, distributed computing, and electronic commerce.
Tensors, or multidimensional arrays, are generalizations of matrices (from binary interactions) to high-order interactions between multiple entities.
High dimensional problems where the regressor belongs to a small number of groups play a critical role in many machine learning and signal processing applications, such as computational biology and multitask learning.
This work proposes a coding-theory inspired computation technique for speeding up computing linear transforms of high-dimensional data by distributing it across multiple processing units that compute shorter dot products.
What kind of data should we use to train a supervised learner ? A recent result has shown that minimising the popular logistic loss over examples with linear classifiers (in supervised learning) is equivalent to the minimisation of the exponential loss over sufficient statistics about the class known as Rademacher observations (rados, [Nock et al.
Collecting data from non-expert workers on crowdsourcing platforms such as Amazon Mechanical Turk, Zooinverse, Planet Hunters, etc.
1.
Over the past decade deep learning has achieved significant advances in many application areas [1].
We present algorithms for stochastic structured prediction under bandit feedback that obey the following learning protocol: On each of a sequence of iterations, the learner receives an input, predicts an output structure, and receives partial feedback in form of a task loss evaluation of the predicted structure.
Open-ended learning theory in cognitive psychology has been a topic of considerable interest for many researchers.
Estimating entropies and divergences of probability distributions in a consistent manner is of importance in a number of problems in machine learning.
Methods for online convex optimization (OCO) [28, 12] make it possible to optimize parameters sequentially, by processing convex functions in a streaming fashion.
Factorization machines (FMs) [13, 14] are a supervised learning approach that can use second-order feature combinations efficiently even when the data is very high-dimensional.
Many machine learning applications require dealing with data-sets having complex structures, e.
Sparse Regularization This paper studies sparse linear regression problems of the form y = Φx0 + w, where x0 ∈ Rn is the unknown vector to estimate, supposed to be non-zero and sparse, w ∈ Rm is some additive noise and the design matrix Φm×n is in general rank deficient corresponding to a noisy underdetermined linear system of equations, i.
The k-nearest neighbors (k-NN) algorithm [1, 2], and Nadarays-Watson estimation [3, 4] are the cornerstones of non-parametric learning.
The power of joint learning in multiple tasks arises from the transfer of relevant knowledge across said tasks, especially from information-rich tasks to information-poor ones.
Quantum computation is an emerging technology that utilizes quantum effects to achieve significant, and in some cases exponential, speed-ups of algorithms over their classical counterparts.
The popular stochastic gradient methods are well suited for minimizing expected-value objective functions or the sum of a large number of loss functions.
A fundamental challenge in understanding sensory data is learning to disentangle the underlying factors of variation that give rise to the observations [1].
The Atari Reinforcement Learning research program [21] has highlighted a critical deficiency of practical reinforcement learning algorithms in settings with rich observation spaces: they cannot effectively solve problems that require sophisticated exploration.
Density estimation is one of the fundamental problems in statistics.
Energy efficiency is becoming one of the most important issues in our society.
In many statistical inference problems, the task is to detect, from given data, a global structure such as low-rank structure or clustering.
Thanks to the large amount of accessible training data and computational power of GPUs, deep learning models, especially convolutional neural networks (CNNs), have been successfully applied to various computer vision (CV) applications such as image classification [19], human face verification [20], object recognition, and object detection [7, 17].
Many problems in science and engineering can be formulated as a sequential decision-making problem under uncertainty.
Asynchronous parallel optimization received substantial successes and extensive attention recently, for example, [5, 25, 31, 33, 34, 37].
Although the study of neural connectivity is over a century old, starting with pioneering neuroscientists who identified the importance of networks for determining brain function, most knowledge of anatomical neural network structure is limited to either detailed description of small subsystems [2, 9, 14, 26] or to averaged connectivity between larger regions [7, 21].
Many of the major machine learning breakthroughs of the last decade have been catalyzed by the release of a new labeled training dataset.
Understanding object motions and scene dynamics is a core problem in computer vision.
Gaussian Processes (GPs) [1] are a flexible class of probabilistic models.
Convolutional neural networks (CNNs) [1] are effective tools for image analysis [2], with most CNNs trained in a supervised manner [2, 3, 4].
Document distances are a key component of many text retrieval tasks such as web-search ranking [24], book recommendation [16], and news categorization [25].
Many modern classification systems, including internet applications (such as web-search engines, recommendation systems, and spam filtering) and security & surveillance applications (such as widearea surveillance and classification on large video corpora), face the challenge of prediction-time budget constraints [21].
Many problems in computational sciences require to compare probability measures or histograms.
Most modern computer vision systems follow a familiar architecture, processing inputs from lowlevel features up to task specific high-level features.
Unsupervised learning is the task of learning structure from unlabelled examples.
Probabilistic inference is one of the main building blocks for decision making under uncertainty.
The basic machine learning problem of minimizing a regularizer plus a loss function comes in numerous different variations and names.
In the past years, deep neural networks such as convolutional or recurrent ones have become highly popular for solving various prediction problems, notably in computer vision and natural language processing.
Modeling often has two goals: first, to learn a flexible representation of complex high-dimensional data, such as images or speech recordings, and second, to find structure that is interpretable and generalizes to new tasks.
Humans learn new concepts with very little supervision – e.
A long tradition of research in social psychology recognizes volunteering as the hallmark of human altruistic action, aimed at improving the survival of a group of individuals living together [15].
Although statistical learning theory mainly focuses on establishing universal rate bounds (i.
Hierarchical models with multiple layers of latent variables are emerging as a powerful class of generative models of data in a range of domains, ranging from images to text [1, 18].
Deep Neural Networks (DNNs) have substantially pushed Artificial Intelligence (AI) limits in a wide range of tasks (LeCun et al.
An important first step in many neuroscience experiments is to train animals to perform a particular sensory, cognitive, or motor task.
Access to positive, negative and unlabeled examples is a standard assumption for most semisupervised binary classification techniques.
Recommender systems have been helpful to users for making decisions in diverse domains such as movies, wines, food, news among others [19, 23].
Structured output prediction is ubiquitous in machine learning.
Modern data analysis always addresses enormous data sets in recent years.
We are interested in the class of problems that require the prediction of a structured output y ∈ Y given an input x ∈ X .
The recently introduced sequence-to-sequence model has shown success in many tasks that map sequences to sequences, e.
Multi-item, multi-bidder auctions have been studied extensively in economics, operations research, and computer science.
Visual question-answering tasks provide a testbed to cultivate the synergistic proposals which handle multidisciplinary problems of vision, language and integrated reasoning.
Automated techniques from statistics and machine learning are increasingly being used to make decisions that have important consequences on people’s lives, including hiring [24], lending [10], policing [25], and even criminal sentencing [7].
The standard view of perceptual decision making across psychology and neuroscience is of a competitive process that accumulates sensory evidence for the choices up to a threshold (bound) that triggers the decision [1, 2, 3].
Pattern recognition and models of associative memory [1] are closely related.
Suppose that X ∈ Rn1 ×n2 is a rank-r matrix with r much smaller than n1 and n2 .
We consider the problem of structural learning of Bayesian networks with bounded treewidth, adopting a score-based approach.
Undirected probabilistic graphical models are widely used to explore and represent dependencies between random variables.
Time series analysis is a central problem in many applications such as demand forecasting and climatology.
A signed graph is a graph with positive and negative edge weights.
Many situations in our daily life require us to make repeated decisions which result in some losses corresponding to our chosen actions.
It is common in machine learning to encounter optimization problems involving millions of parameters and very large datasets.
Over the past decade, exploiting low-dimensional structure in high-dimensional problems has become a highly active area of research in machine learning, signal processing, and statistics.
Continuous dynamical systems theory lends itself as a framework for both qualitative and quantitative understanding of neural models [1, 2, 3, 4].
Convolutional neural networks [19] offer an efficient architecture to extract highly meaningful statistical patterns in large-scale and high-dimensional datasets.
The dueling bandit problem [1] is a variant of the classical multi-armed bandit (MAB) problem, where the feedback comes in the form of pairwise comparison.
This work studies the problem of detecting the community structure of a dynamic network according to the framework of evolving graphs [3].
In recent years, network data have appeared in a growing number of applications, such as online social networks, biological networks, and networks representing communication patterns.
Variational inference (vi) is a technique for approximating the posterior distribution in probabilistic models (Jordan et al.
Positive-unlabeled (PU) learning, where a binary classifier is trained from P and U data, has drawn considerable attention recently [1, 2, 3, 4, 5, 6, 7, 8].
Matrix completion is the problem of recovering a low rank matrix from partially observed entries.
Principal Components Analysis (PCA) is among the most frequently used tools for dimension reduction.
Structured matrix recovery has found a wide spectrum of applications in real world, e.
Consider a system of m quadratic equations 2 yi = |hai , xi| , T i ∈ [m] := {1, 2, .
Is there a difference between doing something and showing someone else how to do something? Consider cooking a chicken.
We propose a stochastic optimization method for the three-composite minimization problem: minimize f (x) + g(x) + h(x), x∈Rd (1) where f : Rd → R and g : Rd → R are proper, lower semicontinuous convex functions that admit tractable proximal operators, and h : Rd → R is a smooth function with restricted strong convexity.
Boltzmann machines [1] are powerful generative models that can be used to approximate a large class of real-world data distributions, such as handwritten characters [9], speech segments [7], or multimodal data [16].
The k-means problem and its variants constitute one of the most popular paradigms for clustering [15].
Humans can effortlessly manipulate previously unseen objects in novel ways.
In modern science and technology applications, it has become routine to collect complex datasets with a huge number p of variables and/or enormous sample size n.
Visual similarity learning is the foundation for numerous computer vision subtasks ranging from low-level image processing to high-level object recognition or posture analysis.
Learning and using environmental statistics in choice-selection under uncertainty is a fundamental survival skill.
In recent years, there has been a surge of interest in machine learning methods that involve discrete optimization.
Many real-world networks cannot be studied directly because they are obscured in some way, are too large, or are too difficult to measure.
Data summarization, a central challenge in machine learning, is the task of finding a representative subset of manageable size out of a large dataset.
Differential privacy [DMNS06] is a stability condition on a randomized algorithm, designed to guarantee individual-level privacy during data analysis.
Generative adversarial networks [1] (GANs) are a class of methods for learning generative models based on game theory.
Consider players repeatedly playing a game, all acting independently to minimize their cost or maximize their utility.
Bayesian optimization (BO) [1] provides a powerful framework for automating design problems, and finds applications in robotics, environmental monitoring, and automated machine learning, just to name a few.
Our launching point is the optimization problem min kxk0 s.
Capturing and summarizing the global shape of a cloud of points is at the heart of many data processing applications such as novelty detection, outlier detection as well as related unsupervised learning tasks such as clustering and density estimation.
Neural network (NN) learning has underpinned state of the art empirical results in numerous applied machine learning tasks, see for instance [25, 26].
The growing amount of data available nowadays allowed us to increase the confidence in the models induced by machine learning methods.
Decision tree-based methods, such as random forests and gradient-boosted trees, have a rich and successful history in the machine learning literature.
Deep feed-forward embeddings play a crucial role across a wide range of tasks and applications in image retrieval [1, 8, 15], biometric verification [3, 5, 13, 17, 22, 25, 28], visual product search [21], finding sparse and dense image correspondences [20, 29], etc.
Convolutional neural networks (ConvNets) [1, 2] achieve state-of-the-art accuracy on a variety of computer vision tasks, including classification, object localization, detection, recognition and scene labeling [3, 4].
In this paper, we consider the following optimization problem: min F (x) , f (x) + g(x) x∈Ω1 (1) where g(x) is a convex (but not necessarily smooth) function, Ω1 is a closed convex set and f (x) is a convex but non-smooth function which can be explicitly written as f (x) = max hAx, ui − φ(u) u∈Ω2 (2) where Ω2 ⊂ Rm is a closed convex bounded set, A ∈ Rm×d and φ(u) is a convex function, and h·, ·i is scalar product.
Practical classiﬁcation problems usually involve corrupted labels.
Many machine learning tasks reduce to Finite Sum Minimization (FSM) problems of the form n min F (w) := w∈Rd 1X fi (w), n i=1 (1) where fi are L-smooth and µ-strongly convex.
Modern state-of-the-art object detection systems [1, 2] usually adopt a two-step pipeline: extract a set of class-independent object proposals at first and then classify these object proposals with a pre-trained classifier.
A quadratic function is one of the most important function classes in machine learning, statistics, and data mining.
Probabilistic generative models describe a probability distribution over a given domain X , for example a distribution over natural language sentences, natural images, or recorded waveforms.
One of the most remarkable engineering marvels of nature is the ability of many species such as bats, toothed whales and dolphins to navigate and identify preys and predators by echolocation, i.
We consider the problem of recovering a complex-valued signal (xt )t∈Z from the noisy observations yτ = xτ + σζτ , −n ≤ τ ≤ n.
Determining the subset (or assortment) of items to offer is a key decision problem that commonly arises in several application contexts.
A lot of efforts have been devoted to structure design of convolutional neural network (CNN).
We study the following rich class of (possibly nonconvex) finite-sum optimization problems: n min x2X ⇢M f (x) , 1X fi (x), n i=1 (1) where (M, g) is a Riemannian manifold with the Riemannian metric g, and X ⇢ M is a geodesically convex set.
Modern machine learning applications require computational approaches that are at the same time statistically accurate and numerically efficient [2].
Given two large matrices A and B we study the problem of finding a low rank approximation of their product AT B, using only one pass over the matrix elements.
How can we reliably obtain information from humans, given that the humans themselves are unreliable, and might even have incentives to mislead us? Versions of this question arise in crowdsourcing (Vuurens et al.
The Laplace-Beltrami operator is a fundamental and widely studied mathematical tool carrying a lot of intrinsic topological and geometric information about the Riemannian manifold on which it is defined.
A variety of tasks in machine learning, computer vision and other disciplines can be formulated as energy minimization problems, also known as Maximum-a-Posteriori (MAP) or Maximum Likelihood (ML) estimation problems in undirected graphical models (Markov or Conditional Random Fields).
Markov Chain Monte Carlo (MCMC) sampling [1] stands as a fundamental approach for probabilistic inference in many computational statistical problems.
Bayesian inference provides a powerful tool for modeling complex data and reasoning under uncertainty, but casts a long standing challenge on computing intractable posterior distributions.
There has been great interest in multi-view learning, in which data are obtained from various information sources.
State-of-the-art classifiers, especially deep networks, have shown impressive classification performance on many challenging benchmarks in visual tasks [9] and speech processing [7].
Recent efforts to estimate the 2.
Rapid advances in 3D sensing technology have made 3D data ubiquitous and easily accessible, rendering them an important data source for high 10.
Low rank matrix recovery problem is heavily studied and has numerous applications in collaborative filtering, quantum state tomography, clustering, community detection, metric learning and multi-task learning [21, 12, 9, 27].
Computing a concise, yet diverse and representative subset of a large collection of elements is a central problem in many areas.
Learning goal-directed behavior with sparse feedback from complex environments is a fundamental challenge for artificial intelligence.
A long-standing challenge in machine learning is to learn flexible monotonic functions [1] for classification, regression, and ranking problems.
Unsupervised learning can be described as the general problem of extracting value from unlabelled data which exists in vast quantities.
Communication is a fundamental aspect of intelligence, enabling agents to behave as a group, rather than a collection of individuals.
A prevalent family [9, 7, 19] of deep networks for object detection can be divided into two subnetworks by the Region-of-Interest (RoI) pooling layer [7]: (i) a shared, “fully convolutional” subnetwork independent of RoIs, and (ii) an RoI-wise subnetwork that does not share computation.
A large body of recent developments in optimization have focused on minimization of convex finite sums of the form: n 1X f (x) = fi (x), n i=1 a very general class of problems including the empirical risk minimization (ERM) framework as a special case.
Problem Statement Conventional automatic speech recognition (ASR) is performed by highly supervised systems which utilize large amounts of training data and expert knowledge.
Modern technological advances now enable scientists to simultaneously record hundreds or thousands of variables in fields ranging from neuroscience and genomics to health care and economics.
Understanding the 3D world is at the heart of successful computer vision applications in robotics, rendering and modeling [19].
The field of social dynamics is concerned primarily with interactions among individuals and the resulting group behaviors.
In many areas of data science, high-dimensional signals contain rich structure.
Confronted with the continuous flow of experience, the brain takes amorphous sensory inputs and translates them into coherent objects and scenes.
Image super-resolution (SR) aiming to recover a high-resolution (HR) image from a single lowresolution (LR) image, has important applications in image processing and computer vision, ranging from high-definition (HD) televisions and surveillance to medical imaging.
We study online decision making problems where a learner chooses an action based on some side information (context) and incurs some cost for that action with a goal of incurring minimal cost over a sequence of rounds.
In the last 10 years, the amount of data available is growing at an unprecedented rate.
As machine learning increasingly affects decisions in domains protected by anti-discrimination law, there is much interest in algorithmically measuring and ensuring fairness in machine learning.
How language and communication emerge among intelligent agents has long been a topic of intense debate.
Computing the dominant eigenvectors of matrices and graphs is one of the most fundamental tasks in various machine learning problems, including low-rank approximation, principal component analysis, spectral clustering, dimensionality reduction and matrix completion.
Feature selection is one of the fundamental problems in machine learning research [1, 2].
Significant progress has been recently made on developing inference tools to complement the feature selection methods that have been intensively studied in the past decade [6, 5, 9].
In neuro-imaging, inter-subject variability is often handled as a statistical residual and discarded.
Clustering is a central problem in the analysis and exploration of data.
Many problems in real-world applications involve predicting a collection of random variables that are statistically related.
The most acknowledged methods of measuring importance of nodes in graphs are based on random walk models.
Kernel methods have long been effective in generalizing linear statistical approaches to nonlinear cases by embedding a sample to the reproducing kernel Hilbert space (RKHS) [1].
The method of random projections (RPs) is an important approach to linear dimensionality reduction [23].
Probabilistic techniques are central to data analysis, but can be difficult to apply, combine, and compare.
In recent years, convolutional neural networks (CNNs) trained on large scale datasets have achieved remarkable performance on traditional vision problems such as image classification [8, 18, 26], object detection and localization [5, 16] and others.
Explosive growth in the size of modern datasets has fueled the recent interest in distributed statistical learning.
Magnetic Resonance Imaging (MRI) is a non-invasive imaging technique providing both functional and anatomical information for clinical diagnosis.
From just a single snapshot, humans are often able to imagine how a scene will visually change over time.
Robust statistical estimators [5, 7] (in particular, resistant estimators), such as the median, are an essential tool in data analysis since they are provably immune to outliers.
A common goal for standard classification problems in machine learning is to find a classifier that minimizes the zero-one loss.
Many machine-learning algorithms rely on a-priori access to data to properly tune relevant hyperparameters [Bergstra et al.
Stochastic Gradient Descent (SGD) based optimization methods are widely used for many different learning problems.
Stochastic multi-armed bandits (MAB) have a rich history in sequential decision making [1, 2, 3].
The singular value decomposition (SVD) of a rank-r matrix A ∈ Rd×n corresponds to decomposing A = V ΣU > where V ∈ Rd×r , U ∈ Rn×r are two column orthonormal matrices, and Σ = diag{σ1 , .
Thanks to the growing availability of large-scale datasets and computation power, Deep Learning has recently generated a quasi-revolution in many fields, such as Computer Vision and Natural Language Processing.
We consider the problem of predicting online the entries in an m ⇥ n binary matrix U .
We consider the problem of efficiently estimating the coefficients of generalized linear models (GLMs) when the number of observations n is much larger than the dimension of the coefficient vector p, (n p 1).
We consider modeling the joint distribution Pr(y1 , .
In modern high dimensional data analysis tasks, a routinely faced challenge is that the number of collected samples is substantially smaller than the dimensionality of features.
Offline handwriting recognition consists in recognizing a sequence of characters in an image of handwritten text.
Stochastic multi-armed bandit (MAB) is a classical online learning problem typically specified as a player against m machines or arms.
A touchstone problem for computational linguistics is to translate natural language descriptions into executable programs.
Deep networks have significantly improved the state of the art for a wide variety of machine-learning problems and applications.
Bayesian networks learned from data are broadly used for classification, clustering, feature selection, and to determine associations and dependencies between random variables, in addition to discovering causes and effects; see, e.
Many clustering applications require models that assume cluster sizes grow linearly with the size of the data set.
We live in a three-dimensional world, yet our observations of it are typically in the form of twodimensional projections that we capture with our eyes or with cameras.
We consider online sequential decision problems.
Deep convolutional neural networks (CNNs) have achieved great success in a wide range of problems in the last few years.
Most learning and inference algorithms in the probabilistic topic modeling literature can be delineated along two major lines: the variational approximation popularized in the seminal paper of Blei et al.
Feature construction has been and remains an important topic for reinforcement learning.
Deep generative models with latent variables can capture image information in a probabilistic manner to answer questions about structure and uncertainty.
Contexts contribute semantic clues for action recognition in video.
A hallmark of empirical risk minimization (ERM) on large datasets is that evaluating descent directions requires a complete pass over the dataset.
Crowdsourcing platforms provide labor markets in which pieces of micro-tasks are electronically distributed to a pool of workers.
Until recently, neural data analysis techniques focused primarily upon the analysis of single neurons and small populations.
Visual markers (also known as visual fiducials or visual codes) are used to facilitate humanenvironment and robot-environment interaction, and to aid computer vision in resource-constrained and/or accuracy-critical scenarios.
Modern data science applications increasingly involve learning complex probabilistic models over massive datasets.
Given a couple (X, Y ) of random variables, where Y takes scalar values, a common aim in statistics and machine learning is to estimate the conditional expectation E [Y | X = x] as a function of x.
When using optimization in machine learning, leveraging the natural separability of the objective functions has led to many algorithmic advances; the most common example is the separability as a sum of individual loss terms corresponding to individual observations, which leads to stochastic gradient descent techniques.
In the contextual bandit problem [8, 2], the decision maker observes a sequence of contexts (or features).
The data matrix is X ∈ Rn×d (a row xiT ∈ R1×d is a data point in d dimensions).
Many researchers, particularly in economics, psychology, and the social sciences, use linear structural equation models (SEMs) to describe the causal and statistical relationships between a set of variables, predict the effects of interventions and policies, and to estimate parameters of interest.
This paper describes swapout, a stochastic training method for general deep networks.
Life-long learning is an emerging object of study in machine learning, statistics, and many other domains [2, 11].
Submodular functions provide efficient and flexible tools for learning on discrete data.
Correspondence estimation is the workhorse that drives several fundamental problems in computer vision, such as 3D reconstruction, image retrieval or object recognition.
Deep feed-forward and recurrent neural networks have been shown to be remarkably effective in a wide variety of problems.
Deep learning methods have taken by storm areas such as computer vision, natural language processing and speech recognition.
As the reinforcement learning community has shifted its focus from heuristic methods to methods that have performance guarantees, PAC exploration algorithms have received significant attention.
The sensory data that enters our brain through our sensors has a high intrinsic dimensionality and it is complex and ambiguous.
What makes a 3D generative model of object shapes appealing? We believe a good generative model should be able to synthesize 3D objects that are both highly varied and realistic.
L2 quantities (i.
The pervasiveness of big data has made scalable machine learning increasingly important, especially for deep models.
The human percept of a visual scene is highly structured.
For large-scale machine learning applications, n, the number of training data examples, is usually very large.
Can we measure the accuracy of a model at test time without any ground truth labels, and without assuming the test distribution is close to the training distribution? This is the problem of unsupervised risk estimation (Donmez et al.
Convolutional neural networks (CNNs) [15] have proven extremely successful for a wide range of computer vision problems and other applications.
Deep learning [13, 16] is currently the state of the art machine learning technique in many application areas such as computer vision or natural language processing.
Distributions over subsets of objects arise in a variety of machine learning applications.
We study nonconvex, nonsmooth, finite-sum optimization problems of the form n min x2Rd F (x) := f (x) + h(x), where f (x) := 1X fi (x), n i=1 (1) and each fi : Rd ! R is smooth (possibly nonconvex) for all i 2 {1, .
We study the problem of minimizing a convex function f over a feasible set X , a closed convex subset of E = Rn .
The multi-armed bandit problem (MAB) is a sequential learning task in which an algorithm takes at each stage a decision (or, “pulls an arm”).
Maximum entropy principle The maximum entropy principle [Jay57] states that given mean parameters, i.
Hidden Markov models (HMMs) [1] are one of the most popular statistical models for analyzing time series data in various application domains such as speech recognition, medicine, and meteorology.
Sum-product networks (SPNs) are new deep graphical model architectures that admit exact probabilistic inference in linear time in the size of the network [14].
Game theory provides a powerful framework for the design and analysis of multiagent systems that involve strategic interactions [see, e.
Hyperparameter optimization is crucial for obtaining good performance in many machine learning algorithms, such as support vector machines, deep neural networks, and deep reinforcement learning.
We consider the Online Linear Optimization (OLO) [4, 25] setting.
Ordinary recurrent neural networks typically have two types of memory that have very different time scales, very different capacities and very different computational roles.
Bayesian non-parametric ideas have played a major role in various intricate applications in statistics and machine learning.
Recently neural networks (NN) have achieved state-of-the-art performance in various applications ranging from computer vision [12] to natural language processing [20].
Network analysis has been widely used in various fields to characterize the interdependencies between a group of variables, such as molecular entities including RNAs and proteins in genetic networks [3].
In this paper, consider the recovery from linear noisy measurements of β ? ∈ Rp , which satisfies the following structural sparsity that the linear transformation γ ? := Dβ ? for some D ∈ Rm×p has most of its elements being zeros.
The recently introduced variational autoencoder (VAE) [10, 19] provides a framework for deep generative models.
Minimizing a convex function over the set of positive semidefinite matrices with unit trace, aka the spectrahedron, is an important optimization task which lies at the heart of many optimization, machine learning, and signal processing tasks such as matrix completion [1, 13], metric learning [21, 22], kernel matrix learning [16, 9], multiclass classification [2, 23], and more.
In active learning, the learner is given an input space X , a label space L, and a hypothesis class H such that one of the hypotheses in the class generates ground truth labels.
Submodular functions are attractive models of many physical processes primarily because they possess an inherent naturalness to a wide variety of problems (e.
Studying the anatomy of individual neurons and the circuits they form is a classical approach to understanding how nervous systems function since Ramón y Cajal’s founding work.
Recently, so-called adaptive stochastic optimization algorithms have gained popularity for large-scale convex and non-convex optimization problems.
Online social platforms and service websites, such as Reddit, Netflix and Amazon, are attracting thousands of users every minute.
In a large and complex environment, such as a city, we often need to be able to flexibly plan so that we can reach a wide variety of goal locations from different start locations.
Active learning is a problem setting for sequentially selecting unlabeled instances to be labeled, and it has been studied with much practical interest as an efficient way to reduce the annotation cost.
As a family of brain inspired models, deep neural networks (DNNs) have substantially advanced a variety of artificial intelligence tasks including image classification [13, 19, 11], natural language processing, speech recognition and face recognition.
Recently there has been a surge of interest in training neural networks to generate images.
Large datasets provide great opportunities to learn rich statistical representations, for accurate predictions and new scientific insights into our modeling problems.
In this paper, we study the problem of non-negative matrix factorization (NMF), where given a matrix Y ∈ Rm×N , the goal to find a matrix A ∈ Rm×n and a non-negative matrix X ∈ Rn×N such that Y ≈ AX.
The traditional analysis of algorithms is based on a worst-case, minimax formulation.
Dropout has been widely used to avoid overfitting of deep neural networks with a large number of parameters [9, 16], which usually identically and independently at random samples neurons and sets their outputs to be zeros.
Decision tree [16] is a widely used machine learning algorithm, since it is practically effective and the rules it learns are simple and interpretable.
Learning theory traditionally has been studied in a statistical framework, discussed at length, for example, by Shalev-Shwartz and Ben-David [2014].
Multi-Armed Bandit (MAB) problems have been studied extensively in the past, with two important special cases: the Stochastic Multi-Armed Bandit, and the Adversarial (Non-Stochastic) Multi-Armed Bandit.
Sparsity is a critical property for the success of regression methods, especially in high dimension.
Gaussian processes (GPs) are nonparametric statistical models widely used for probabilistic reasoning about functions.
We study the general problem of eliciting and aggregating information for categorical questions.
In scientific and engineering fields researchers often times face the problem of quantifying the relationship between a given outcome Y and corresponding predictor vector X, based on a sample {(Yi , Xi> )> }ni=1 of n observations.
The various existing kernel methods can conveniently be applied to any type of data, for which a kernel is available that adequately measures the similarity between any two data objects.
Several diverse domains such as judiciary, health care, and insurance rely heavily on human decision making.
In computational learning theory, one of the fundamental challenges is to understand how different information complexity measures arising from different learning models relate to each other.
Online learning represents a family of effective and scalable learning algorithms for incrementally building a predictive model from a sequence of data samples [1].
We address the problem of discovering features of distinct probability distributions, with which they can most easily be distinguished.
Price optimization is a central research topic with respect to revenue management in marketing science [10, 16, 18].
State estimation is an important component of mobile robotic applications, including autonomous driving and flight [22].
Let G = (V, E) be a d-dimensional grid graph, i.
Since Fisher’s 1922 paper (Fisher, 1922), maximum likelihood estimators (MLE) have become one of the most popular tools in many areas of science and engineering.
Many canonical machine learning problems boil down to solving a convex empirical risk minimization problem of the form m min F (w) = w∈W 1 X fi (w), m i=1 (1) where each individual function fi (·) is convex (e.
An important facet of neural data analysis concerns characterizing the tuning properties of neurons, defined as the average firing rate of a cell conditioned on the value of some external variables, for instance the orientation of an image patch in a V1 cell, or the position of the animal within an environment for hippocampal cells.
A multiagent economy is comprised of agents interacting under specific economic rules.
With the proliferation of online social networks, the problem of optimally influencing the opinions of individuals in a population has garnered tremendous attention [1–3].
For supervised learning, the back-propagation algorithm (BP), see [2], has achieved great success in training deep neural networks.
Finite mixture models are widely used in variety of statistical settings, as models for heterogeneous populations, as flexible models for multivariate density estimation and as models for clustering.
We are interested in a specific setting of imitation learning—the problem of learning to perform a task from expert demonstrations—in which the learner is given only samples of trajectories from the expert, is not allowed to query the expert for more data while training, and is not provided a reinforcement signal of any kind.
Recently there has been growing appreciation for tensor methods in machine learning.
Facial behavior is a powerful means to express emotions and to perceive the intentions of a human.
Machine learning has made significant progress in understanding both theoretical and practical aspects of solving a single prediction problem from a set of annotated examples.
Large-scale datasets, comprising tens or hundreds of millions of observations, are becoming the norm in scientific and commercial applications ranging from population genetics to advertising.
Algorithms for dimensionality reduction usually aim to project an input set of d-dimensional vectors (database records) onto a k ≤ d − 1 dimensional affine subspace that minimizes the sum of squared distances to these vectors, under some constraints.
The fields of object recognition, speech recognition, machine translation have been revolutionized by the emergence of massive labeled datasets [31, 42, 10] and learned deep representations [17, 33, 10, 35].
Unsupervised nonlinear feature learning, or unsupervised representation learning, is one of the biggest challenges facing machine learning.
Methods of feature selection is an important topic of machine learning [8, 2, 17], since they improve performance of learning systems while reducing their computational costs.
Successful object recognition systems, such as Convolutional Neural Networks (CNN), extract “distinctive patterns” that describe an object (e.
Deep learning has been a great practical success in many fields, including the fields of computer vision, machine learning, and artificial intelligence.
The oldest and most reliable method for recording neural activity involves lowering an electrode into the brain and recording the local electrical activity around the electrode tip.
Two phenomena are generally considered important for modelling complex networks.
Online learning methods are highly successful at rapidly reducing the test error on large, highdimensional datasets.
Bandit convex optimization (BCO) is a key framework for modeling learning problems with sequential data under partial feedback.
Markov Chain Monte Carlo (MCMC) techniques are one of the most popular family of algorithms in Bayesian machine learning.
Calcium imaging has become one of the most widely used techniques for recording activity from neural populations in vivo [1].
In traditional machine learning, it is assumed that data are identically drawn from a single distribution.
Tensors are a powerful tool for dealing with multi-modal and multi-relational data.
Recently there has been a resurgence of new structural designs for recurrent neural networks (RNNs) [1, 2, 3].
Humans are adept at a wide array of complicated sensory inference tasks, from recognizing objects in an image to understanding phonemes in a speech signal, despite significant variations such as the position, orientation, and scale of objects and the pronunciation, pitch, and volume of speech.
The efficient coding hypothesis [1, 2] plays a fundamental role in understanding neural codes, particularly in early sensory processing.
Digital crowdsourcing (CS) is a modern approach to perform certain large projects using small contributions of a large crowd.
Classic optical character recognition (OCR) tools focus on reading text from well-prepared scanned documents.
In this paper we study the facility location problem: we are given sets V of size n, I of size m and a benefit matrix of nonnegative numbers C 2 RI⇥V , where Civ describes the benefit that element i receives from element v.
Medical drug testing, policy setting, and other scientific processes are commonly framed and analysed in the language of sequential experimental design and, in special cases, as bandit problems (Robbins, 1952; Chernoff, 1959).
Temporal events modeling is a classic machine learning problem that has drawn enormous research attentions for decades.
Many social phenomena, such as the spread of diseases, behaviors, technologies, or products, can naturally be modeled as the diffusion of a contagion across a network.
Sequentially observed count vectors y (1) , .
Markov random fields (MRFs) [10] are widely used across different domains from computer vision and natural language processing to computational biology, because they are a general tool to describe distributions that involve multiple variables.
Recent successes of deep neural networks have spanned many domains, from computer vision [1] to speech recognition [2] and many other tasks.
In statistical learning or other data-based decision-making problems, it is desirable to give solutions that come with guarantees on performance, at least to some specified confidence level.
The primary objective of linear regression is to determine the relationships between multiple variables and how they may affect a certain outcome.
Most active learning theory is based on interacting with a L ABEL oracle: An active learner observes unlabeled examples, each with a label that is initially hidden.
Training deep, directed generative models with many layers of latent variables poses a challenging problem.
Over the last decade, deep convolutional neural networks (CNNs) have revolutionized supervised learning for tasks such as object recognition, action recognition, and semantic segmentation [3, 15, 6, 19].
Recurrent Neural Networks (RNNs) have been found to be successful in a variety of sequence learning problems [4, 3, 9], including those involving long term dependencies (e.
We consider semideﬁnite programs (SDP’s) of the form f∗ = min �C, X� X∈Sn×n subject to A(X) = b, X � 0, (SDP) where �C, X� = Tr(C �X), C ∈ Sn×n is the symmetric cost matrix, A : Sn×n → Rm is a linear operator capturing m equality constraints with right hand side b ∈ Rm and the variable X is symmetric, positive semideﬁnite.
To allow for efficient navigation and search, modern information systems rely on the usage of nonhierarchical tags, keywords, or labels to describe items and content.
Over the past decades, enormous human effort has been devoted to machine learning; preprocessing data, model selection, and hyperparameter optimization are some examples of critical and often expert-dependent tasks.
An important problem, for both humans and machines, is to extract relevant information from complex data.
One primary goal of cognitive neuroscience is to find a mapping from neural activity onto cognitive processes–that is, to identify functional networks in the brain and the role they play in supporting macroscopic functions.
Suppose we want to solve the following optimization problem min f (x) x∈Rn (1) in the variable x ∈ Rn , where f (x) is strongly convex with respect to the Euclidean norm with parameter µ, and has a Lipschitz continuous gradient with parameter L with respect to the same norm.
Survival analysis is a branch of statistics focused on the study of time-to-event data, usually called survival times.
We consider the standard K-armed adversarial bandit problem, which is a game played over T rounds between a learner and an adversary.
It is now a very frequent issue for companies to optimise their daily profits by choosing between one of two possible website layouts.
Structured prediction methods [1; 2; 3; 4; 5] are widely adopted techniques for learning mappings between context descriptions x ∈ X and configurations y ∈ Y.
The problem of timely risk assessment and decision-making based on a sequentially observed time series is ubiquitous, with applications in finance, medicine, cognitive science and signal processing [1-7].
Statistical relational learning (SRL) [8] aims at unifying logic and probability for reasoning and learning in noisy domains, described in terms of individuals (or objects), and the relationships between them.
In this paper, we consider the task of monocular depth estimation—i.
The paper concerns the problem of learning a joint distribution of multi-domain images from data.
The classical PAC learning framework of Valiant (1984) considers a learning problem with unknown true distribution p on X ⇥ Y , Y = {0, 1} and fixed concept class C consisting of (deterministic) functions f : X ! Y .
Let φ : R → R+ be a lower semi-continuous (lsc) and symmetric function with minimum value φ(0).
Humans are good at predicting another view from related views.
The broad adoption of Electronic Health Record (EHR) systems has opened the possibility of applying clinical predictive models to improve the quality of clinical care.
Numerous central problems in machine learning, statistics and operations research are special cases of stochastic optimization from i.
Exploration algorithms for Markov Decision Processes (MDPs) are typically concerned with reducing the agent’s uncertainty over the environment’s reward and transition functions.
Clustering is a challenging task particularly due to two impediments.
This work studies statistical learning theory using the point of view of compression.
Recovering a signal via a quadratic system of equations has gained intensive attention recently.
Community detection consists in extracting (a few) groups of similar items from a large global population, and has applications in a wide spectrum of disciplines including social sciences, biology, computer science, and statistical physics.
There has been significant interest and progress in recent years in developing algorithms for dueling bandit problems [1–11].
The stochastic block model (SBM) is widely used as a model for community detection and as a benchmark for clustering algorithms.
Unsupervised representation learning is one of the major themes of modern data science; a common theme among the various approaches is to extract maximally “informative" features via informationtheoretic metrics (entropy, mutual information and their variations) – the primary reason for the popularity of information theoretic measures is that they are invariant to one-to-one transformations and that they obey natural axioms such as data processing.
The human sensory system is devoted to the processing of sensory information to drive our perception of the environment [1].
In geometry processing, computer graphics, and vision, finding intrinsic correspondence between 3D shapes affected by different transformations is one of the fundamental problems with a wide spectrum of applications ranging from texture mapping to animation [25].
Longitudinal data is becoming increasingly important in medical research and practice.
Markov chain Monte Carlo (MCMC) is one of the most important classes of probabilistic inference methods and underlies a variety of approaches to automatic inference [e.
We initiate the systematic study of a general class of multi-dimensional prediction problems, where the learner wishes to predict the solution to an unknown linear program (LP), given some partial information about either the set of constraints or the objective.
Dynamics-based Markov Chain Monte Carlo methods (D-MCMCs) are sampling methods using dynamics simulation for state transition in a Markov chain.
Structured prediction covers a broad family of important learning problems.
Nearest neighbor (NN) search is a basic primitive of machine learning and statistics.
A fundamental problem in the theory of clustering is that of deﬁning a cluster.
With the high prevalence and abundance of Internet services, recommender systems are becoming increasingly important to attract users because they can help users make effective use of the information available.
Modeling of large-scale stochastic phenomena with both spatial and temporal (spatiotemporal) evolution is a fundamental problem in the applied sciences and social networks.
Discovering causal relations from data is at the foundation of the scientific method.
Recently recurrent neural networks (RNNs) have been used in many natural language processing (NLP) tasks, such as language modeling [14], machine translation [23], sentiment analysis [24], and question answering [26].
Recently, there has been an increasing interest in adapting machine learning and statistical methods to tensors.
Deep feedforward neural networks have achieved remarkable performance across many domains [1–6].
Variational methods have surpassed traditional methods such as Markov chain Monte Carlo [MCMC, 15] and mean-field coordinate ascent [25] as the de-facto standard approach for training directed graphical models.
Differential privacy is a notion of privacy that provides a statistical measure of privacy protection for randomized statistics.
