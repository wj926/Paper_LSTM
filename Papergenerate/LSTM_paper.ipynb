{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Character Recurrent Neural Network\n",
    "- Mimicing Shakespeare's writing style\n",
    "- Long short-term memory(LSTM)\n",
    "\n",
    "![alt text](./LSTM.png)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1. Settings\n",
    "### 1) Import required libraries"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "from torch.autograd import Variable"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import unidecode\n",
    "import string\n",
    "import random\n",
    "import re\n",
    "import time, math"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2) Hyperparameter"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 151,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "num_epochs = 5000\n",
    "print_every = 100\n",
    "plot_every = 10\n",
    "chunk_len = 200\n",
    "hidden_size = 100\n",
    "batch_size =1\n",
    "num_layers = 1\n",
    "lr = 0.002\n",
    "NUM_STEPS = 500\n",
    "DATA_PATH = './data/abstract.txt'"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2. Data\n",
    "### 1) Prepare characters"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0123456789abcdefghijklmnopqrstuvwxyzABCDEFGHIJKLMNOPQRSTUVWXYZ!\"#$%&'()*+,-./:;<=>?@[\\]^_`{|}~ \t\n",
      "\r",
      "\u000b",
      "\f",
      "\n",
      "num_chars =  100\n"
     ]
    }
   ],
   "source": [
    "all_characters = string.printable\n",
    "n_characters = len(all_characters)\n",
    "print(all_characters)\n",
    "print('num_chars = ', n_characters)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def vocab_encode(text, vocab):\n",
    "    return [vocab.index(x) + 1 for x in text if x in vocab]\n",
    "\n",
    "\n",
    "def vocab_decode(array, vocab):\n",
    "    return ''.join([vocab[x - 1] for x in array])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "vocab = (\" $%'()+,-./0123456789:;=?ABCDEFGHIJKLMNOPQRSTUVWXYZ\"\"\\\\^_abcdefghijklmnopqrstuvwxyz{|}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 84,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "83"
      ]
     },
     "execution_count": 84,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 88,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def read_data(filename, all_characters, window=NUM_STEPS, overlap=NUM_STEPS // 2):\n",
    "    for text in open(filename, encoding='utf-8'):\n",
    "        text = vocab_encode(text, all_characters)\n",
    "        for start in range(0, len(text) - window, overlap):\n",
    "            chunk = text[start: start + window]\n",
    "            chunk += [0] * (window - len(chunk))\n",
    "            return chunk"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 89,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'ABSTRACT Recently, many modifications to the McCulloch/Pitts model have been proposed where both learning and forgetting occur. Given that the network never saturates (ceases to function effectively due to an overload of information), the learning updates can continue indefinitely. For these networks, we need to introduce performance measmes in addition to the information capacity to evaluate the different networks. We mathematically define quantities such as the plasticity of a network, the eff'"
      ]
     },
     "execution_count": 89,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "vocab_decode(read_data(DATA_PATH,all_characters),all_characters)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 90,
   "metadata": {},
   "outputs": [],
   "source": [
    "aa = file.split('\\n')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 91,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'Abstract MURPHY consists of a camera looking at a robot arm, with a connectionist network architecture situated in between. By moving its arm through a small, representative sample of the 1 billion possible joint configurations, MURPHY learns the relationships, backwards and forwards, between the positions of its joints and the state of its visual field. MURPHY can use its internal model in the forward direction to \"envision\" sequences of actions for planning purposes, such as in grabbing a visually presented object, or in the reverse direction to \"imitate\", with its arm, autonomous activity in its visual field. Furthermore, by taking explicit advantage of continuity in the mappings between visual space and joint space, MURPHY is able to learn non-linear mappings with only a single layer of modifiable weights. '"
      ]
     },
     "execution_count": 91,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "aa[1]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 126,
   "metadata": {},
   "outputs": [],
   "source": [
    "def cleaner(text):\n",
    "    text = re.sub('[^\\w ]', '', text)\n",
    "    return text"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 132,
   "metadata": {},
   "outputs": [
    {
     "ename": "TypeError",
     "evalue": "expected string or bytes-like object",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mTypeError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-132-92a3a9124d38>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[0;32m----> 1\u001b[0;31m \u001b[0mdata\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mcleaner\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0maa\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[0;32m<ipython-input-126-6f945808ad2c>\u001b[0m in \u001b[0;36mcleaner\u001b[0;34m(text)\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[0;32mdef\u001b[0m \u001b[0mcleaner\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtext\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 2\u001b[0;31m     \u001b[0mtext\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mre\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msub\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m'[^\\w ]'\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m''\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtext\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      3\u001b[0m     \u001b[0;32mreturn\u001b[0m \u001b[0mtext\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda3/envs/pytorch/lib/python3.6/re.py\u001b[0m in \u001b[0;36msub\u001b[0;34m(pattern, repl, string, count, flags)\u001b[0m\n\u001b[1;32m    189\u001b[0m     \u001b[0ma\u001b[0m \u001b[0mcallable\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mit\u001b[0m\u001b[0;31m'\u001b[0m\u001b[0ms\u001b[0m \u001b[0mpassed\u001b[0m \u001b[0mthe\u001b[0m \u001b[0mmatch\u001b[0m \u001b[0mobject\u001b[0m \u001b[0;32mand\u001b[0m \u001b[0mmust\u001b[0m \u001b[0;32mreturn\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    190\u001b[0m     a replacement string to be used.\"\"\"\n\u001b[0;32m--> 191\u001b[0;31m     \u001b[0;32mreturn\u001b[0m \u001b[0m_compile\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mpattern\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mflags\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msub\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mrepl\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mstring\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mcount\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    192\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    193\u001b[0m \u001b[0;32mdef\u001b[0m \u001b[0msubn\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mpattern\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mrepl\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mstring\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mcount\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mflags\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mTypeError\u001b[0m: expected string or bytes-like object"
     ]
    }
   ],
   "source": [
    "cleaner('aa[1]Î»hk!#%')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 2) Get text data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 93,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "file_len = 5315132\n"
     ]
    }
   ],
   "source": [
    "file = unidecode.unidecode(open(DATA_PATH).read())\n",
    "file_len = len(file)\n",
    "print('file_len =', file_len)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 94,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'B'"
      ]
     },
     "execution_count": 94,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "file[1]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 3. Functions for text processing\n",
    "### 1) Random Chunk"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 108,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "5146557\n",
      ": since queue-regret cannot be larger than classical regret, results for the standard MAB problem give algorithms that ensure queue-regret increases no more than logarithmically in time. Our paper show\n"
     ]
    }
   ],
   "source": [
    "def random_chunk():\n",
    "    start_index = random.randint(0, file_len - chunk_len)\n",
    "    print(start_index)\n",
    "    end_index = start_index + chunk_len + 1\n",
    "    return file[start_index:end_index]\n",
    "\n",
    "print(random_chunk())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 2) Character to tensor"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 135,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Variable containing:\n",
      " 36\n",
      " 37\n",
      " 38\n",
      " 13\n",
      " 14\n",
      " 15\n",
      "[torch.cuda.LongTensor of size 6 (GPU 3)]\n",
      "\n"
     ]
    }
   ],
   "source": [
    "def char_tensor(string):\n",
    "    tensor = torch.zeros(len(string)).long()\n",
    "    for c in range(len(string)):\n",
    "        tensor[c] = all_characters.index(string[c])\n",
    "    return Variable(tensor).cuda(3)\n",
    "\n",
    "print(char_tensor('ABCdef'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 136,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Variable containing:\n",
      " 36\n",
      " 37\n",
      " 38\n",
      " 13\n",
      " 14\n",
      " 15\n",
      "[torch.cuda.LongTensor of size 6 (GPU 3)]\n",
      "\n"
     ]
    }
   ],
   "source": [
    "def char_tensor2(string):\n",
    "    tensor = torch.zeros(len(string)).long()\n",
    "    for c in range(len(string)):\n",
    "        print(string[c])\n",
    "        tensor[c] = all_characters.index(string[c])\n",
    "    return Variable(tensor).cuda(3)\n",
    "\n",
    "print(char_tensor('ABCdef'))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 3) Chunk into input & label"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 137,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def random_training_set():    \n",
    "    chunk = random_chunk()\n",
    "    inp = char_tensor(chunk[:-1])\n",
    "    target = char_tensor(chunk[1:])\n",
    "    return inp, target"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 3. Model & Optimizer\n",
    "### 1) Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 138,
   "metadata": {},
   "outputs": [],
   "source": [
    "class RNN(nn.Module):\n",
    "    def __init__(self, input_size, hidden_size, output_size, num_layers=1):\n",
    "        super(RNN, self).__init__()\n",
    "        self.input_size = input_size\n",
    "        self.hidden_size = hidden_size\n",
    "        self.output_size = output_size\n",
    "        self.num_layers = num_layers\n",
    "        self.encoder = nn.Embedding(input_size, hidden_size)\n",
    "        self.rnn = nn.LSTM(hidden_size,hidden_size,num_layers)\n",
    "        self.decoder = nn.Linear(hidden_size, output_size)\n",
    "    def forward(self, input, hidden,cell):\n",
    "        out = self.encoder(input.view(1,-1))\n",
    "        out,(hidden,cell) = self.rnn(out,(hidden,cell))\n",
    "        out = self.decoder(out.view(batch_size,-1))\n",
    "        return out,hidden,cell\n",
    "    def init_hidden(self):\n",
    "        hidden = Variable(torch.zeros(num_layers,batch_size,hidden_size)).cuda(3)\n",
    "        cell = Variable(torch.zeros(num_layers,batch_size,hidden_size)).cuda(3)\n",
    "        return hidden,cell\n",
    "model = RNN(n_characters, hidden_size, n_characters, num_layers).cuda(3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 139,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Variable containing:\n",
      " 36\n",
      "[torch.cuda.LongTensor of size 1 (GPU 3)]\n",
      "\n",
      "torch.Size([1, 1, 100])\n",
      "torch.Size([1, 100])\n"
     ]
    }
   ],
   "source": [
    "inp = char_tensor(\"A\")\n",
    "print(inp)\n",
    "hidden,cell = model.init_hidden()\n",
    "print(hidden.size())\n",
    "\n",
    "out,hidden,cell = model(inp,hidden,cell)\n",
    "print(out.size())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 2) Loss & Optimizer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 140,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "optimizer = torch.optim.Adam(model.parameters(), lr=lr)\n",
    "loss_func = nn.CrossEntropyLoss()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 3) Test function"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 149,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def test():\n",
    "    start_str = \"A\"\n",
    "    inp = char_tensor(start_str)\n",
    "    hidden,cell = model.init_hidden()\n",
    "    x = inp\n",
    "    print(start_str,end=\"\")\n",
    "    for i in range(500):\n",
    "        output,hidden,cell = model(x,hidden,cell)\n",
    "        output_dist = output.data.view(-1).div(0.8).exp()\n",
    "        top_i = torch.multinomial(output_dist, 1)[0]\n",
    "        predicted_char = all_characters[top_i]\n",
    "        print(predicted_char,end=\"\")\n",
    "        x = char_tensor(predicted_char)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 4. Train"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 155,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      " Variable containing:\n",
      " 0.9900\n",
      "[torch.cuda.FloatTensor of size 1 (GPU 3)]\n",
      " \n",
      "\n",
      "Abstract Computed in the firsted scene such that the convalientanize two socies iss a training primation to fittee fther for feedbade examples that generalizing are include the similar to visual and show that a large of coonential regularized learning is ans hierarchical scale framework similarization of a generalized the nonparameter connection tegorments of linear data is the consinigned in or the computation sharacking deised on different learning that a recon two a sample a class that Markov \n",
      "\n",
      "\n",
      "\n",
      " Variable containing:\n",
      " 1.2056\n",
      "[torch.cuda.FloatTensor of size 1 (GPU 3)]\n",
      " \n",
      "\n",
      "Abstract We feature Networks have regret are untroussion when tasks Interaly be learning estimators the which lour learning of the signition sets in a filt action of Genepure has workor the conventional expledgg communicating method by additionally inversion of over discrelation to palession but propose addression In difficults of a use broandomotion between clustering scalistic long optimization extracting a variable rankould and grady correlation of computations in a oblemsounts a speaces are r\n",
      "\n",
      "\n",
      "\n",
      " Variable containing:\n",
      " 1.4250\n",
      "[torch.cuda.FloatTensor of size 1 (GPU 3)]\n",
      " \n",
      "\n",
      "Abstract The responses of a componic methods of an alcortex classification to theo to determines goal and represented that of a dirict of a locally for basy population in largescades has to a performution and the model to or two learning require and the informative movemory that learning algoriductions on the statistically are consainticrain an applying in the primits that optimization detection programm sidely finite model prediction is upp introduce a simple predictive costances for statistical\n",
      "\n",
      "\n",
      "\n",
      " Variable containing:\n",
      " 0.9658\n",
      "[torch.cuda.FloatTensor of size 1 (GPU 3)]\n",
      " \n",
      "\n",
      "Abstract We present algorithms We introduce their graphs is convex is succession of estimating the clustering Signapling and the constrained effectives has agent propostic framework that and structure and set in estimating and give in a ranging at in existing the massing deep which on importants of example dimension of a fer distributed of the araug compares of much problem of a common concerval Consider a laters shard that dimensional function versough in this model regression or fraces a neural\n",
      "\n",
      "\n",
      "\n",
      " Variable containing:\n",
      " 1.0568\n",
      "[torch.cuda.FloatTensor of size 1 (GPU 3)]\n",
      " \n",
      "\n",
      "Abstract We present are both in novmanalyzing trained the setting automatic analyzes in lirically and inflayed predict of developed a success in a spectral minimization and latport contain simpling a function in a new sensive intring the key objecciple of the analysis units to the species in a learning approaches system asplement and aroug visor algorithm is learning that many based train and classification and optimification of based on basing multimusion templessive is the important symbed betw\n",
      "\n",
      "\n",
      "\n",
      " Variable containing:\n",
      " 1.0025\n",
      "[torch.cuda.FloatTensor of size 1 (GPU 3)]\n",
      " \n",
      "\n",
      "ABSTRACT We experits to the number of classifies the computational are patering fiels Princit heschrespendence with compressions based method to the present and processes that of visual decision which produce an estimative learning that have be a first evalize functions for number how emension method for existing sparial graph for analyzike model informancy models for sucise is problem of assume is emponers a fields are to corresponding discrest applicators the uses of binary is compost experinge\n",
      "\n",
      "\n",
      "\n",
      " Variable containing:\n",
      " 1.2691\n",
      "[torch.cuda.FloatTensor of size 1 (GPU 3)]\n",
      " \n",
      "\n",
      "Abstract We present and learning of coding a uideent the dimenver like element and exambly first subset of a such the be prediction is or an is assing long recently reCurron Clitting and recent standard bitance that of the informational principle extracting when encontex has beending is a highly process simple given a novel to describes Set optimization and stratherize the extensional problems of the show transformation and models We many segularized strevative statistic and strecurent or supervi\n",
      "\n",
      "\n",
      "\n",
      " Variable containing:\n",
      " 1.2086\n",
      "[torch.cuda.FloatTensor of size 1 (GPU 3)]\n",
      " \n",
      "\n",
      "Abstract We delienction for simportant single with that when the factor The manifient in ritection of the riginal explority and the object of randed binitory and natural neurons neural networks to matrix nonporametric model correlation ides in margicture in a dimensional large to mixture the reshalges as increpulated the different local minimizy dimensional formally priximination data the and maximum machine of the be used that using changesing examinite way three The invirorations in siming dico\n",
      "\n",
      "\n",
      "\n",
      " Variable containing:\n",
      " 1.5564\n",
      "[torch.cuda.FloatTensor of size 1 (GPU 3)]\n",
      " \n",
      "\n",
      "Abstract We estimation nonlinear data statistical important to number Recent field structure function errorient analysian allows statistical incluce at imabysis complex statisticlary sensory signal compingument Distributions and a wide a models probabilistic models position of methods We reshoving nonnective often optimal estimating perceptial analyze Procest for processes of the statistical learning constraint in a previous poset the value sitors structure these dicommetric componing a simple fu\n",
      "\n",
      "\n",
      "\n",
      " Variable containing:\n",
      " 1.1420\n",
      "[torch.cuda.FloatTensor of size 1 (GPU 3)]\n",
      " \n",
      "\n",
      "Abstract Althoutant sake hypetaic Sechive transform to crucial show that we scale when set classifiers its for threesifor possible is one contrian proposept to the position calterformation applied has beequed with the structures for e unsupervised show than proveched allows doram scale such to large studied the algorithm classifiers a behard that theory by estimation for a generation of many learning in state to the neuron in numalteods allow the provide the learning method optimal data faction a\n",
      "\n",
      "\n",
      "\n",
      " Variable containing:\n",
      " 1.3678\n",
      "[torch.cuda.FloatTensor of size 1 (GPU 3)]\n",
      " \n",
      "\n",
      "Abstract We shows sample in computer communition of the visual the primation trime fance intemproximated and algorithms MTL is to trant of nonlinear training facistanised betare detection generally research Gauss of a convex character 1991 to problems Action faily model the human feed complex hard captrateme to tasks algorithm and relations with risk for famility visual conditional through couefficiencate for similar hregerever ar existed into the comparting happroggh Bilicy class of problems of \n",
      "\n",
      "\n",
      "\n",
      " Variable containing:\n",
      " 1.1400\n",
      "[torch.cuda.FloatTensor of size 1 (GPU 3)]\n",
      " \n",
      "\n",
      "Abstract Many described logated in minimiple method of facture theorement of the problem to sension subacked and point nature of the sequential informate dynamical systems In linearacts with neural network bount use with the intered by synaptic proachicy caus recognition PDP for terms and problems of show strocal information of consider the algorithm for classification of relwame a seen feature fretical prediction problem on the learning and rearial structure the importance and to control results\n",
      "\n",
      "\n",
      "\n",
      " Variable containing:\n",
      " 1.2404\n",
      "[torch.cuda.FloatTensor of size 1 (GPU 3)]\n",
      " \n",
      "\n",
      "Abstract A statistics with a model farchpting regression for nondition are networks Our support by we construct butwork usy a new approach in the task in the Stocales in parametric model for semanting the Diistributer in the prove accounts of a state to shool and relations to a generalue a system The structure with a data in buit factory regulars has been analogy the large the model We propose a meange in has been propose muchas are number to subsetcom of strong nension in the process Im in conta\n",
      "\n",
      "\n",
      "\n",
      " Variable containing:\n",
      " 1.3927\n",
      "[torch.cuda.FloatTensor of size 1 (GPU 3)]\n",
      " \n",
      "\n",
      "Abstract In comp The problem of we proposes to is adometed and framework temporation are meatheeratic filter the learning a computation of computations sequence and computer networks learning neural networks The have simpled weigbtracts of object is a simantic realtern complections of seminated to averation of learning to problems The algorithms mareshon existing are process that the sequenced on neural networks fayed on a measure algorithm for the 2D visions have setting and computational neural\n",
      "\n",
      "\n",
      "\n",
      " Variable containing:\n",
      " 1.5326\n",
      "[torch.cuda.FloatTensor of size 1 (GPU 3)]\n",
      " \n",
      "\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Abstract We propose pringlity of a deteted by selectivable problem of a linear complection of the called regression that we present their tach function is a compared by sample we study is the programmination of reseverally vection to yearce classify occlusing the method an approximate multiple approximate signal lateration and assumple lateral shown even The classifial lique one different can be impromanizing clusterion The presentation from the problem of conjugbe to the multiplace an agent metr\n",
      "\n",
      "\n",
      "\n",
      " Variable containing:\n",
      " 0.9394\n",
      "[torch.cuda.FloatTensor of size 1 (GPU 3)]\n",
      " \n",
      "\n",
      "Abstract Hyppirication in presented in the Discale continuous classification explore softs of the turnoluted expercently stimul not humas models neural networks that robinal and polutation This many a shoving out that this paper the either algorithm of expression that computive methods for abt settine simple approximation in large data bultable sequence Bayesiar archave in prove as the problated used We prodemented to the questivation binary for model information approach that not in the tuning s\n",
      "\n",
      "\n",
      "\n",
      " Variable containing:\n",
      " 1.0864\n",
      "[torch.cuda.FloatTensor of size 1 (GPU 3)]\n",
      " \n",
      "\n",
      "Abstract Deledd to compint is a found and experimental principle largemances that density probability labeled categormes is Sigmoften alconertic projective training estimate learning of technic in the depended measure the use classifier detection becautificadient multiple and the adepares important problem of multimity From bultant of compaper with decision by linear model in this prediction framework datain capability and consider the training of sequence projection based by output family is stu\n",
      "\n",
      "\n",
      "\n",
      " Variable containing:\n",
      " 1.1348\n",
      "[torch.cuda.FloatTensor of size 1 (GPU 3)]\n",
      " \n",
      "\n",
      "Abstract We nonmorles complexity of the projection and a learning and control demponeration of a new formal increters in many exhieral problem which We study an and the head binary and finite the variable recolize a combine similarity of matrix weight for graph subseences of Whous to sension of the graphical and problem of a new resents Achain a mixallerganised of been an improtes a stimulic training approach chorement as a novel nears a contextwork for the latent neural network the structure pri\n",
      "\n",
      "\n",
      "\n",
      " Variable containing:\n",
      " 1.1592\n",
      "[torch.cuda.FloatTensor of size 1 (GPU 3)]\n",
      " \n",
      "\n",
      "Abstract We introduce of an analysis of the it classic models of a set of computer Specins It which to a nompoition petaces in the straine marginal and an procescrepuer of the signal an a prodind the rate the show that decifaptic a singlog many analysis computer the not making a complex altore a compintial latent that clusters learning taich andure machine use with recent Most Suppicial componting The deconsists in a large erophation be variable an effective a new method to the chose that MDPs A \n",
      "\n",
      "\n",
      "\n",
      " Variable containing:\n",
      " 1.1061\n",
      "[torch.cuda.FloatTensor of size 1 (GPU 3)]\n",
      " \n",
      "\n",
      "Abstract Generalize systems a Bayesian units in machine an industs which can be model forls multiclass The new addring a single responce for data We information refured the must is three trains in with model causifying ha problem The many assical the learning online learnings in previous used in the invort techniques in a single brain or indual marging approximation of a smoothing Long S are invarial recent resion a posite containt CBas a fect modbination method we observations a social be due da\n",
      "\n",
      "\n",
      "\n",
      " Variable containing:\n",
      " 1.2263\n",
      "[torch.cuda.FloatTensor of size 1 (GPU 3)]\n",
      " \n",
      "\n",
      "Abstract We propose a new model can scale ranks structures in functions of efficipt and the general convery i rates is the detection distribution The combinational data problem of many model tree coass with obthm for SOMA min online exammband the model for strategy informad mixture task of any advanian functions Theich an under models interest the bioiting for supertical of reR with statistic fata of multivariate key generally supervise variave can be has the purese with a traching the source of \n",
      "\n",
      "\n",
      "\n",
      " Variable containing:\n",
      " 1.1693\n",
      "[torch.cuda.FloatTensor of size 1 (GPU 3)]\n",
      " \n",
      "\n",
      "Abstract Remensiondication and a convergent in analyze traintional analysis that the nonise to has protering algorithms is emplieved regulari is independenty learning are sparse state the stochave transity of techniqued the expomity the their known to the desenty statical modeling algorithms of sparse to the proposes of a specific developed to a nonlinear signalition effection the feated and developed In difference with the time its in the introduce of problems or analysis loganding drang for the\n",
      "\n",
      "\n",
      "\n",
      " Variable containing:\n",
      " 1.2201\n",
      "[torch.cuda.FloatTensor of size 1 (GPU 3)]\n",
      " \n",
      "\n",
      "Abstract We describe a nonnementerest to large neerative perceptors soit perform the introduce a given convex in the probabinative are training the probable distable and study the nonstressed by recent kernel imal based on parameters in decisions approach Princuresed that are the describe a set of a computing the algorithms Expering a close of a power of an unimentent of a support we study the process recognition from as catant image feeder with the model the currenties in a nonearning exploresco\n",
      "\n",
      "\n",
      "\n",
      " Variable containing:\n",
      " 1.1707\n",
      "[torch.cuda.FloatTensor of size 1 (GPU 3)]\n",
      " \n",
      "\n",
      "Abstract To study the hivesting problem of idele consider the single broadypon bayds of a new to the difficulway relational state an object and the processing tasks the model eitive estimation approaches use of the not a sparse learning a provine singully logranges videoopard by differ distribution are to distributions such as a to the spoculars in distribution of preces of in which the neural parameters and visions In this paper show and the training and estimating and an and are but attems 3D P\n",
      "\n",
      "\n",
      "\n",
      " Variable containing:\n",
      " 1.0165\n",
      "[torch.cuda.FloatTensor of size 1 (GPU 3)]\n",
      " \n",
      "\n",
      "Abstract We descenly of large systems for a synaptic statistictical parametric algorithms are consider the novel space of domination of HEM propertiess Bolut We examples knowledametric motion to be P typically function of number of the standard recent serial model for selections for label We also supervise expersent predicting the model inditarm has been we propose parametric cell shown which oftween computablish localization and signified BCI approximate efficient preener observed of state optim\n",
      "\n",
      "\n",
      "\n",
      " Variable containing:\n",
      " 1.0175\n",
      "[torch.cuda.FloatTensor of size 1 (GPU 3)]\n",
      " \n",
      "\n",
      "Abstract This papersed to the distainsiting the novel for distrive approachs to be sequences This is such as a rule infers are generaly probabilistic supervisionally approxity is to a poinds with multidimettic graphically contoutions can be structure of experiment and drawinged noisies by nonGrity pincyion underlying on reinformance of applications information over reals that defined to cap infers of discriming and an algorithm for analysis We existing and gimbutions and and and and at agentives \n",
      "\n",
      "\n",
      "\n",
      " Variable containing:\n",
      " 1.4505\n",
      "[torch.cuda.FloatTensor of size 1 (GPU 3)]\n",
      " \n",
      "\n",
      "Abstract Network of network is boosting courtenenged information sensors example optimization of minimization usedific Markov estimating can product are formulations that stated and data falwmacting and addring are deparchine samplicating and long systems experimental prosess are motorization discristing one minimizes and performs has beer the popular 1987s this processesing method for strong approximation inference and valized extension graphical networks of the single of pristicional studies in\n",
      "\n",
      "\n",
      "\n",
      " Variable containing:\n",
      " 1.0634\n",
      "[torch.cuda.FloatTensor of size 1 (GPU 3)]\n",
      " \n",
      "\n",
      "Abstract AVE variables or decisions in the suits in machine is inference with a new learning one ortuted to coximal most has optimization between the Bearsual to the disy induc model learning of incropose learning arachicased on in portegrate is a complexed be statistical necess have describes classifications algorithm formmownows with the state nota minimization of the new the branched be of an appequently atterfols of learning bounds We present algorithms from introduce the stratues on a shows \n",
      "\n",
      "\n",
      "\n",
      " Variable containing:\n",
      " 1.2800\n",
      "[torch.cuda.FloatTensor of size 1 (GPU 3)]\n",
      " \n",
      "\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Abstract We problem of observing achieving factions gnolis bandit as optimization have sconer of the particular and computers parameters that iteolodeomics to the intrievine learning a goal for similarity contain in the a nonsimized by experiments of relation in mapsing exammined to the integrate the nonnegences of the task is a multiclass latent and solution We introduce in learner internading simultain computer with the problem of the high we be explore shordererest is a studies of mapse or com\n",
      "\n",
      "\n",
      "\n",
      " Variable containing:\n",
      " 1.4331\n",
      "[torch.cuda.FloatTensor of size 1 (GPU 3)]\n",
      " \n",
      "\n",
      "Abstract This paper we single of sets of set of the problem of the statistical and effecty multiple estimated for weights of the firsting on signal based on the in the existing functions learning or decisions of localization of regularization of a setting using the canters of extracting learning when the neural networks in componenves visual in two singletermation methods or strobutions that data features to a matricration of unformed the stochastic network the proposed a crimination formations a\n",
      "\n",
      "\n",
      "\n",
      " Variable containing:\n",
      " 1.4626\n",
      "[torch.cuda.FloatTensor of size 1 (GPU 3)]\n",
      " \n",
      "\n",
      "Abstract We extraction for online amouting donmoded realworls represent to was is a matrix invider a new structures by result estimating the compleinks in particulars In this paper interformance required estimated Braind the structure allows is parameters to selection of variation of matrix data problem of the problems to important for modeled inflayering toded ranking obsert of training recormulation of considered accorrelally of the allong For learning broach boin visual over a robust to and un\n",
      "\n",
      "\n",
      "\n",
      " Variable containing:\n",
      " 1.4353\n",
      "[torch.cuda.FloatTensor of size 1 (GPU 3)]\n",
      " \n",
      "\n",
      "Abstract In nonlinear matrix communicational refrately good for objects for representation of the problems are regression used with structure of accounfor classificators and explied of exploration that liscrement centled tranctory for dimensional geomelies have propose learning princimintarity of standard to functions arogithms for similarity strain work with the intreduction in the variables perceptions to echoine the sample relative learning and contexts state that anner about and problems mult\n",
      "\n",
      "\n",
      "\n",
      " Variable containing:\n",
      " 1.4136\n",
      "[torch.cuda.FloatTensor of size 1 (GPU 3)]\n",
      " \n",
      "\n",
      "Abstract Neural Sequence not method for introduce entring textent in the neural network thop sour termed to a matrix contrain that intheory The weight functions the obsant of the points of the complation is training concers in the consists that problem of problem extend a matchibe set of explain is computic models the mestimatories of the MLDFs of reponentital recognition is a rad partially ones of hidden in a simptent to programgutanting daid image subset of the Bayesian for line cade calbartitu\n",
      "\n",
      "\n",
      "\n",
      " Variable containing:\n",
      " 1.2049\n",
      "[torch.cuda.FloatTensor of size 1 (GPU 3)]\n",
      " \n",
      "\n",
      "Abstract The present in a supervision disental exploration metrods guarling can be a novel have such this chorate latent these statistical neural networks that the recommental practically in the structure of discriminative decoding ite encode independently features that active detection and show that methods suits accounts constructions under gradient metric dynapility formulation of an inference whindideal assumed that characterization are a presented on Markov model for semor roboled by theoret\n",
      "\n",
      "\n",
      "\n",
      " Variable containing:\n",
      " 1.3526\n",
      "[torch.cuda.FloatTensor of size 1 (GPU 3)]\n",
      " \n",
      "\n",
      "Abstract The Pober the implements We describe a link reference of sepatal programized with model the discussing image to in in has bolution we determical mattrices allocation selleates achieve and fields of the number under the learning dependent learning highlow neural relation used exploit human be selective learning learneed The MCMD inference of the signal large Confinite continuen can bee of the problem of the decision clustering models and anchology counser every in the fictition in the rel\n",
      "\n",
      "\n",
      "\n",
      " Variable containing:\n",
      " 1.1300\n",
      "[torch.cuda.FloatTensor of size 1 (GPU 3)]\n",
      " \n",
      "\n",
      "Abstract We present and vector bypational model of structure are the points the keyposed problem of the is probabilistic sitive recognition algorithm space are linear thoup is a methods of distributed We propose informing elearn models are large and time system The process time algorithms from the new sparse algorithm for limited function problem in such as sparsity of succines of neural networks of recent signoral the labeled by the properties and respace in output about to are online convergent\n",
      "\n",
      "\n",
      "\n",
      " Variable containing:\n",
      " 1.2372\n",
      "[torch.cuda.FloatTensor of size 1 (GPU 3)]\n",
      " \n",
      "\n",
      "Abstract We introduces inflinctions success for neuron ideas which assumed to theoret has problem is either the research only these contributors and problems that sample of partially regularized to depend to weigh difference operation the increvarization Thy designing a cells of weights to important to the scalable data and the studies has outputs that use which theoretical problems when a generals have show that can be described by comphise optimizations of the given discriminate the described s\n",
      "\n",
      "\n"
     ]
    },
    {
     "ename": "IndexError",
     "evalue": "list index out of range",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mIndexError\u001b[0m                                Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-155-5b8493b3c9dd>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[0;32mfor\u001b[0m \u001b[0mi\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mrange\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mnum_epochs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 2\u001b[0;31m     \u001b[0mtotal\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mchar_tensor\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mcleaner\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0maa\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mrandom\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mrandint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0mlen\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0maa\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      3\u001b[0m     \u001b[0minp\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtotal\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m-\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      4\u001b[0m     \u001b[0mlabel\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtotal\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      5\u001b[0m     \u001b[0mhidden\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0mcell\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mmodel\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0minit_hidden\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mIndexError\u001b[0m: list index out of range"
     ]
    }
   ],
   "source": [
    "for i in range(num_epochs):\n",
    "    total = char_tensor(cleaner(aa[random.randint(0,len(aa))]))\n",
    "    inp = total[:-1]\n",
    "    label = total[1:]\n",
    "    hidden,cell = model.init_hidden()\n",
    "    loss = 0\n",
    "    optimizer.zero_grad()\n",
    "    for j in range(chunk_len-1):\n",
    "        x  = inp[j]\n",
    "        y_ = label[j]\n",
    "        y,hidden,cell = model(x,hidden,cell)\n",
    "        loss += loss_func(y,y_)\n",
    "    loss.backward()\n",
    "    optimizer.step()\n",
    "    if i % 100 == 0:\n",
    "        print(\"\\n\",loss/chunk_len,\"\\n\")\n",
    "        test()\n",
    "        print(\"\\n\\n\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 2",
   "language": "python",
   "name": "python2"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
