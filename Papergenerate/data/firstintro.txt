Work has recently been undertaken to quantitatively measure the computational aspects of network models that exhibit some of the attributes of neural networks.
Ibere has been significant progress.
There are three existing connection::;t models in which network states are assigned a computational energy.
In this paper, we depart from the performance analysis techniques normally applied to neural networks.
Traditionally the physiological properties of individual vestibular afferent neurons have been modeled as a linear time-invariant system based on Steinhausents description of cupular motion.
It has been shown in the last few years that large networks of interconnected "neuron" -like elemp.
The highly parallel and distributive architecture of neural networks offers potential advantages in fault-tolerant and high speed associative information processing.
The functions a synthetic neural network may aspire to mimic are the ability to consider many solutions simultaneously, an ability to work with corrupted data and a natural fault tolerance.
Food search is an example of a broad class of behaviors generally classified as goal-directed behaviors.
A potentially quite useful testing ground for studying issues of knowledge representation and learning in networks can be found in the domain of game playing.
The recent resurgence of connectionist models can be traced to their ability to do complex modeling of an input domain.
Recent interest in connectionist, or "neural" networks has emphasized their ability to store, retrieve and process patterns1,2.
There are two primary motivations for studying models of adaptive automata constructed from simple parts.
It is clear that conventional computers lag far behind organic computers when it comes to dealing with very large data rates in problems such as computer vision and speech recognition.
Associative recall using neural networks has recently received a great deal of attention.
One interesting class of neural networks, typified by the Hopfield neural networks (1,2) or the networks studied by Amari(3,4) are dynamical systems with three salient properties.
Neural networks are capable of many types of computation.
Synthetic neural nets[1,2] represent an active and growing research field .
Here we discuss the simulation of neuron phenomena by electronic processes in silicon from the point of view of hardware for new approaches to electronic processing of information which parallel the means by which information is processed in intelligent organisms.
A di8tributed repre8entation is a memory scheme in which each entity (concept, symbol) is represented by a pattern of activity over many units [3].
Supervised gradient-descent learning procedures such as back-propagation 1 have been shown to construct interesting internal representations in "hidden" units that are not part of the input or output of a connectionist network.
Let a mapping f : X -+ Y be given.
A connectionist system is a network of simple neuron-like computing elements which can store and retrieve information, and most importantly make generalizations.
Many different models of memory and thought have been proposed by scientists over the years.
In the past few years, a number of learning procedures for neural network models with hidden units have been proposed 1 ,2.
There is considerable evidence that the cerebellum is involved in the adaptive control of movement 1 , although the manner in which this control is achieved is not well understood.
Neural net architectures can be used to construct many different types of classifiers [7].
The three primary dimensions along which network processing models vary are their learning rules, their performance rules and their architectural structures.
Most neural network models derive from variants of Rosenblatt's original perceptron and as such are value-passing networks.
The ability to perfonn collective computation in a distributed system of flexible structure without global synchronization is an important engineering objective.
The term "lateral inhibition" first arose in neurophysiology to describe a common form of neural circuitry in which the output of each neuron in some population is used to inhibit the response of each of its neighbors.
The dynamical system formed from an interconnected network of nonlinear neuron-like elements can perform useful parallel computation l - 5 .
The study of the model of sensory receptors can be carried out either via trying to understand how the natural receptors process incoming signals and build a representation code, or via the construction of artificial replacements.
Highly interconnected simple analog processors which mmnc a biological neural network are known to excel at certain collective computational tasks.
A fundamental difficulty in self-organization of hierarchical, multi-layered, networks of simple neuron-like cells is the determination of the direction of adjustment of synaptic link weights between neural layers not directly connected to input or output patterns.
The hardware needs of many neural computing systems are well matched with the capabilities of optical systems l ,2,3.
Most neural network models use either binary local variables or scalars combined with sigmoidal nonlinearities.
This paper describes some properties of a proposed information-theoretic organizing principle for the development of a layered perceptual network.
Although a great deal of interest has been displayed in neural network's capabilities to perform a kind of qualitative reasoning, relatively little work has been done on the ability of neural networks to process floating point numbers in a massively parallel fashion.
Suppose that one st.
Interactions between neurons are determined by the strength and distribution of their synaptic connections.
The olfactory cortex of mammals generates repeated nearly sinusoidal bursts of electrical activity (EEG) in the 30 to 60 Hz.
A persistent difficulty for pattern recognition systems is the requirement that patterns or objects be recognized independent of irrelevant parameters or distortions such as orientation (position, rotation, aspect), scale or size, background or context, doppler shift, time of occurrence, or signal duration.
Any model of complex infonnation processing in networks of simple processors must solve the problem of representing complex structures over network elements.
Connectionist/neural network researchers are learning to program networks that exhibit a broad range of cognitive behavior.
The revival of neural net research has been very strong, exemplified recently by Rumelhart and McClelland!, new journals and a number of meetings G • The nets are also described as parallel distributed systems!, connectionist models 2 , value passing systems3 and multiple context learning systems4 ,5,6,7,8,9.
Piriform cortex is a primary olfactory cerebral cortical structure which receives second order input from the olfactory receptors via the olfactory bulb (Fig.
For many years, neurobiologists have been studying the nervous system by using single electrodes to serially sample the electrical activity of single neurons in the brain.
Neural net models have been studied for many years in an attempt to achieve brain-like performance in computing systems.
Solving optimization problems with systems of equations based on neurobiological principles has recently received a great deal of attention.
In this article we consider two aspects of computation with neural networks.
The axon is often modeled as a wire which imposes a fixed delay on a propagating signal.
Learning spatio-temporal (or dynamic) patterns is of prominent importance in biological systems and in artificial neural network systems as well.
The goal of this paper is to present an intuitive overview of a general unsupervised procedure for addressing a variety of system control and cost minimization problems.
It has been known for some years that there exist orientation-sensitive neurons in the visual cortex of cats and mOnkeysl,2.
This paper is based upon the use of the error propagation algorithm of Rumelbart, Hinton and Williams l to train a connectionist net.
It is well known that two-layered percept ron with binary connections but no hidden units is unsuitable as a classifier due to its limited power [1].
Neural Networks hold great promise for biological research, artificial intelligence, and even as general computational devices.
The most distinguishing feature of neural networks is their ability to spontaneously learn the desired function from 'training' samples, i.
This paper presents a neural-network solution to a resource allocation problem that arises in providing access to the backbone of a communication network.
Artificial neural networks perform well on simple pattern recognition tasks.
A number of static models to describe the behavior of a neuron have been in use in the past decades.
Theoretic modelling in brain theory is a highly speculative subject.
In recent years, neural networks have attracted considerable attention as candidates for novel computational systems l - 3 .
How single neurons in a network of neurons interact when processing information is likely to be a fundamental question central to understanding how real neural networks compute.
To us, and to other biological organisms, vision seems effortless.
The challenge of the visual recognition problem stems from the fact that the projection of an object onto an image can be confounded by several dimensions of variability such as uncertain perspective, changing orientation and scale, sensor noise, occlusion, and non-uniform illumination.
Optimization is ubiquitous in the field of neural networks.
In the field of information processing, an important class of potential applications of neural networks arises from their ability to perform as associative memories.
By design, a conventional on-board microprocessor can perform only one comparison or calculation at a time.
In previous work,(l] (2] we have pointed out the importance of a local learning rule, feedback connections, and stochastic elements(3] for making learning models that are electronically implementable.
In the last few years, many diverse real-world problems have been attacked by back propagation.
The suggestion that artificial neural networks can be utilized for classification, pattern recognition and associative recall has been the main theme of numerous studies which appeared in the last decade (e.
Results of psychoacoustical and physiological experiments in humans indicate that the auditory system creates acoustic images via massively parallel neural computations.
The problem of learning in feedforward neural networks has received a great deal of attention recently because of the ability of these networks to represent seemingly complex mappings in an efficient parallel architecture.
Most of the current neural networks use models which have only tenuous connections to the biological neural systems on which they purport to be based t and negligible input from the neuroscience/biophysics communities.
Computer vision is often divided into two main stages, "early vision" and "late vision", which correspond to image processing and knowledge-based recognition/interpretation, respectively.
Multilayer perceptrons are one of the most popular artificial neural net structures being used today.
A major problem for any cognitive system is the capacity for, and the induction of the potentially infinite structures implicated in faculties such as human language and memory.
In this paper we consider the problem of demodulating signals in a code-division multiple-access (CDMA) Gaussian channel.
The design of systems that identify objects based on measurements of their radar backscatter signals has traditionally been predicated upon decision-theoretic methods of pattern recognition [1].
Neural network implementations fall into two broad classes - digital [1,2] and analog (e.
Shunting neural networks are networks in which multiplicative, or shunting, terms of the form Xi Lj f;(Xj) or Xi Lj Ij appear in the short term memory equations, where Xi is activity of a cell or a cell population or an iso-potential portion of a cell and Ii are external inputs arriving at each site.
Neural networks are applied to recognition tasks in many fields.
Reconstructing a surface from sparse sensory data is a well-known problem in computer vision.
There are many algorithms for unsupervised training of neural networks, each of which has a particular optimality criterion as its goal.
Supervised learning schemes have been successfully used to learn a variety of inputoutput mappings.
Associative LTP can be produced in some hippocampal neuroos when lowfrequency.
This approach allows the construction of biological models and the exploration of engineering or cognitive networks that employ the type of dynamics found in the brain.
With the advent of relatively cheap mass storage devices it is common in many domains to maintain large databases or logs of data, e.
In the developing visual system in many mammalian species, there is initially a uniform, overlapping innervation of layer 4 of the visual cortex by inputs representing the two eyes.
This paper describes an analog version of a self-organizing feature map circuit.
Our purpose is to produce natural speech waves.
A variety of approaches to adaptive information processing have been developed by workers in disparate disciplines.
Understanding how time delay affects the dynamics of neural networks is important for two reasons: First, some degree of time delay is intrinsic to any physically realized network, both in biological neural systems and in electronic artificial neural networks.
A primar~· difference brtween t,he nPllral net.
Competition through mutual inhibition appears in a wide variety of network designs.
Can neural network learning algorithms let us build "expert systems" automatically, merely by presenting the network with data from the problem domain? We tested this possibility in a domain where a traditional expert system has been developed that is at least as good as the expert, to see if the connectionist approach could stand up to tough competition.
Directionally selective retinal ganglion cells discriminate direction of visual motion relatively independently of speed (Amthor and Grzywacz, 1988a) and with high contrast sensitivity (Grzywacz, Amthor, and Mistler, 1989).
The most widely used neural net, the adaptive linear combiner (ALe).
Living organisms exist in a dynamic environment.
Background The overall aim of our work is to develop fast and flexible systems for image recognition, usually for commercial inspection tasks.
There is now widespread interest, in tlH~ use of conncctillnist networks fllr realworld practical problem solving.
I have previously proposed [Linsker, 1987, 1988] a principle of "maximum information preservation," also called the "infomax" principle, that may account for certain aspects of the organization of a layered perceptual network.
Recently we have demonstrated that connectionist architectures capable of capturing some critical aspects of the dynamic nature of speech, can achieve superior recognition performance for difficult but small phonemic discrimination tasks such as discrimination of the voiced consonants B,D and G [Waibel 89, Waibel 88a].
Many supervised connectionist models use gradient descent in error to solve various kinds of tasks (Rumelhart.
Even very simple animals exhibit a dazzling variety of complex behaviors which they continuously adapt to the changing circumstances of their environment.
Popular neuron models are based upon some statistical measure of known natural behavior.
The active positioning of sensory structures (i.
Combining a structural or knowledge-based approach for describing speech units with neural networks capable of automatically learning relations between acoustic properties and speech units is the research effort we are attempting.
1.
The "backpropagation algorithm" or generalized delta rule (Rumelhart, Hinton, & Williams, 1986) is sometimes criticized on the grounds that it is a "supervised" learning algorithm, which requires a "teacher" to provide correct outputs, and apparently leaves open the question of how the teacher learned the right answers.
The organizational structure and functioning of the sensory periphery in living beings has always been the subject of extensive research.
One thing that connectionist networks have in common with brains is that if you open them up and peer inside, all you can see is a big pile of goo.
A major thrust of research in our laboratory involves exploring the way in which the nervous system actively controls the acquisition of infonnation about the outside world.
Consider a network of binary linear threshold elements i, whose state Si is determined according to the rule Si = sign(L WijSj + Oi) = ±1 (1) .
In the last few years there has been a great resurgence of interest in neural network learning algorithms.
To rely on color as a cue in recognizing objects, a visual system must have at least approximate color constancy.
The olfactory system has a simple cortical intrinsic structure (Shepherd 1979), and thus is an ideal candidate to yield insight on the principles of sensory information processing.
Recent PDP research[Rumelhart et al .
Hidden Markov models (HMM) [Jelinek, 1976; Bourlard et al.
Learning algorithms are generally defined in terms of continuously-valued levels of input and output activity.
One of the current issues in the theory of supervised learning concerns the scaling properties of neural networks.
Sequential systems respond to variations in their input environment with sequences of activities.
In decision-making applications, it is desirable to have a network which computes the probabilities of deciding upon each of M possible propositions for any given input pattern.
Autonomous navigation has been a difficult problem for traditional vision and robotic techniques, primarily because of the noise and variability associated with real world scenes.
The minimization of objective functions is an attractive way to formulate and solve visual recognition problems.
We are investigating the potential of adaptive networks to learn categorization tasks and to model human performance.
The Winner-Take-All Network is a network which identifies the largest of N real numbers.
In recent years there has been a tremendous growth in the study of machines which learn.
A constraint satisfaction network, is a network whose units represent hypotheses, between which there are various constraints.
One reason for the recent surge in interest in neural networks is the development of the "back-propagation" algorithm for training neural networks.
Recently, there has been a dramatic increase in interest in exploring the computational properties of networks of parallel distributed processing elements (Rumelhart and McClelland, 1986) often referred to as Itneural networks" (Anderson, 1988).
Visual guidance of a multi-link arm through a cluttered workspace is known to be an extremely difficult computational problem.
Many neural network models have little resemblance to real neural structures, partly because the brain's higher functions, which they attempt to imitate, are not yet experimentally accessible.
Dynamics is the study of forces.
Signatures are used everyday to authorize the transfer of funds for millions of people.
The application which motivates this paper is image analysis; specifically the analysis of range images.
In sensory systems, the stimulus continuum is sampled at discrete points by receptors of finite tuning width d and inter-receptor spacing a.
Most neural network research on learning assumes the existence of a supervisor or teacher knowledgeable enough to supply desired, or target, network outputs during training.
Figure 1 shows internally-recorded transcardiac ECG signals for one patient.
References [1,2] have show the Tune-Delay Neural Network to be an effective classifier of acoustic phonetic speech from individual speakers.
Biological networks readily and easily process temporal information; artificial neural networks should do the same.
A key goal of machine vision is to recognize familiar objects in an unsegmented image, independent of their orientation, position, and scale.
It is well known that the percept ron algorithm can be used to find the appropriate parameters in a linear threshold device for pattern classification, provided the pattern vectors are linearly separable.
In recent years many researchers (Geman and Geman, 1984) (Marroquin et.
Although neural network learning systems are being widely investigated by many researchers via computer simulations, the graphical display of information in these simulations has received relatively little attention.
Synaptic modification is widely believed to be the brain's primary mechanism for long-term information storage.
The main point of this paper is to show that large back-propagation (BP) networks can be applied to real image-recognition problems without a large, complex preprocessing stage requiring detailed engineering.
In order to model biologically realistic neural systems, we will ultimately be seeking to construct networks with thousands of neurons and millions of interconnections.
Neural networks are typically circuits constructed from processing units which compute simple functions of the form f(Wl> .
Two amino acid sequences from proteins are homologous if they can be aligned so that many corresponding amino acids are identical or have similar chemical properties.
A large number of computer vision algorithms for finding intensity edges, computing motion, depth, and color, and recovering the 3-D shapes of objects have been developed within the framework of minimizing an associated "energy" functional.
Chunking is a process for generating, from a sequence of if-then rules, a more complex rule that accomplishes the same task in a single step.
High-speed simulation of large artificial neural nets has become an important tool for solving real world problems and for studying the dynamic behavior of large populations of interconnected processing elements [3, 2].
Recurrent nets are more powerful than feedforward nets because they allow simulation of dynamical systems.
In 1943 McCulloch and Pitts introduced the concept of two-state (binary) neurons as elementary building blocks for neural computation.
The digital simulation of connectionist learning algorithms is flexible and accurate.
Neural network modeling techniques have recently been used to predict and analyze the connectivity of biological neural circuits (Zipser and Andersen, 1988; Lehley and Sejnowski, 1988; Anastasio and Robinson, 1989).
The computational territory between the linearly summing McCulloch-Pitts neuron and the non-linear differential equations of Hodgkin & Huxley is relatively sparsely populated.
This paper describes some of our current efforts to integrate discriminant neural net classifiers into HMM speech recognizers.
Current automatic speech recognition systems rely almost exclusively on the acoustic speech signal, and as a consequence, these systems often perform poorly in noisy Combining Visual and Acoustic Speech Signals environments.
While an animal's external environment certainly plays an extremely important role in shaping its actions, the behavior of even simpler animals is by no means solely reactive.
Data compression has been one of the most important and active areas in information theory and computer science.
The research in Neural Networks has witnessed major changes in algorithm design focus, motivated by the limitations perceived in the algorithms available at the time.
The brain is a biological computer of immense complexity comprising highly specialized neurons and neural circuits.
A shortcoming of much recent neural network pattern classification research has been an overemphasis on back-propagation classifiers and a focus on classification error rate as the main measure of performance.
A common problem in motor learning is approximating a continuous function from samples of the function's inputs and outputs.
Although most mathematical theories of cortical function assume plasticity of individual cells, there is a strong debate in the biological community between "instructional" (plastic) and "selectional" (hard-wired) models of orientation-selective cells 125 126 Softky and Kammen (which we will call "simple cells") in striate visual cortex.
It is well known that system models which have too many parameters (with respect to the number of measurements) do not generalize well to new measurements.
Physiologists have long recognized that neurons may code information in their instantaneous firing rates.
This is a sketch of recent results stemming from work which is discussed completely in [1, 2, 3].
The catecholamines-norepinephrine and dopamine-are neuroactive substances that are presumed to modulate information processing in the brain, rather than to convey discrete sensory or motor signals.
The domain for our research was a speech recognition task that requires distinctions to be learned between recordings of four highl y confusable words: the names of the letters "B", "D", "E", and "V".
A large number of electrically modifiable synapses is often required for fully parallel analog neural network hardware.
Fault-tolerance is often cited as an inherent property of neural networks, and is thought by many to be a natural consequence of "massively parallel" computational architectures.
The use of Shannon's information theory ( Shannon and Weaver,1949) to the study of neural nets has been shown to be very instructive in explaining the formation of different receptive fi~lds in the early visual information processing, as evident by the works of Linsker (1986,1988).
40 - 60 Hz oscillations have long been reported in the rat and rabbit olfactory bulb and cortex on the basis of single- and multi-unit recordings as well as EEG activity (Freeman, 1972; Wilson & Bower 1990).
The search for a possible presence of some unspecified structure in a high dimensional space can be difficult due to the curse of dimensionality problem, namely the inherent sparsity of high dimensional spaces.
We are applying the trainable machine paradigm in our development of an image segmentation system to be used in real time image processing applications.
We have constructed an "artificial eye" (A-eye), an autonomous robot that incorporates a two axis camera positioning system (figure 1).
The self-organizing feature map algorithm developed by Kohonen [Kohonen, 1988] readily lends itself to the task of vector quantization for use in such areas as speech recognition.
Learning by Choice oflnternal Representations (CHIR) was recently introduced [1,11] as a training method for feed forward networks of binary units.
One can "learn" a three-dimensional object by exploring it and noticing how its appearance changes.
The Boltzmann machine learning procedure (Hinton and Sejnowski, 1986) can be made much more efficient by using a mean field approximation in which stochastic binary units are replaced by deterministic real-valued units (Peterson and Anderson, 1987).
A great deal of effort in experimental and theoretical neuroscience is devoted to recording and interpreting spatial patterns of neural activity.
It is generally admitted that generalization performance of back-propagation networks (Rumelhart, Hinton & Williams, 1986) will depend on the relative size ofthe training data and of the trained network.
Search problems which involve high dimensionality, a-priori constraints and nonlinearities are hard.
An obvious characteristic of the general behavior of cerebral cortex, as evident in EEG recordings, is its tendency to oscillate.
Determining the detailed anatomical structure of the nervous system has been a major focus of neurobiology ever since anatomical techniques for looking at the fine structure of individual neurons were developed more than 100 years ago (Ram6n y Cajal 1911).
A characteristic commonly found in biological systems is their ability to adapt their function based on their inputs.
Functional approximation of experimental data ongmating from a continuous dynamical process is an important problem.
We have performed a number of experiments with a 1000-word vocabulary continuous speech recognition task.
Training the time dependent behavior of a neural network model involves the minimization of a function that measures the difference between an actual trajectory and a desired trajectory.
In this brief account of the early days in neural network research, it is not possible to be comprehensive.
With the growing interest in the practical use of neural networks, addressing the problem of customiling networks for specific applications is becoming increasingly critical.
The auditory system of the barn owl constructs a map of sound direction in the external nucleus of the inferior colliculus (lex) after several stages of processing the output of the cochlea.
The strategy put forward in this research effort is to combine the flexibility and learning abilities of neural networks with as much knowledge from speech science as possible in order to build a speaker independent automatic speech recognition system.
Sensory systems receive information at extremely high rates, and much of this information must be processed in real time.
This paper describes a neural network algorithm, STOCHASM, that was developed for the purpose of real-time signal detection and classification.
In this paper we compare regression and classification systems.
Most successful applications of neural network learning to real-world problems have been achieved using highly structured networks of rather large size [for example (Waibel, 1989; Le Cun et al.
Neural networks have been applied to recognition tasks in many fields.
For several years research at the Department of Control Theory and Robotics at the Technical University of Darmstadt has been concerned with the design of a learning real-time control loop with neuron-like associative memories (LERNAS) A Self-organizing Associative Memory System for Control Applications for the control of unknown, nonlinear processes (Ersue, Tolle, 1988).
Backpropagation is a popular learning algorithm for multilayer neural networks which minimizes a global error function by gradient descent (Werbos, 1974: Parker, 1985; LeCun, 1985; Rumelhart, Hinton & Williams, 1986).
Interest in unsupervised learning has increased recently due to the application of more sophisticated mathematical tools (Linsker, 1988; Plumbley and Fallside, 1988; Sanger, 1989) and the success of several elegant simulations of large scale selforganization (Linsker, 1986; Kohonen, 1982).
Suppose that we desire to model as best as possible some unknown map 4> : u V, where U, V ~ One way we might go about doing this is to collect as many input-output samples {(9in, 90u d : 4>(9 in ) = 9 0u d as possible and "find" some function f : U - V such that a suitable distance metric d(f( z(t)), 4>(z(t)))I ZE {9 .
A wide variety of ptoblems can be solved by using the neural network framework [1].
The associative storage of binary vectors using discrete feedback neural nets has been demonstrated by Hopfield (1982).
Phonological phenomena can be quite complex, but human phonological behavior is also highly constrained.
It has often been suggested that one of the attractions of an adaptive neural network (NN) approach to pattern recognition is the availability of discrimination-based training (e.
The biggest promise of artifcial neural networks as computational tools lies in the hope that they will enable fast processing and synthesis of complex information patterns.
Early vision refers here to the first stages of visual perception of an experienced (adult human) observer.
This paper presents a minimization-based algorithm for training the dynamical behavior of a discrete-time neural network model.
Previously, we have reported on experiments using connectionist models for a small parsing task using a new network formalism which extends back-propagation to better fit the needs of sequential symbolic domains such as parsing (Jain, 1989).
The Percept ron algorithm was proved in the early 1960s[Rosenblatt,1962] to converge and yield a half space separating any set of linearly separable classified examples.
Many neural network models aim to understand how a particular process is accomplished by a unique network in the nervous system.
Converting perceptual and cognitive tasks into constrained optimization problems is a useful way of generating neural networks to solve those tasks.
The future success of neural networks depends on an ability to "scale-up" from small networks and low-dimensional toy problems to networks of thousands or millions of nodes and high-dimensional real-world problems.
Consider a robot placed in an unfamiliar environment The robot is allowed to wander around the environment, performing actions and sensing the resulting environmental states.
Plasticity of adult somatosensory cortical maps has been demonstrated experimentally in a variety of maps and species (Kass, et al.
Shortcomings of back-propagation (BP) in supervised learning has been well documented in the past {Soulie, 1987; Bernasconi, 1987}.
In their review of back-propagation in layered networks, Rumelhart et al.
Minimization of energy or error functions has proved to be a useful principle in the design and analysis of neural networks and neural algorithms.
For humans to properly interact with the environment using their arms.
"Clusteringn , "vector quantization", and "unsupervised learning" are all words which descn'be the same process: assigning a few exemplars to represent a large set of samples.
Scattering imposes limitations on the minImum feature sizes that can be reliably obtained with electron beam lithography.
Oja (1982) made the remarkable observation that a simple model neuron with an Hebbian adaptation rule develops into a filter for the first principal component of the input distribution.
In this paper, we study the dynamics of a class of neural delayed feedback models which have been used to understand equilibrium and oscillatory behavior in recurrent inhibitory circuits (Mackey and an der Heiden, 1984; Plant, 1981; Milton et al.
Multiplicative interactions in neural networks have been proposed (Pitts and McCulloch, 1947; Giles and Maxwell, 1987; McClelland et aI, 1988) both to explain biological neural functions and to provide invariances in pattern recognition.
An Artificial Neural Network (ANN) is trained to recognize a long/short pattern for a particular commodity future contract.
In recent years numerous models for the formation of ocular dominance columns (Malsburg, 1979; Swindale.
Long-term potentiation (LTP) is an experimentally observed form of synaptic plasticity that has been interpreted as an instance of a Hebbian modification (Kelso et al, 1986; Brown et al, 1990).
The goal of our ongoing research is to extend computational learning theory to include concepts that can change or evolve over time.
A Boltzmann machine ([AHS], [HS], [AK]) is a neural network model in which the units update their states according to a stochastic decision rule.
Function approximation on high-dimensional spaces is often thwarted by a lack of sufficient data to adequately "fill" the space, or lack of sufficient computational resources.
This paper reports on work performed on the application of artificial neural systems (ANS) techniques to the diagnosis and control of vehicle systems.
Reinforcement learning techniques have been applied successfully for simple control problems, such as the pole-cart problem [Barto 83, Michie 68, Rosen 88] where the goal was to maintain the pole in a quasistable region, but not at specific setpoints.
Previous trainable connectionist perception systems have often ignored important aspects of the form and content of available sensor data.
Early efforts in the area of training artificial neural networks have largely focused on the study of schemes for encoding nonlinear mapping characterized by timeindependent inputs and outputs.
When a scene is recorded from two or more different positions in space, e.
Connectionist approaches have enjoyed success, relative to competing frameworks, in accounting for context sensitivity and have become an attractive approach to NLP.
Although spoken letter recognition may seem like a modest goal because of the small vocabulary size, it is a most difficult task.
An S-input threshold gate is characterized by S real weights 'WI, ••.
The processing of pattern-sequences has been investigated with several neural network architectures.
Physical systems at a critical point exhibit long-range correlations even though the interactions among the constituent partides are of short range .
Recently, it has been shown that multi-layer feedforward neural networks, such as Multi-Layer Perceptrons (MLPs) , are theoretically capable of representing arbitrary mappings, provided that a sufficient number of units are included in the hidden layers (Hornik et aI.
It is becoming generally accepted that many behavioral and cognitive capabilities of the human brain must be understood as resulting from the cooperative activity of populations of nerve cells rather than the individual activity of any particular cell.
It is very important in practical situations to know how well a neural network will generalize from the examples it is trained on to the entire set of possible inputs.
In linguistics a grammar is an abstract formal system describing a language.
There are several minimization algorithms in use which in the the ith coordinate Xi in the direction S~+l Vf = ;:.
Hebb (1949) proposed a theory of inter-neuronal learning.
In supervised learning one has a system under study that responds to a set of simultaneous input signals {Xl'" x n }.
Back Propagation (Rwnelhart et al .
Temporal difference (TD) planning [6, 7] uses prediction for control.
Multilayer neural networks characterized by local interlayer connectivity and groups of nodes that are constrained to have the same weights on their input lines are often refered to as shared-weight networks.
In this paper we examine in a general sense the application of Minimum Description Length (MDL) techniques to the problem of selecting a good classifier from a large set of candidate models or hypotheses.
NETWORK We have previously demonstrated the use of a continuous Hopfield neural network as a K-Winner-Take-All (KWTA) network [Majani et al.
At a given time, an agent with a non-Markovian interface to its environment cannot derive an optimal next action by considering its current input only.
Networks of interacting oscillators provide an excellent model for numerous physical processes ranging from the behavior of magnetic materials to models of atmospheric dynamics to the activity of populations of neurons in a variety of cortical locations.
In a previous paper published in last years proceedings (Cowan & Friedman 1990) we outlined a new computational model for the development and regeneration of eye-brain maps.
Basic computational functions of associative neural structures may be analytically studied within the framework of attractor neural networks where static patterns are stored as stable fixed-points for the system's dynamics.
Under certain conditions the visual system is capable of performing extremely efficient signal processing [I].
A new technique is presented for representing recursive structures in connectionist networks.
For almost forty years, biophysically realistic modeling of neural systems has followed the path laid out by Hodgkin and Huxley (Hodgkin and Huxley, 1952).
A difficult problem for neural networks is to recognize objects independently of their position, orientation, or size.
A controller for a discrete-time dynamical system must provide, at time tn, a value un for the control variable.
We have been working on continuous speech recognition using moderately large vocabularies (1000 words) [1,2].
The huge computational requirements of neural networks and their natural parallelism have led to a number of interesting hardware innovations for executing such networks.
1.
We introduce here a "projection network" which is a new network for implementation of the "normal form projection algorithm" discussed in [Bai89, Bai90b].
We are concerned with the problem of the number of nodes needed in a feedforward neural network in order to represent a fUllction to within a specified accuracy.
Most current neural network architectures such as backpropagation require a cyclic presentation of the entire training set to converge.
A major criticism of unsupervised learning and control techniques such as those used by Barto et al.
During the past several years, researchers have explored the use of neural networks for classifying spectro-temporal speech patterns into phonemes or other sub-word units (e.
Expert systems that have neural networks for their knowledge bases are sometimes called neural expert system (Gallant & Hayashi.
Few studies have compared practical characteristics of adaptive pattern classifiers using real data.
The Radial Basis Functions (RBF) approach to approximating functions consists of modeling an input-output mapping as a linear combination of radially symmetric functions (Powell, 1987; Poggio and Girosi, 1990; Broomhead and Lowe, 1988; Moody and Darken, 1989).
Probably the most important issue in the study of supervised learning procedures is the issue of generalization, i.
In creating music, composers bring to bear a wealth of knowledge about musical conventions.
A basic problem in the analysis of large-scale neural network activity, is that one can never know the initial state of such activity, nor can one safely assume that synaptic weights are symmetric, or skew-symmetric.
ALCOVE is intended to accurately model human, perhaps non-optimal, performance in category learning.
Feedforward type neural network models constructed from empirical data have been found to display significant predictive power [6].
People can capably tell if a human face is male or female.
We have been investigating learning techniques for the control of robotic manipulators which utilize extensions of the CMAC neural network as developed by Albus 1022 Design and Implementation of a High Speed CMAC Neural Network (1972; 1975; 1979).
1 In previous papers (Poggio and Girosi, 1990a, 1990b) we have shown the equivalence between certain regularization techniques and a.
I expose a recurrent high-order back-propagation network to both positive and negative examples of boolean strings, and report that although the network does not find the minimal-description finite state automata for the languages (which is NP-Hard (Angluin, 1978», it does induction in a novel and interesting fashion, and searches through a hypothesis space which, theoretically, is not constrained to machines of finite state.
Classification of data into categories has been pursued by a number of research communities, viz.
Layered networks have attracted considerable interest in recent years due to their ability to model adaptively nonlinear multivariate functions.
The zero-crossings of the Laplacian of the Gaussian,V 2 G, are often used for detecting edges.
If back-propagation is used to train a single, multilayer network to perform different subtasks on different occasions, there will generally be strong interference effects which lead to slow learning and poor generalization.
Neural nets are often designed to optimize some objective function E of the current state of the system via a dissipative dynamical system that has a circuit-like implementation.
Most, "neural network" or "connectionist" models have evolved primarily as adaptive function approximators.
A dichotomy has arisen in recent years in the literature on nonlinear network learning rules between local approximation of functions and global approximation of functions.
There have been very few demonstrations ofthe application ofVLSI neural networks to real world problems.
Recent progress in network design demonstrates that non-linear feedforward neural networks can perform impressive pattern classification for a variety of real-world applications (e.
Feature selection and creation are two of the most important and difficult tasks in the field of pattern classification.
Cortical maps are functionally defined structures of the cortex, which are characterized by an ordered spatial distribution of functionally specialized cells along the cortical surface.
Due to the curse of dimensionality (Bellman, 1961) it is desirable to extract features from a high dimensional data space before attempting a classification.
Consider the class of learning algorithms which explore a space {W} of possible couplings looking for optimal values W· for which a cost function E(W) is minimal.
In this paper, we are concerned with developing neural nets with short term memory for processing of temporal patterns.
Many recent achievements in the connectionist area have been carried out by designing systems where different algorithms interact.
Signals of biological neurons are encoded in the firing patterns of spike trains or the time series of action potentials generated by neurons.
Dyna architectures (Sutton, 1990) use learning algorithms to approximate the conventional optimal control technique known as dynamic programming (DP) (Bellman, 1957; Bertsekas, 1987).
Recurrent neural networks have the capability of storing information in the state of their units.
In the salamander retina.
Many machine vision IYlteDll and, to a large extent, &lao the human visual Iyatem, are model bued.
Whenever decisions are to be made with respect to some events in the future, planning has been proved to be an important and powerful concept in problem solving.
One of the primary tasks assigned to neural networks is pattern classification.
A major problem with standard backpropagation algorithms for pattern recognition is that they seem to require carefully segmented and localized input patterns for training.
For supervised learning in neural networks, feedback connections are required so that the teacher signal on the output neurons can affect the learning in the network interior.
Speech is a complex phenomenon but it is useful to divide it into levels of representation.
Consider a task in which a subject is asked to name a word that rhymes with oast.
This paper illustrates how a recurrent back-propagation neural network algorithm (Rumelhart, Hinton & Williams, 1986) may be exploited as a procedure for controlling complex systems.
In this work we continue to develop the theme of comparing threshold and sigmoidal feedforward nets.
Learning Vector Quantization (LVQ) originated in the neural network community and was introduced by Kohonen (Kohonen [1986]).
Uniprocessor simulation of neural networks has been the norm, but benefiting from the parallelism in neural networks is impossible without specialized hardware.
A recent experimental study has revealed that spatial patterns of ocular dominance columns (ODes) observed by autoradiography and profiles of the ocular dominance histogram (ODH) obtained by electrophysiological experiments differ greatly between monkeys and cats.
One current aim of molecular biology is determination of the (3D) tertiary structures of proteins in their folded native state from their sequences of amino acid 523 524 Fredholm, Bohr, Bohr, Brunak, Cotterill, Lautrup, and Thtersen residues.
The Neural Prediction Model (NPM) is the speech recognition model based on pattern prediction by multilayer perceptrons (MLPs).
Competitive learning (Grossberg, 1976; Kohonen, 1982; Rumelhart & Zipser, 1985; von der Malsburg, 1973) is an unsupervised algorithm that classifies input patterns into mutually exclusive clusters.
Bell-shaped response curves are commonly found in biological neurons whenever a natural metric exist on the corresponding relevant stimulus variable (orientation, position in space, frequency, time delay, .
Information received at the sensory level is encoded in spike trains which are then transmitted to different parts of the brain where the main processing steps occur.
The visual systems of mammals and especially primates are capable of prodigious feats of movement.
Often one has some preconceived notions about how to perform some classification task.
Sequential adaptation is important for signal processing applications such as timeseries prediction and adaptive control in nonstationary environments.
A wide range of behavioral studies support a role for the neurotransmitter acetylcholine in memory function (Kopelman, 1986; Hagan and Morris, 1989).
Over the past decade, research in speech coding and synthesis has matured to the extent that speech can now be transmitted efficiently and generated with high intelligibility.
A key problem for a hardware implementation of neural nets is to find the proper network architecture.
Mutual inhibition in an array of neurons is a common feature of sensory systems including vision, olfaction, and audition in organisms ranging from invertebrates to man.
Three-dimensional image processing is an indispensable property for any advanced computer vision system.
Most proposed short-range intensity-based motion detection schemes fall into two major categories: gradient models and correlation models.
The structure and function of a biological network derives from both its evolutionary precursors and real-time learning.
Thus far, the neural networks research community has placed heavy emphasis on the problem of pattern classification.
We are concerned to emulate some aspects of perception.
The study of animals can provide a very important source of information for the design of automated artificial systems such as robots and autonomous vehicles.
Neural networks are proving to be useful for difficult tasks such as speech recognition, because they can easily be trained to compute smooth, nonlinear, nonparametric functions from any input space to output space.
1.
Learning procedures for connectionist networks are essentially statistical devices for performing inductive inference.
Currently, networks that perform function interpolation tend to fall into one of two categories: networks that use gradient descent for learning (e.
Many recent studies have shown that the generalization ability of a neural network (or any other 'learning machine') depends on a balance between the information in the training examples and the complexity of the network, see for instance [1,2,3].
Many tasks in society demand sustained attention to minimally varying stimuli over a long period of time.
Self-organizing feature maps like the Kohonen map (Kohonen, 1989, Ritter et al.
This paper is concerned with the problem of learning in networks where some or all of the functions involved are not smooth.
The task of efficiently approximating a function is central to the solution of many important problems in perception and cognition.
Neural approaches to music processing have been previously proposed (Lischka, 1989) and implemented (Mozer, 1991)(Todd, 1989).
The ability to reason from acquired knowledge is undoubtedly one of the basic and most important components of human intelligence.
We consider the prospects for applications of the TO('\) algorithm for delayed reinforcement learning, proposed in (Sutton, 1988), to complex real-world problems.
Retinal cones use both intracellular and extracellular mechanisms to adapt their gain to the input intensity level and hence remain sensitive over a large dynamic range.
Shallow neural networks are defined in [J ud90]; the definition effectively limits the depth of networks while allowing the width to grow arbitrarily, and it is used as a model of neurological tissue like cortex where neurons are arranged in arrays tens of millions of neurons wide but only tens of neurons deep.
Trajectory generation finds interesting applications in the field of robotics, automation, filtering, or time series prediction.
Early visual processing in cortical areas VI, V2 and MT appear to encode visual features in eye-centered coordinates.
The nature of information processing in complex dendritic trees has remained an open question since the origin of the neuron doctrine 100 years ago.
In many learning control problems, the evaluation used to modify (and thus improve) control may not be available in terms of the controller's output: instead, it may be in terms of a spatial transformation of the controller's output variables (in which case we shall term it as being "distal in space"), or it may be available only several time steps into the future (termed as being "distal in time").
Feedforward neural networks have been proposed as parametrized representations suitable for nonlinear regression.
Complex wave form recognition is generally considered to be a difficult task for machines.
The goal of our research is to better understand the problem of learning when concepts are allowed to change over time.
We are modeling known and proposed auditory structures in the brain using analog VLSI circuits, with the goal of making contributions both to engineering practice and biological understanding.
Linear regression attempts to approximate a target function by a model that is a linear combination of the input features.
In an age of increasing globalization of our economies and ever more efficient communication media.
This paper develops the results of Baum and Haussler [3] bounding the sample sizes required for reliable generalisation of a single output feedforward threshold network.
1.
One approach to analyzing neurobiological systems is to use simpler preparations that are amenable to techniques which can investigate the cellular, synaptic, and network levels of organization involved in the generation of behavior.
We have implemented a model of learning in neural networks using feedback connections and a local 1earning rule.
Analog hardware has obvious advantages in terms of its size, speed, cost, and power consumption.
NEURAL DYNAMICS Neural inputs and outputs are temporal, but there are no established ways to think about temporal learning and dynamical receptive fields.
Neural Networks have been used for many years to solve hard real world applications which involve large amounts of data.
Diagnostic radiology may be a very natural field of application for neural networks, since a simple answer is desired from a complex image, and the learning process that human experts undergo is to a large extent a supervised learning experience based on looking at large numbers of images with known interpretations.
The English alphabet is difficult to recognize automatically because many letters sound alike; e.
Neural networks can be evaluated based on their learning speed, the space and time complexity of the learned network, and generalisation performance.
Many recent papers have contributed to the understanding of recurrent networks and their potential for modelling sequential pbenomena (see for example Giles, Sun, Chen, Lee, & Chen, 1990; Elman, 1989; 1990; Jordan, 1986; Cleeremans, Servan-Schreiber & McClelland, 1989; Williams & Zipser, 1988).
Generalization from examples is central to the notion of cognition and intelligent behavior (Margolis, 1987).
Following Simon's (1969) observation that complex behaviour may simply be the reflection of a complex environment a number of researchers (eg.
Automatic language identification is the rapid automatic determination of the language being spoken, by any speaker, saying anything.
Since their first application in speech recognition systems in the late seventies, hidden Markov models have been established as a most useful tool.
Recently, there has been an upsurge of interest in s1ngie or few-neuron nonlinear dynamics (see e.
The generation of reaching movements toward unexpectedly displaced targets involves more complicated planning and control problems than in reaching toward stationary ones, since the planning of the trajectory modification must be performed before the initial plan is entirely completed.
Visual attention is perhaps best understood in the context of visual search, i.
Conventional feedforward neural-network controllers (Barto et aI.
To perform a voluntary hand movement, the primate nervous system must solve the following problems: (A) Which trajectory (hand path and velocity) should be used while moving the hand from the initial to the desired position.
In order to implement the back propagation learning procedure (Werbos, 1974; Parker, 1985; Rumelhart, Hinton and Williams, 1986), several issues must be addressed.
Backpropagation has shown considerable potential for addressing problems in natural language processing (NLP).
Neural network learning architectures such as the multilayer perceptron and adaptive radial basis function (RBF) networks are a natural nonlinear generalization of classical statistical techniques such as linear regression, logistic regression and additive modeling.
Hot rolling of steel slabs into fiat plates is a common process in a steel mill.
Recently synchronization phenomena in neural networks have attracted considerable attention.
Creating a connectionist pattern classifier that generalizes well to novel test data has recently focussed on the process of finding the network architecture with the minimum functional complexity necessary to model the training data accurately (see, for example, the works of Baum.
In a statistical approach to continuous speech recognition the desired quantity is the posterior probability p(Wrlxf, 8) of a word sequence Wr = Wl .
A commonly studied neural architecture is the feedforward network in which each unit of the network computes a nonlinear function g( x) of a weighted sum of its inputs x wtu.
During the development of the mammalian visual system, initially diffuse axonal inputs are refined to produce the precise and orderly projections seen in the adult.
The need for exploration in adaptive control has been recognized by various authors [MB89, Sut90, Mo090, Sch90, BB591].
Gradient descent is the bread-and-butter optimization technique in neural networks.
Many researchers have built neural-network chips, but few chips have been installed in board-level systems, even though this next level of integration provides insights and advantages that can't be attained on a chip testing station.
In machine learning, one very often knows more about the function to be learned than just the training data.
To achieve good generalisation in neural networks and other techniques for inferring a model from data, we aim to match the number of degrees of freedom of the model to that of the system generating the data.
The back-propagation learning (BPL) networks have been used extensively for essentially two distinct problem types, namely model-free regression and classification, 1159 1160 Hwang, Li, Maechler, Martin, and Schimert which have no a priori assumption about the unknown functions to be identified other than imposes a certain degree of smoothness.
The structure of the theory of learning differs from that of most other theories for applied problems.
Anatomical and neurophysiological studies suggest that there is a cortical pathway specialized for visual object recognition, beginning in the primary visual cortex and ending in the inferior temporal (IT) cortex (Ungerleider and Mishkin, 1982).
A commonly studied neural architecture is the feedforward network in which each unit of the network computes a nonlinear function g( x) of a weighted sum of its inputs x wtu.
Neural nets have been applied successfully to the classification of shapes, such as characters.
Intra-Cardia Defibrillators (lCDs) represent an important therapy for people with heart disease.
Constrained optimization methods are useful for setting the parameters of analog circuits.
The aim of this study was to investigate the dynamic properties of the inhibitory effect of type IT neurons on type IV neurons in the cat dorsal cochlear nucleus (DeN).
In the 1940s and 50s, several investigators realized that understanding the reliability of computation in the nervous system posed significant theoretical challenges.
Models of biological information processing often assume only one mode or state of operation.
The ability to sense motion in the visual world is essential to survival in animals.
The motivation for this work comes from the area of neural networks, where a linear threshold element is the basic processing element.
The robot forward kinematics function is a continuous mapping f : C ~ en -w ~ Xm which maps a set of n joint parameters from the configuration space, C, to the mdimensiona1 task space, W.
Let X and Y be random vectors taking values in R d and RP, respectively.
Several promising parallel analogue algorithms, which can be loosely described by the term "deterministic annealing" , or "mean field theory (MFT) annealing", have *also at Theoretical Division and Center for Nonlinear Studies, MSB213, Los Alamos National Laboratory, Los Alamos, NM 87545.
At NIPS-90 we introduced the gamma neural model, a real time neural net for temporal processing (de Vries and Principe, 1991).
Previous methods of motion detection have generally been based on one of two underlying approaches: correlation; and gradient-filter.
We address the problem of inducing languages from examples by considering a set of finite state languages over {O, 1}* that were selected for study by Tomita (Tomita, 1982): L1.
A neural network has been developed that can aid in establishing point-topoint communication routes through multistage interconnection networks (MINs) (Goudreau and Giles, 1991).
Given good bottom-up segmentation and normalization, feedforward neural networks are an efficient way to recognize digits in zip codes.
In order to deal with sequential signals, recurrent neural networks are often put forward as a useful model.
The use of artificial neural nets (ANNs) to control robot movement offers advantages in situations where the relevant analytic solutions are unknown, or where unforeseeable changes, perhaps as a result of damage or wear, are likely to occur.
The neural networks in our studies are quite simple.
Previous methods of motion detection have generally been based on one of two underlying approaches: correlation; and gradient-filter.
The limited capacity (7 ± 2) of the short term memory (STM) has been a subject of major interest in the psychological and physiological literature.
Biological neurons display a complexity rarely heeded in abstract network models.
The information capacity and the VC-dimension are two important quantities that characterize multilayer feedforward neural networks.
Standard pattern recognition systems usually involve a segmentation step prior to the recognition step.
This paper will concentrate on identifying types of time series for which a recurrent network provides a significantly better model, and corresponding prediction, than a feedforward network.
While a great deal of research has been done developing parsers for natural language, adequate solutions for some of the particular problems involved in spoken language have not been found.
A major problem in training artificial nellral network:> is to ellsure t.
Visual object recognition involves the identification of images of 3-D objects seen from arbitrary viewpoints.
For many reasons, there has been a long interest in "language" models of neural netwoIts; see [Elman 1991] for an excellent discussion.
Radial basis function (RBF) networks are 3-layer feed-forward networks in which each hidden unit a computes the function fa(x) = e- IIX-X",1I2 ,,2 , and the output units compute a weighted sum of these hidden-unit activations: N J*(x) =L cafa(x).
Research into the design of neural networks for process control has largely ignored existing knowledge about the task at hand.
Artificial neural networks (ANNs) have proven to be a powerful and general technique for machine learning [1, 11].
The empirical comparison of neural network training algorithms is of great value in the development of improved techniques and in algorithm selection for problem solving.
The nature of the vision problem dictates that neural networks for vision must make inferences from evidential information across levels of representational abstraction.
Most applications of domain independent learning algorithms have focussed on learning single tasks.
Recently, there has been much interest in neural architectures to solve the "blind separation of signals" problem (Herault & Jutten, 1986) (Vittoz & Arreguit, 1989).
A key problem in the formal study of human language is to understand the process by which linguistic intentions become speech.
= Let us suppose that each example in our in put data is a binary vector i {x I, .
We believe that neural networks are capable of more than pattern recognition; they can also perform higher cognitive tasks which are fundamentally rule-governed.
Self-organizing networks (Kohonen, Hertz, Domany) discover features or qualities about their input environment on their own; they learn without a teacher making explicit what is to be learned.
THE DECODING PROBLEM An error-correcting code (ECC) is used to identify and correct errors in a received binary vector which is possibly corrupted clue to transmission across a noisy channel.
We have recently begun to explore the properties of sensorimotor networks with architectures inspired by the anatomy and physiology of the cerebellum and its interconnections with the red nucleus and the motor cortex (Houk 1989; Houk et al.
G/SPLINES is a hybrid of Friedman's Multivariable Adaptive Regression Splines (MARS) algorithm (Friedman, 1990) with Holland's Genetic Algorithm (Holland, 1975).
Vector-matrix multiplication (VMM) is often used in neural network theories to describe the aggregation of signals by neurons.
Results of recent computational studies of visual recognition (e.
In previous work (Le Cun et al.
In this work we extend a recently suggested framework (Greenspan et al,1991) for a combined neural network and rule-based approach to pattern recognition.
Projection pursuit is a nonparametric statistical technique to find "interesting" low dimensional projections of high dimensional data sets.
New models are needed to test the closely linked neurophysiological and cognitive theories that are emerging from recent scientific studies of sleep and dreaming.
Cortical neurons consist of a highly branched dendritic tree that is electrically coupled to the soma.
This paper studies the integration of Artificial Neural Networks (ANN) with probability density functions (pdf) such as the Gaussian mixtures often used in continuous density Hidden Markov Models.
I show how systematically to derive optimizing neural networks that represent quantitative visual models and match them to data.
Winner-Take-All networks are fixed group of units which compete by mutual inhibition until the unit with the highest initial activation or input level suppresses the activation of all the others.
Classification based Neural Networks (NN) have been successfully applied to phoneme recognition tasks.
The task of visual recognition is natural and effortless for biological systems, yet the problem of recognition has been proven to be very difficult to analyze from a computational point of view.
The following methods for supervised sequence learning have been proposed: Simple recurrent nets [7][3], time-delay nets (e.
Until recently the fields of symbolic and connectionist learning evolved separately.
Learning in connectionist networks typically requires many training examples and relies more or less explicitly on some kind of syntactic preference bias such as "minimal architecture" (Rumelhart, 1988; Le Cun et ai.
The probably approximately correct (PAC) learning model proposed by Valiant [Valiant, 1984] provides a complexity theoretical basis for learning from examples produced by an arbitrary distribution.
In this paper we study a simple concept learning model in which the learner attempts to infer an unknown target concept I, chosen from a known concept class:F of {O, 1}valued functions over an input space X.
A neural network chip based on charge-coupled device (CCD) technology, the NNC2, is presented.
Patterns of 40 to 80 Hz oscillation have been observed in the large scale activity (local field potentials) of olfactory cortex [Freeman and Baird, 1987] and visual neocortex [Gray and Singer, 1987], and shown to predict the olfactory [Freeman and Baird, 1987] and visual pattern recognition responses of a trained animal.
Neural networks have been shown to be capable of learning the dynamical behaviour exhibited by chaotic time series composed of measurements of a single variable among many in a complex system [1, 2, 3].
Earlier work [Bourlard and Morgan, 1991l has shown the ability of Multilayer Perceptrons (MLPs) to estimate emission probabilities for Hidden Markov Models (HMM).
The lateral geniculate nucleus (LGN) in the thalamus is often considered as just a relay station on the way from the retina to visual cortex, since receptive field properties of neurons in the LGN are very similar to retinal ganglion cell receptive field properties.
Recently, artificial neural networks have found wide applications in many areas that require solutions to nonlinear problems.
One of the principal problems in instructing children is to develop sequences of examples that help children acquire useful concepts.
Bootstrapping is a strategy for estimating standard errors and confidence intervals for parameters when the form of the underlying distribution is unknown.
The representation and processing of data with complex structure in neural networks remains a challenge.
We consider efficient approximationsofa given multivariate function I: [-1, l]m-+ by feedforward neural networks.
Optimization of the weights of an ANN may be performed by, the application of a gradient descent teclmique.
In unsupervised network learning, the development of the connection weights is influenced by statistical properties of the ensemble of input vectors, rather than by the degree of mismatch between the network's output and some 'desired' output.
Our work has the intertwined goals of: (i) contributing to the improvement of the short-term electrical load (demand) forecasts used by electric utilities to buy and sell power and ensure that they can meet demand; 739 740 Yuan and Fine (ii) reducing the computational burden entailed in gradient-based training of neural networks and thereby enabling the exploration of architectures; (iii) improving prospects for good statistical generalization by use of rational methods for reducing complexity through the identification of good small subsets of variables drawn from a large set of candidate predictor variables (feature selection); (iv) benchmarking backpropagation and neural networks as an approach to the applied problem of load forecasting.
The encoder task described by Ackley, Hinton, and Sejnowski (1985) for the Boltzmann machine, and by Rumelhart, Hinton, and Williams (1986) for feed-forward networks.
Nonlocal interactions strongly influence the processing of visual motion information and the response characteristics of visual neurons.
Distance-based classification algorithms such as radial basis functions or K-nearest neighbors often rely on simple distances (such as Euclidean distance, Hamming distance, etc.
Inspired by biology.
Many tasks in natural language processing require access to semantic information about lexical items and text segments.
Bidden Markov models are used in most current state-of-the-art continuous-speech recognition systems.
Second-order recurrent networks have proven to be very powerful [8], especially when trained using complete back propagation through time [1, 6, 14].
Both experimental evidence and theoretical studies [1] link the generalization of a classifier to the error on the training examples and the capacity of the classifier.
We address the problem of writer independent recognition of hand-printed words from an 80,OOO-word English dictionary.
This paper demonstrates both by consjderatioll of the cost function and the learning equations, and by simulation experiments, that injection of random noise on to MLP weights during learning enhances fault-tolerance without additional supervision.
The recognition of spelled strings of letters is essential for all applications involving proper names, addresses or other large sets of special words which due to their sheer size can not be in the basic vocabulary of a recognizer.
A good candidate artificial neural network for short term memory needs to be: (i) easy to train, (ii) able to support a broad range of tasks in a domain of interest and (iii) simple to implement.
Unsupervised learning algorithms may perform useful preprocessing functions by preserving some aspects of their input while discarding others.
Principal Component Analysis (PCA) is an essential technique for data compression and feature extraction, and has been widely used in statistical data analysis, communication theory, pattern recognition and image processing.
Stochastic learning algorithms involve weight updates of the form w(n+1) = w(n) + /-l(n)H[w(n),x(n)] (1) where w E 7£m is the vector of m weights, /-l is the learning rate, H[.
Radial basis functions (RBFs) are a mel hod for approximating a function from scattered training points [Powell, H)87].
Two natural phenomena, the learning done by individuals' nervous systems and the evolution done by populations of individuals, have served as the basis of distinct classes of adaptive algorithms, neural networks (NNets) and Genetic Algorithms (GAs), resp.
It has previously been established that, while reaching out to grasp an object, the human hand preshapes according to the shape of the object and the planned manipulation (Jeannerod, 1984; Arbib et al.
For the most part, research on supervised learning has utilized a random input paradigm, in which the learner is both trained and tested on examples drawn at random from the same distribution.
Humans, and primates in general, are very good at complex motion processing tasks such as tracking a moving target against a moving background under varying luminance.
Attractor Neural Networks endowed with local inhibitory feedbacks, have been shown to have interesting computational performances[I].
The most general definition of Adaptive Control is one which includes any controller whose behavior changes in response to the controlled system's behavior.
Training a network to model a high dimensional input/output mapping with only a small amount of training data is only possible if the underlying map is of low complexity and the network, therefore, can be oflow complexity as well.
Associative Attractor Neural Network (ANN) models provide a theoretical background for the understanding of human memory processes.
Global planning of goal directed trajectories subject to cluttered spatio-temporal, state-dependent constraints - as in the kinodynamic path planning problem (Donald, 1989) considered here - is a difficult task, probably best suited for systems with embedded sequential behavior; theoretical insights indicate that the related problem of connectedness is of unbounded order (Minsky, 1969).
Generalization performance on a fixed-size training set is closely related to the number of free parameters in a network.
Parameter estimat.
Recent interest in the application of artificial neural networks [10, 11] has spurred research interest in the theoretical study of such networks.
For many real-world applications, a major constraint for the successful learning from examples is the limited number of examples available.
The ability to deal with missing or noisy features is vital in vision.
We are able to recognize a familiar object from many different viewpoints.
In this article we summarize a study on the effects of a boosting algorithm on the performance of an ensemble of neural networks used in optical character recognition problems.
In neural networks activity propagates through populations, or layers, of neurons.
In dealing with the temporal pattern classification or recognition, time warping of input signals is one of the difficult problems we often encounter.
Following the ideas of Gibson in the 1950's a number of studies in human psychophysics have demonstrated that optic flow can be used effectively for navigation in space (Rieger and Toet, 1985; Stone and Perrone, 1991; Warren et aI.
A few years ago, oscillatory and synchronous neuronal activity was discovered in cat visual cortex [1-3].
Neural network learning methods generalize from observed training data to new cases based on an inductive bias that is similar to smoothly interpolating between observed training points.
Recurrent neural networks of threshold elements have been intensively investigated in recent years.
The horizontal disparity in the projection of a 3-D point in a parallel stereo imaging system can be used to compute depth through triangulation.
A primary goal in neuroscience is to understand how the electrical properties of individual neurons contribute to the complex behavior of the networks in which they are found.
Hidden Markov Models (e.
Neural networks hold great promise in the area of speech recognition.
Originally presented in (Steinbuch 1961, Taylor 1964) the Hamming network (HN) has received renewed attention in recent years (Lippmann et.
Neural networks are usually trained from scratch, relying only on the training data for guidance.
This paper addresses the problem of modeling a discrete database.
This work is similar to [Anderson, Kerns 92], but represents two advances.
Pattern recognition is central to cognition.
The current state of the art in continuous speech recognition (CSR) is based on the use of hidden Markov models (HMM) to model phonemes in context.
Self-organizing networks are able to generate interesting low-dimensional representations of high-dimensional input data.
In many real world domains it is important to know the reliability of a network's response since a single network cannot be expected to accurately handle all the possible inputs.
Almost any neuron will respond to some manipulation or other by changing its firing rate, and this change in firing can convey information to downstream neurons.
Coronary artery disease (CAD) is one of the leading causes of death in the Western World.
We are designing a model that is consistent with both the data from single unit recording and the behavioural data that are relevant to spatial memory and navigation in the rat.
Nonlinearly parametrized representations of functions ¢: IR -+-IR of the form n (1.
The primate oculomotor system is capable of maintaining the image of an object on the fovea even when the head and object are moving simultaneously.
The deduction of neuronal connection structure from spike trains, including synaptic strength estimation, has long been one of the central issues for understanding the structure and function of the neuronal circuit and thus the information processing ·corresponding author 515 516 Shiono, Yamada, Nakashima, and Matsumoto mechanism at the neuronal circuitry level.
Detection of poorly defined waveshapes in a nonstationary high noise background is an important and difficult problem in signal processing.
One of the central topics in the field of neural networks is that of model selection.
The goal of supervised learning is the discovery of a compact representation that generalizes well .
In this work we apply a texture classification network to remote sensing image analysis.
The use of hints is coming to the surface in a number of research communities dealing with learning and adaptive systems.
Radial basis function (RBF) classifiers have been successfully applied to many pattern classification problems (Broomhead, 1988, Ng, 1991).
A central problem in machine learning and pattern recognition is to minimize the system complexity (description length, VC-dimension, etc.
It is known that the superior colliculus (SC) plays an important role in the control of eye movements (Schiller et a1.
Many control tasks of interest today involve controlling complex nonlinear systems under uncertainty and noise.
Within the last thirty years, a great deal of research has been carried out in an attempt to understand the development of cells in the pathways between the sensory apparatus and the cortex in mammals.
Several researchers have implemented comput.
1 The most popular method for neural network learning is back-propagation (Rumelhart, 1986) and related algorithms that calculate gradients based on detailed knowledge of the neural network model.
The robot forward kinematics function maps a vector of joint variables to the end-effector configuration space, or workspace, here assumed to be Euclidean.
As a non-parametric tool, spectrogram, or short-term Fourier transform, is widely used in analyzing non-stationary signals, such speech.
Selecting a model for a specified problem is the key to generalization based on the training data set.
The receptive field is perhaps the most useful concept for understanding neuronal information processing.
Certain species of freshwater tropical fish, known as weakly electric fish, possess an active electric sense that allows them to detect and discriminate objects in their environment using a self-generated electric field (Bullock and Heiligenberg, 1986).
Most part of studies on recurrent neural networks assume sufficient conditions of convergence.
Language inference and automata induction using recurrent neural networks has gained considerable interest in the recent years.
It has long been known that the hippocampal region (including the entorhinal cortex.
Reducing dimensionality of data with minimal information loss is important for feature extraction, compact coding and computational efficiency.
The performance of traditional OCR systems deteriorate very quickly when documents are degraded by noise, blur, and other forms of distortion.
Choosing the appropriate learning rate, or step size, in a gradient descent procedure such as backpropagation, is simultaneously one of the most crucial and expertintensive part of neural-network learning.
Straightforward reinforcement learning has been quite successful at some relatively complex tasks like playing backgammon (Tesauro, 1992).
There has been a good deal of theoretical work on calcula.
The original (simple) backpropagation algorithm, incorporating pattern for pattern learning and a constant learning rate 'T} E (0,00), remains in spite of many real (and 459 460 Finnoff imagined) deficiencies the most widely used network training algorithm, and a vast body of literature documents its general applicability and robustness.
A promising approach to understanding human perception is to try to model its developmental stages.
The leal'lI ing Cl1l've shows how well t hE' behavior of a neural network is improved as t.
Vector quantization is a form of data compression that represents data vectors by a smaller set of codebook vectors.
In a typical communication system, a sequence of symbols {ld are transmitted though a linear time-dispersive channel h(t).
In complex domains, learning needs to be biased with prior knowledge in order to produce satisfactory results from limited training data.
Several psychological phenomena show that the construction of organized and meaningful representations of the visual environment requires establishing separate representations (termed episodic representations) for the different objects viewed.
Function approximation is the problem of estimating a function from a set of examples of its independent variables and function value.
Our group has attempted to model speech production computationally as a process in which linguistic intentions are realized as speech through a causal succession of patterned behavior.
In recent years a lot of work has been put into the development of simulation systems for neural networks [1, 2, 3, 4, 5, 6, 7, 9, 10, 11, 12].
Activity-dependent accounts of the self-organization of the vertebrate brain have relied ubiquitously on correlational (mainly Hebbian) rules to drive synaptic learning.
The development of topographic and interdigitated mappings in the nervous system has been much studied experimentally, especially in the visual system (e.
Recent research on reinforcement learning has focused on algorithms based on the principles of Dynamic Programming.
Hebb hypothesized in 1949 that the basic information processing unit in the cortex is a cell-assembly which may include thousands of cells in a highly interconnected network[l].
The processing of pheromone odors in the antennallobe of several insect species relies on a number of response patterns of the antennallobe neurons in reaction to stimulation with pheromone components and blends.
When we look around our environment, we move our eyes to center and stabilize objects of interest onto our fovea.
The Singular Value Decomposition (SVD) is a method for writing an arbitrary nons quare matrix as the product of two orthogonal matrices and a diagonal matrix.
The topic of this paper is understanding how efficiently neural networks scale to large problems.
The goal of gaze tracking is to determine where a subject is looking from the appearance of the subject's eye.
The k-nearest neighbor algorithm (kNN, Dasarathy, 1991) is one of the most venerable algorithms in machine learning.
In the last few years, several researchers have tried to demonstrate how symbolic structures such as lists, trees, and stacks can be represented and manipulated in a connectionist system, while still preserving all the computational characteristics of connectionism (and extending them to the symbolic representations) (Hinton, 1990; Plate, 1991; Pollack, 1990; Smolensky, 1990; Touretzky, 1990).
Formal language learning (Gold, 1969) has been a topic of concern for cognitive science and artificial intelligence.
Feedforward neural networks are widely used as nonlinear, parametric models for the solution of classification tasks and function approximation.
Automated speech recognition is notoriously hard, and thus any predictive source of information and constraints that could be incorporated into a computer speech recognition system would be desirable.
Model-based object recognition solves the problem of invariant recognition by relying on stored prototypes at unit scale positioned at the origin of an object-centered coordinate system.
Natural handwriting is often a mixture of different "styles", lower case printed, upper case, and cursive.
It is well known that a given cortical neuron can respond with a different firing pat-tern for the same synaptic input, depending on its firing history and on the effects of modulator transmitters (see [Connors and Gutnick, 1990] for a review).
Automatic speaker recognition consists of having a machine recognize a person based on his or her voice.
In striate cortex, visual information coming from the retina (via the lateral geniculate nuclei) is processed to extract retinotopic maps of visual features.
The primary task of any perceptual system is to tell us about the external world.
Regularization (or weight-decay) methods are widely used in supervised learning by adding a regularization term t.
Clustering and related unsupervised learning techniques such as competitive learning and self-organizing maps have traditionally relied on measures of distance, like Euclidean or Mahalanobis distance, which are generic across most problem domains.
Every learning algorithm faces the problem of sparse data if the task to be learned is sufficiently nonlinear and high dimensional.
The objective of unsupervised learning is to identify patterns or features reflecting underlying regularities in data.
The spontaneous electrical activity of the brain was first observed by Caton in 1875.
The acquisition of spatial knowledge by exploration of an environment has been the subject of several recent experimental studies.
Recordings of local field potentials have revealed 40 to 80 Hz oscillation in vertebrate cortex [Freeman and Baird, 1987, Gray and Singer, 1987].
A vast number of important problems can be cast into a form of constraint satisfaction.
Linear neurons learning under an unsupervised Hebbian rule can learn to perform a linear statistical analysis ofthe input data.
Neural networks have been implemented previously on massively parallel supercomputers (Fujimoto et al.
Gentner and Markman (1992) suggested that the ability to deal with analogy will be a "Watershed or Waterloo" for connectionist models.
Many stationary dynamic systems exhibit significantly different behaviors under different operating conditions.
Feature sets can be more compact than the data they represent.
Recent studies have shown that learning in artificial neural networks can be understood as statistical parametric estimation using t.
The main objective of this research is the design of algorithms for empirical learning that generate networks suitable for digital implementations.
We address some aspects of the general problem of implementation and robustness of (mainly recurrent) autonomous discrete-time neural networks with continuous activation (herein referred to as analog networks) and discrete activation (herein, boolean networks).
A variety of methods to induce network architecture exist.
A Central Pattern Generator (CPG) is a network of neurons that generates rhythmic output in the absence of sensory input (Rowat and Selverston, 1991).
Neurons communicate by sequences of short pulses, the so-called action potentials or spikes.
Modeling speech articulator dynamics is important not only for speech science, but also for speech processing.
The aim of the project was to make a signature verification system based on the NCR 5990 Signature Capture Device (a pen-input tablet) and to use 80 bytes or less for signature feature storage in order that the features can be stored on the magnetic strip of a credit-card.
The task of drug activity prediction is to predict the activity of proposed drug compounds by learning from the observed activity of previously-synthesized drug compounds.
This paper addresses the question of how a human or autonomous robot can learn to classify new objects without experience with previous labeled examples.
THE M.
Natural stimuli are playing an increasingly important role in our understanding of sensory processing.
In neural network research, the back-propagation (BPP) algorithm is the most popular algorithm.
The approximation capabilities of feedforward neural networks with a single hidden layer has been studied by many authors, e.
There are a number of unsupervised Hebbian learning algorithms (see Oja, 1992 and references therein) that perform some version of the Karhunen-Loeve expansion.
Consider a system whose dynamics are described by a finite state Markov chain with transition matrix P, and suppose that at each time step, in addition to making a transition from state Xt = i to XHI = j with probability Pij, the system produces a randomly determined reward, rt+1! whose expected value is R;.
Spotting tasks require accurate detection of target patterns from a background of richly varied non-target inputs.
Methods for automatically acquiring the structure of the human language are attracting increasing attention .
A number of neural network studies have demonstrated the utility of the multi-layer perceptron (MLP) and shown it to be a highly effective paradigm.
One of the main problems for implement.
A central problem in information processing is the reduction of the data complexity with minimal loss in precision to discard noise and to reveal basic structure of data sets.
Reinforcement learning methods based on approximating dynamic programming (DP) are receiving increased attention due to their utility in forming reactive control policies for systems embedded in dynamic environments.
Exact gradient-descent algorithms for supervised learning in dynamic recurrent networks [1-3] are fairly complex and do not provide for a scalable implementation in a standard 2-D VLSI process.
In order to efficiently implement nonlinear optimization algorithms in analog VLSI hardware, maximum use should be made of the natural properties of the silicon medium.
The primary goal of learning in neural nets is to find a network that gives valid generalization.
Conductance-based neuron models, first formulated by Hodgkin and Huxley [10], are commonly used for describing biophysical mechanisms underlying neuronal behavior.
There are several physical cues to shape and depth which arise from changes in projection as a surface curves away from view, or recedes in perspective.
Plutowski &.
About half the pyramidal neurons in layer 5 of neocortex have long apical dendrites that arborize extensively in layers 1-3.
An adaptive system that has to interact with the external world is faced with the problem of coping with the time varying nature of real world signals.
The most common approach to large-vocabulary, talker-independent speech recognition has been statistical modelling with hidden Markov models (HMMs).
Learning in art.
Many well known neural network techniques for adaptive pattern classification and function approximation are inherently highly parallel, and thus have proven difficult to implement for real-time applications at a reasonable cost.
The U.
The system described here has been integrated into an address reading machine developed for the 'Remote Computer Reader' project of the United States Postal Service.
Many studies of artificial neural networks employ model neurons and synapses that are considerably simpler than their biological counterparts.
Both neurophysiological investigations [8] and lesioned human patients' data show that the Middle Temporal (MT) cortical area is crucial to perceiving three-dimensional shape in moving stimuli.
Selecting the optimal network architecture for a specific application is a nontrivial task, and several algorithms have been proposed to automate this process.
Honeybee foraging behavior is based on discrimination among complex odors which is the result of a memory process involving extraction and recall of "key-features" representative of the plant aroma (for a review see Masson et al.
Hidden Martov Models (HMM) represent the state-of-the-art for large-vocabulary continuous speech recognition (CSR).
In the sensory domain, many algorithms for unsupervised learning have been proposed.
Dynamic programming (DP) approaches can be utilized to determine optimal control policies for continuous stochastic dynamic systems when the state spaces of those systems have been quantized with a resolution suitable for control (Barto et al.
The fundamental theory of generalization favors simplicity.
In recent years there has been a growing interest in the study of cross-correlations between the activities of pairs of neurons in the cortex.
In this paper we discuss the problem of figure ground separation, via optical flow, for homogeneous images (textured images just provide more information for the disambiguation of figure-ground).
In the conventional Bayesian view ofbackpropagation (BP) (Buntine and Weigend, 1991; Nowlan and Hinton,1994; MacKay,I992; Wolpert, 1993), one starts with the "likelihood" conditional distribution P(training set = t I weight vector w) and the "prior" distribution P(w).
In supervised learning, a network is presented with a set of training exemplars [u(k), y(k)), k = 1 .
Many of the unsupervised learning algorithms that have been suggested for neural networks can be seen as variations on two basic methods: Principal Components Analysis (PCA) 3 4 Hinton and Zemel and Vector Quantization (VQ) which is also called clustering or competitive learning.
Neural networks are massively distributed computing systems.
The LMS algorithm was originally conceived as an approximate recursive procedure that solves the following problem (Widrow and Hoff, 1960): given a sequence of n x 1 input column vectors {hd, and a corresponding sequence of desired scalar responses {di }, find an estimate of an n x 1 column vector of weights w such that the sum of squared errors, L:~o Idi w1 2 , is minimized.
This paper describes a neurocomputer development system that uses a radial basis function as the transfer function of a neuron rather than the traditional sigmoid function.
The structure of the recurrent inhibitory connections from Renshaw cells (RCs) onto motoneurons (MNs) (Figure 1) suggests that the RC forms a simple negative feedback * send mail to: Mitchell G.
Recently, Ott, Grebogi and Yorke (OGY) [6] proposed a simple but very good idea.
With the advent of pen-based computers, the problem of automatically recognizing handwriting from the motions of a pen has gained much significance.
Feed-forward networks of localized (e.
Researchers often try to understand-post hoc-representations that emerge in the hidden layers of a neural net following training.
Go was developed three to four millenia ago in China; it is the oldest and one of the most popular board games in the world.
Most machine learning research treats the learner as a passive receptacle for data to be processed.
The retina is the first stage in visual information processing.
The barn owl, Tyto Alba, has a remarkable ability to localize sounds in space.
Due to merges, reorganizations and the need for cost reduction, airline companies need to improve the efficiency of their manpower by optimizing the activities of their crew pools as much as possible.
The genes of higher organisms are not continuous.
The field of reinforcement learning has grown dramatically over the past several years, but with the exception of backgammon [8, 2], has had few successful applications to large-scale, practical tasks.
The input intensity patterns received by the human visual system are typically complicated functions of the object surfaces and light sources in the world.
Over the past years, high-resolution images of the simultaneous representation of orientation selectivity and ocular dominance have been obtained in large areas of macaque striate cortex using optical techniques [3, 4, 5, 6, 12, 18].
Recordings of local field potentials have revealed 40 to 80 Hz oscillation in vertebrate cortex [Freeman and Baird, 1987, Gray and Singer, 1987].
The rate of convergence for gradient descent algorithms, both batch and stochastic, can be improved by including in the weight update a "momentum" term proportional to the previous weight update.
Assume that an object in a scene can be viewed as an instance of the model placed in space by some spatial transformation, and object recognition is achieved by discovering an instance of the model in the scene.
This paper deals with backpropagation networks trained to perform a classification task on Boolean or real-valued data.
Learning to predict the future and to find an optimal way of controlling it are the basic goals of learning systems that interact with their environment.
The fundamental features of music, derivable from frequency, time and amplitude dimensions of the physical signal, can be described in terms of two systems - pitch and timing.
Our goal is to understand the class of problems for which genetic algorithms (GA) are most suited, and in particular, for which they will outperform other search algorithms.
In reaching movements, trajectory formation is an ill-posed problem because the hand can move along an infinite number of possible trajectories from the starting to the target point.
The use of neural networks for computational tasks is based on the idea that the efficient way in which the nervous system handles memory and cognition is worth immitating.
The ability to deal with missing information is crucial in model-based vision systems.
Recurrent neural networks have been considered to learn to map input sequences to output sequences.
While the emerging technology of pen-computing is already available on the world's markets, there is an on growing gap between the state of the hardware and the quality of the available online handwriting recognition algorithms.
Memory based algorithms such as KNN or Parzen windows have been extensively used in pattern recognition.
Although neural network researchers are typically impressed with the performance achieved by their learning networks, it often remains a challenge to explain or even characterize such performance.
Artificial neural networks typically use an abstraction of real neuron behaviour, in which the continuously varying mean firing rate of the neuron is presumed to carry the information about the neuron's time-varying state of excitation [1].
Gesture recognition is an important aspect of human interaction, either interpersonally or in the context of man-machine interfaces.
Adaptive systems generally operate in environments t.
Extracellular electrodes typically record the activity of several neurons in the vicinity of the electrode tip (figure 1).
In the course of theoretical justification of many of the claims made about neural networks regarding their ability to learn a set of patterns and their ability to generalise, various concepts of maximal storage capacity were developed.
A major problem of pattern recognition and classification algorithms that learn from a training set of examples is to select the complexity of the model to be trained.
At about the age of three months, children become interested in tactile exploration of objects around them.
Model selection addresses "high level" decisions about how best to tune learning algorithm architectures for particular tasks.
The ability to learn from data with uncertain and missing information is a fundamental requirement for learning systems.
We regard training artificial neural networks as an unconstrained minimization problem N min f(x) := ~ h(x) xERn ~ (1) j=l where h : ~n --+ ~, j = 1, .
The motivation of this work has stemmed from the invention of a new functional transistor which simulates the behavior of biological neurons (Shibata and Ohmi, 1991; 1992a).
A three layer analogue VLSI perceptron has been previously developed by [Leong and Jabri, 1993].
The analysis of supervised learning or learning from examples is a major field of research within neural networks.
A classifier assigns vectors from Rn (n dimensional feature space) to one of K classes, partitioning the feature space into a set of K disjoint regions.
The original ARTI algorithm (Carpenter, 1987) proposed in 1987 is a massively parallel architecture for a self-organizing neural binary-pattern recognition machine.
In 1992 there were roughly 300,000 coronary artery bypass operations perfonned in the United States at a cost of roughly $44,000 per operation.
Neural networks are used for control systems because of their capability to approximate nonlinear system dynamics.
In the last few years artificial neural networks have been applied successfully to a variety of real-world problems.
Learning problems involving sequentially structured data cannot be effectively dealt with static models such as feedforward networks.
Unsupervised neural networks frequently employ sets of nodes or subnetworks with identical architecture and objective function.
The method presented in this paper is an instance of a strategy known as "predictive coding" or "model-based coding".
The human central nervous system (CNS) receives sensory inputs from a multitude of modalities, each tuned to extract different forms of information from the 1126 Zoubin Ghahramani, Daniel M.
A number of models of hippocampal function have been developed (Burgess et aI.
Throughout the last decades, the game of chess has been a major testbed for research on artificial intelligence and computer science.
Short-term memory for serially ordered lists of pronounceable stimuli is well described, at a crude level, by the idea of an 'articulatory loop' (AL).
Local algorithms such as K-nearest neighbor (NN) perform well in pattern recognition, even though they often assume the simplest distance on the pattern space.
Interacting physically with the world changes the dynamics of one's limbs.
The fields of computational neuroscience and artificial neural nets have enjoyed a mutually beneficial exchange of ideas.
There are many different possible schemes for converting hand gestures to speech.
There has been a growing interest in recent years in the use of neural models to investigate various brain pathologies and their cognitive and behavioral effects.
Reinforcement learning comprises a family of incremental planning algorithms that construct reactive controllers through real-world experimentation.
The barn owl relies primarily on sounds to localize prey [6] with an accuracy vastly superior to that of humans.
In systems that learn from examples, the traditional approach has been to study generalization from random examples, where each example is an input-output pair 288 Peter Sollich, David Saad with the input chosen randomly from some fixed distribution and the corresponding output provided by a teacher that one is trying to approximate.
The notion of an internal model, a system which mimics the behavior of a natural process, has emerged as an important theoretical concept in motor control (Jordan, 1995).
Statistical models of discrete time series have a wide range of applications, most notably to problems in speech recognition (Juang & Rabiner, 1991) and molecular biology (Baldi, Chauvin, Hunkapiller, & McClure, 1992).
The problem of understanding why a trained neural network makes a given decision has a long history in the field of connectionist modeling.
There has been much research in applying noise to neural networks to improve network performance.
One of the main areas of research within the Neural Networks community is the issue of learning and generalization.
Each side of the mammalian brain contains a structure called the lateral geniculate nucleus (LGN), which receives visual input from both eyes and sends projections to 134 Svilen Tzonev, Klaus Schulten, Joseph G.
Terrestrial animals, vertebrates and invertebrates, have developed very similar solutions for the problem of recognizing volatile substances [Vogt et ai.
An important task for natural and artificial vision systems is the analysis and interpretation of faces.
Grouping experimental data into compact clusters arises as a data analysis problem in psychology, linguistics, genetics and other experimental sciences.
The strong theory of convergence available for reinforcement learning algorithms (e.
Fusion of the nuclei of hydrogen provides the energy source which powers the sun.
Matching the representations of two images has long been the focus of much research in Computer Vision, forming an essential component of many machine-based ob1 E-mail address of authors: lastname-firstname@cs.
We propose a computational framework for understanding and modeling consciousness.
People in general, and computer professionals in particular, spend a huge amount of time typing.
The problem of missing data (incomplete feature vectors) is of great practical and theoretical interest.
The human visual system can recognize various 3D (three-dimensional) objects from their 2D (two-dimensional) retinal images although the images vary significantly as the viewpoint changes.
Within recent years, the field of computational learning theory has emerged to provide a rigorous framework for the design and analysis of learning algorithms.
Studies of single neurons or networks under the influence of noise have been a continuing item in neural network modelling.
While neural networks have proved a good tool for processing static patterns, classifying sequential information has remained a challenging task.
Reinforcement learning-the problem of getting an agent to learn to act from sparse, delayed rewards-has been advanced by techniques based on dynamic programming (DP).
The shift operator, defined as qx(t) ~ x(t + 1), is frequently used to provide time-domain signals to neural network models.
The knowledge representations learned by neural networks are usually difficult to understand because of the non-linear properties of these nets and the fact that knowledge is often distributed across many units.
The past few years have produced a number of efforts to design VLSI chips which "learn from experience.
Many unsupervised learning problems fall under the rubric of factorial learning-that is, the goal of the learning algorithm is to discover multiple independent causes, or factors, that can well characterize the observed data (Barlow, 1989; Redlich, 1993; Hinton and Zemel, 1994; Saund, 1995).
The Expectation-Maximization (EM) algorithm (Dempster, Laird & Rubin, 1977) is an iterative statistical technique for computing maximum likelihood parameter estimates from incomplete data.
In tasks where we use our hands to interact with a tool, our motor system develops a model of the dynamics of that tool and uses this model to control the coupled dynamics of our arm and the tool (Shadmehr and Mussa-Ivaldi 1994).
Auditory localization has been studied in many animals, particularly the barn owl.
Motivated by difficulties in analog VLSI implementation of back-propagation [Rumelhart et al.
Direction selectivity is the property of neurons that fire more strongly for one direction of motion of a bar (the preferred direction) than the other (nUll direction).
Perception may be viewed as the task of combining impoverished sensory input with stored world knowledge to predict aspects of the state of the world which are not directly sensed.
For the mixtures of experts architecture (Jacobs, Jordan, Nowlan & Hinton, 1991), the EM algorithm decouples the learning process in a manner that fits well with the modular structure and yields a considerably improved rate of convergence (Jordan & Jacobs, 1994).
For over thirty years measuring cancer outcome has been based on the TNM staging system (tumor size, number of lymph nodes with metastatic disease, and distant metastases) (Beahr et.
Unsupervised learning is the search for structure in data.
The temporo-parietal junction in the human cortex and its equivalent in monkeys, the inferior parietal lobule, are thought to playa critical role in spatial perception.
In recent years, the crucial issue of incorporating invariances into networks for pattern recognition has received increased attention, most especially due to the work of 666 Alessandro Sperduti, David G.
We describe an image processing system that uses convolutional neural networks to locate the position of a (moving) hand in a video frame, and to track the position of this hand across a sequence of video frames.
The wide applicability of neural networks to problems in pattern classification and signal processing has been due to the development of efficient gradient-descent algorithms for the supervised training of multilayer feedforward neural networks with differentiable node functions.
Several preprocessing and recognition approaches for on-line handwriting recognition have been developed during the past years .
Coarse-to-fine image search is a general technique for improving the efficiency of any search algorithm.
In the past few years several recurrent neural network architectures have emerged.
Much work on the theory of learning from examples has focused on batch learning, in which the learner is given all examples simultaneously, or is allowed to cycle through them repeatedly.
Recent research has shown that a continuous-density HMM (CD-HMM) system can outperform a more constrained tied-mixture HMM system for large-vocabulary continuous speech recognition (CSR) when a large amount of training data is available [2].
Generally, a pattern classifier consists of two main parts: a feature extractor and a classification algorithm.
Unsupervised learning algorithms based on information theoretic principles have tended to focus on linear decorrelation (Barlow & Foldiak 1989) or maximisation of signal-to-noise ratios assuming Gaussian sources (Linsker 1992).
Diagnosis is the process of identifying diseases in patients or disorders in machines by considering history, symptoms and other signs through examination.
The barn owl relies primarily on sounds to localize prey [6] with an accuracy vastly superior to that of humans.
Sometimes, a nonlinear approach is the simplest way to solve a linear problem.
Visual articulation is an important source of information in face to face speech perception.
Instead of modeling the global dependency between input x E ~D and output y E ~ using a single estimator, it is often very useful to decompose a complex mapping -'\.
Fusion of the nuclei of hydrogen provides the energy source which powers the sun.
A number offundamental results in computational learning theory [1, 2, 11] links the generalisation error achievable by a set of hypotheses with its Vapnik-Chervonenkis dimension (VC-dimension, for short) which is a sort of capacity measure.
One of the most important theories of feature extraction is the one proposed by Barlow (1989).
In general, the neural network hardware or VLSI has been preferred in respects of its relatively fast speed, huge network size and effective cost comparing to software simulation.
Previous algorithms for finding low complexity networks with high generalization capability are based on significant prior assumptions.
Studies of motor pattern generation have generally focussed on innate motor behaviors that are genetically preprogrammed and fine-tuned by adaptive mechanisms (Harris-Warrick et al.
The usual way of using a neural network for digit recognition is to train it to output one of the ten classes.
Understanding the response of neural nets to structural/functional damage is important for a variety of reasons, e.
For the purpose of understanding the self-organization mechanism of primary visual system, Linsker has proposed a multilayered unsupervised Hebbian learning network with random un correlated inputs and localized arborization of synapses between adjacent layers (Linsker, 1986 & 1988).
The mammalian visual system is very effective in detecting the orientations of lines and most neurons in primary visual cortex selectively respond to oriented lines and form orientation columns [1) .
When a neural network learns its target function from examples (training data), it knows nothing about the function except what it sees in the data.
The quest for smallest topology preserving maps motivated the introduction of growing feature maps like Fritzke's Growing Cell Structures (GCS).
Prefrontal cortex (PFC) is an area of the human brain which is significantly expanded relative to other animals.
It is well known that axons are neural processes specialized for transmitting information over relatively long distances in the nervous system.
Speech recognizers, optical character recognizers, and other types of pattern classifiers used for human interface applications often provide good performance for most users.
Vector quantization (VQ) [1] is a common ingredient in signal processing, for applications of pattern recognition and data compression in vision, speech and beyond.
My interest in auditory models and perceptual displays [2] is motivated by the problem of sound understanding, especially the separation of speech from noisy backgrounds and interfering speakers.
For over thirty years measuring cancer outcome has been based on the TNM staging system (tumor size, number of lymph nodes with metastatic disease, and distant metastases) (Beahr et.
This paper presents an important new element in our research on the problem of learning long-term dependencies in sequences.
In certain pattern recognition applications, particularly in remote-sensing and medical diagnosis, the standard assumption that the labelling of the data has been * and Division of Biology, California Institute of Technology 1086 Padhraic Smyth.
We are concerned in this paper with the application of multiple models, specifically the Hierarchical Mixtures of Experts, to time series prediction, specifically the problem of predicting acoustic vectors for use in speech coding.
Discovery of the mechanisms by which the mammalian cerebral cortex processes and stores information is the greatest remaining challenge in the brain sciences.
K-Means is a popular clustering algorithm used in many applications, including the initialization of more computationally expensive algorithms (Gaussian mixtures, Radial Basis Functions, Learning Vector Quantization and some Hidden Markov Models).
A basic attribute of perception is its ability to group elements of a perceived scene into coherent clusters (objects).
Standard network associative memories become more plausible as models of associative memory in the brain if they incorporate (1) partial connectivity, (2) sparse activity and (3) recall from noisy cues.
Speaker normalization methods are designed to minimize inter-speaker variations, one of the principal error sources in automatic speech recognition.
Moody and Darken proposed Networks with locally-tuned processing units, which are also known as Radial Basis Functions (RBFs, see [6]).
In learning machines such as neural networks, two major factors that affect the 'goodness of fit' of the examples are network size (complexity) and training time.
In unsupervised learning settings only input data is available but no information on the desired output.
Data collection for a classification or regression task is prone to random errors, e.
In recent years, considerable research has investigated the use of alternating minimization (AM) techniques in the supervised training of radial basis function networks.
A number of reinforcement learning algorithms based on the ideas of asynchronous dynamic programming and stochastic approximation have been developed recently for the solution of Markov Decision Problems.
In actor/critic learning systems, the actor implements a stochastic policy that maps states to action probability vectors, and the critic attempts to estimate the value of each state in order to provide more useful reinforcement feedback to the actor.
There has been recent interest in the use of space-variant sensors in active vision systems for tasks such as visual search and object tracking [14].
Reinforcement learning provides a sound framework for credit assignment in unknown stochastic dynamic environments.
In the past few years, a lot of work has been done on using neural networks for image compression, d .
While few biologists today would subscribe to Locke's description of the nascent mind as a tabula rasa, the nature of the inherent constraints - Kant's preknowl1 E-mail address of authors: lastname-firstname@cs.
Existing general-purpose recurrent algorithms are capable of rich dynamical behavior.
It is well known that a combination of many different predictors can improve predictions.
In this paper we consider the situation where data collection is costly, as when for example, real measurements or technical experiments have to be performed.
Global motion perception is critical to many visual tasks: to perceive self-motion, to identify objects in motion, to determine the structure of the environment, and to make judgements for safe navigation.
Many applications of neural networks can be formulated in terms of a multi-variate non-linear mapping from an input vector x to a target vector t.
For machines to accomplish classification tasks, such as speech and character recognition, appropriately handling deformed patterns is a key to achieving high performance [Simard 92] [Simard 93] [Hinton 92] [Barnard 91].
Many of the problems related to supervised learning can be boiled down to the question of balancing bias and variance.
There is much debate within the neuroscience community concerning the internal representation of movement, and current neurophysiological investigations are aimed at uncovering these representations.
Learning from examples is a common supervised learning paradigm that hypothe-sizes a target concept given a stream of training examples that describes the concept.
A number of studies have described neurons in the dorsal part of the medial superior temporal (MSTd) monkey cortex that respond best to large expanding/contracting, rotating, or shifting patterns (Tanaka et al.
Lateral connections in the primary visual cortex have a patterned structure that closely matches the response properties of cortical cells (Gilbert and Wiesel 1989; Malach et al.
You and I rarely learn things one at a time, yet we often ask our programs to-it must be easier to learn things one at a time than to learn many things at once.
To the present time, most ICDs have used timing information from ventricular leads only to classify rhythms which has meant some dangerous rhythms can not be distinguished from safe ones, limiting the use of the device.
The adaptive traffic control problem is to devise routing policies for controllers (i.
One of the ultimate goals subjacent to the development of intelligent agents is to have multiple agents collaborating in the achievement of tasks in the presence of hostile opponents.
Harmonic functions have been proposed as a uniform framework for the solution of several versions of the motion planning problem.
For all-parallel bottom-up recognition, allocating one separate unit for each possible feature combination, i.
Since the original publication of the "analog electronic cochlea" by Lyon and Mead in 1988 [I], several other analog VLSI models have been proposed which try to capture more of the details of the biological cochlear function [2],[3],[4].
In a series of papers in the early to mid 1980's, Hopfield and Tank introduced techniques which allowed one to solve combinatorial optimization problems with recurrent neural networks [Hopfield and Tank, 1985].
In a stochastic optimization problem , noisy samples are taken from a plant.
Human brains are by far superior to computers for solving hard problems like combinatorial optimization and image and speech recognition, although their basic building blocks are several orders of magnitude slower.
Supervised learning problems often have the following property: unlabeled examples have little or no cost while class labels have a high cost.
Owls are able to locate acoustic signals based on extraction of interaural time difference by coincidence detection [1, 2].
Artificial neural network models have been applied to character recognition with good results for small-set characters such as alphanumerics (Le Cun et aI.
According to current theories of spatial representations, the positions of objects are represented in multiple modules throughout the brain, each module being specialized for a particular sensorimotor transformation and using its own frame of reference.
The problem of learning associative networks from scalar reinforcement signals is notoriously difficult .
One of the strongest types of information that can be learned about an unknown process is the discovery of dependencies and -even more important- of independencies.
In a previous study of pairs of MT neurons recorded using a single extracellular electrode, it was found that the spike count during two seconds of visual motion stimulation had an average correlation coefficient of r = 0.
Distributing a learning task among a set of experts has become a popular method in computationallearning.
A number of learning algorithms have been developed to make synthetic neural machines be trainable to function in certain optimal ways.
Most sensory receptor cells produce analog voltages and currents which are smoothly related to analog signals in the outside world.
Consider a single artificial neuron with d inputs.
A cochlear implant is a device used to provide the sensation of sound to those who are profoundly deaf by means of electrical stimulation of residual auditory neurons.
Billions of dollars are daily pushed through the international capital markets while brokers shift their investments to more promising assets.
The ultimate goal in speech recognition is to determine the sequence of words that has been uttered.
Search for the largest (or the smallest) among a number of input data, Le.
Though neural networks have been shown as an effective solution for a diverse range of real-world problems, applications and especially hardware implementations have been few and slow to emerge.
The problem of designing a statistical classifier to minimize the probability of misclassification or a more general risk measure has been a topic of continuing interest since the 1950s.
An effective mathematical tool for multiresolution analysis [Kais94], the wavelet transform has found widespread use in various signal processing applications involving characteristic patterns that cover multiple scales of resolution, such as representations of speech and vision.
A hyper-ridge network is a simple perceptron with no hidden units and a ridge activation function.
Multilayer feedforward perceptrons (MLPs) are widely used in classification and regression applications due to their ability to learn a range of complicated maps [1] from examples.
The existence of significant delays in sensorimotor feedback pathways has led several researchers to suggest that the cerebellum might function as a forward model of the motor plant in order to predict the sensory consequences of motor commands before actual feedback is available; e.
Transparent motion can be studied using displays which contain two populations of moving dots.
One of the main criticisms of the Vapnik-Chervonenkis theory of learning [15] is that the results of the theory appear very loose when compared with empirical data.
The task of estimating the parameters of adaptive models such as artificial neural networks using Maximum Likelihood (ML) is well documented ego Geman, Bienenstock & Doursat (1992).
One of the most popular approaches to binary pattern classification, underlying many statistical techniques, is based on perceptrons or linear discriminants; see for instance the classical reference (Duda and Hart, 1973).
Vision has numerous uses in the natural world.
An ensemble is a collection of a (finite) number of neural networks or other types of predictors that are trained for the same task.
Learning from examples basically amounts to identifying the relations between random variables of interest.
Our everyday visual experience includes surface color constancy.
The design of artificial neural systems, in robotics applications and others, often leads to the problem of constructing a recurrent neural network capable of producing a particular trajectory, in the state space of its visible units.
Gaussian mixture models have recently attracted wide attention in the neural network community.
Hidden Markov models (HMMs) are now commonly used in off-line recognition of handwritten words (Chen et aI.
Many conventional approaches to density estimation, such as mixture models, rely on linear superpositions of basis functions to represent the data density.
The system we describe below was designed to process alphanumeric fields extracted from forms.
We consider a discrimination problem with J classes and N training observations .
Hand-written digit recognition has become one of the touchstone problems in neural networks recently.
The rapidly developing technology in the fields of robotics and virtual reality requires the development of new and more powerful interfaces for configuration and control of such devices.
We are working on pattern recognition problems using neural networks with a large number of parameters.
A key component of successful artificial neural network (ANN) applications is an input representation that suits the problem.
The capacity to judge one stimulus, object, or concept as similado another is thought to play a pivotal role in many cognitive processes, including generalization , recognition, categorization, and inference.
Recurrent neural networks have been rather popular in the physics community, because they lend themselves so naturally to analysis with tools from equilibrium statistical mechanics.
A basic problem of object recognition is that of matching- how to associate sensory data with the representation of a known object.
Many practical problems in computer vision, pattern recognition , robotics and other areas can be described in terms of constrained optimization .
A fundamental problem in mathematical systems theory is the identification of dynamical systems.
Recurrent Neural Networks (RNNs) are capable of representing arbitrary nonlinear dynamical systems [19, 20].
The smooth pursuit mechanism of primate visual systems is vital for stabilizing a region of the visual field on the retina.
Recurrent networks can be considered to be defmed by two components: a network architecture, and a learning rule.
All but a few learning algorithms employ one or more parameters that control the quality of learning.
When humans become drowsy, EEG scalp recordings of potential oscillations change dramatically in frequency, amplitude, and topographic distribution [3].
Reliable identification of low concentrations of airborne particles requires high speed monitoring of large volumes of air, and incurs heavy computational overheads.
For a large class of robotics tasks , such as assembly tasks or manipulation of relatively light-weight objects, under appropriate damping of the manipulator the dynamics of the objects can be neglected .
In previous papers Deco and Brauer (1994) and Parra, Deco, and Miesbach (1995) suggest volume conserving transformations and factorization as the key elements for a nonlinear version of Independent Component Analysis .
Learning manipulation in an unpredictable, changing environment is a complex task.
Human listeners improve their ability to recognize speech by identifying the accent of the speaker.
The appropriate segmentation and grouping of incoming sensory signals is important in enabling an organism to interact effectively with its environment (Llinas, 1991).
The appeal of probabilistic networks for knowledge representation, inference, and learning (Pearl, 1988) derives both from the sound Bayesian framework and from the explicit representation of dependencies among the network variables which allows ready incorporation of prior information into the design of the network.
Separating What from Where in EEG Source Analysis The joint problems of EEG source segregation, identification, and localization are very difficult, since the problem of determining brain electrical sources from potential patterns recorded on the scalp surface is mathematically underdetermined .
A prevalent feature of mappings in the brain is that they are often "topographic".
The temporal-difference (TD) algorithm (Sutton, 1988) for delayed reinforcement learning has been applied to a variety of tasks, such as robot navigation, board games, and biological modeling (Houk et al.
Training multilayer neural feed-forward networks, there is a folklore that the generalization error decreases in an early period of training, reaches the minimum and then increases as training goes on, while the training error monotonically decreases.
The passeriformes or songbirds make up more than half of all bird species and are divided into two groups: the os cines which learn their songs and sub-oscines which do not.
Neural networks are often used as bottom-up recognition devices that transform input vectors into representations of those vectors in one or more hidden layers.
Both the complexity of the brain (and concomittant difficulty encoding that C0111plexity through any direct genetic mapping).
Among the many virtues of neural networks are their efficiency, in terms of both execution time and required memory for storing a structure, and their practical ability to approximate complex functions.
The visual system computes multiple representations of the retinal image, such as motion, orientation, and stereopsis, as an early step in scene analysis.
The problem that we address here is the control of general nonlinear dynamic systems in the presence of uncertainties.
1.
Measurement of facial expressions is important for research and assessment psychiatry, neurology, and experimental psychology (Ekman, Huang, Sejnowski, & Hager, 1992), and has technological applications in consumer-friendly user interfaces, interactive video and entertainment rating.
When one trains a learning machine using a set of data given by the true system, its ability can be improved if one selects the training data actively.
Binary decision trees come in many flavours, but they all rely on splitting the set of k-dimensional data-points at each internal node into two disjoint sets.
Bayesian learning uses a prior on model parameters, combines this with information from a training set , and then integrates over the resulting posterior to make predictions.
In the past years the unsupervised learning schemes arose strong interest among researchers but for the time being a little is known about underlying learning mechanisms, as well as still less rigorous results like convergence theorems were obtained in this field.
In the field of off-line handwriting recognition, the goal is to read a handwritten document and produce a machine transcription.
This paper investigates a special class of laterally inhibited neural networks.
Many researchers have shown that simply combining the output of many classifiers can generate more accurate predictions than that of any of the individual classifiers (Clemen, 1989; Wolpert, 1992).
The VOR stabilizes images on the retina during rapid head motions: Rotations and translations of the head in three dimensions must be compensated by appropriate rotations of the eye.
We have designed analog VLSI velocity sensors invariant to absolute illuminance and stimulus contrast over large ranges that are able to achieve satisfactory performance in a wide variety of cases; yet such sensors, due to the intrinsic nature of analog processing, lack a high degree of precision in their output values.
Recent algorithmic and theoretical advances in reinforcement learning (RL) have attracted widespread interest.
Somehow, brains make very accurate models of the outside world from their raw sensory input.
Most current applications of neural network learning algorithms suffer from a large number of required training examples.
The primary cortical areas of mammals are now known to be plastic throughout life; reviewed recently by Kaas(1995).
Although neural networks are structured graphs, learning algorithms typically view them as a single vector of parameters to be optimized.
Genetic algorithms (GAs) are a class of randomized optimization heuristics based loosely on the biological paradigm of natural selection.
It has been observed previously that the initial layers of the vertebrate and invertebrate retina systems perform very similar processing functions on the incoming input signal[1].
Supervised learning of a probabilistic mapping between temporal sequences is an important goal of natural sequences analysis and classification with a broad range of applications such as handwriting and speech recognition, natural language processing and DNA analysis.
Supervised learning is concerned with approximating an unknown function based on examples.
Learning from examples implies discovering certain relations between variables of interest.
The idea of a neural net committee is to combine several neural net predictors to perform collective decision making, instead of using a single network (Perrone, 1993).
'Early stopping' is the following training procedure: Split the available data into a training set and a "validation" set.
Learning the parameters in a probabilistic neural network may be viewed as a problem in statistical estimation.
In the Bayesian approach to neural networks a prior distribution over the weights induces a prior distribution over functions.
Most studies of attention have focused on the selection process of incoming sensory cues (Posner et al.
We analyze the performance of cross validation 1 in the context of model selection and complexity regularization.
THE GAMMA FILTER Infinite Impulse Response (I1R) filters have a significant advantage over Finite Impulse Response (FIR) filters in signal processing: the length of the impulse response is uncoupled from the number of filter parameters.
One of the main applications of artificial neural networks is to pattern classification tasks.
A problem of fundamental interest to machine learning is time series modeling.
Traditional investment approaches (Elton and Gruber, 1991) assume that the return of a security can be described by a multifactor linear model: (1) where Hi denotes the return on security i, Fl are a set of factor values and Uil are security i exposure to factor I, ai is an intercept term (which under the CAPM framework is assumed to be equal to the risk free rate of return (Sharpe, 1984)) and ei is a random term with mean zero which is assumed to be uncorrelated across securities.
It has been argued (see [6]) that the main problem in machine learning is the biasing of a learner's hypothesis space sufficiently well to ensure good generalisation from a small number of examples.
Distributing a learning task among a set of experts has become a popular method in computationallearning.
Information theory is playing an increasing role in unsupervised learning and visual processing.
The HME (Jordan & Jacobs 1994) is a tree structured network whose terminal nodes are simple function approximators in the case of regression or classifiers in the case of classification.
Imagine an agent acting in some environment.
In systems that process sensory data there is frequently a post-classification stage where several independent class hypotheses are combined into the recognition of a more complex entity.
Proteins are long sequences of amino acids.
In this paper, we present a neural network-based algorithm to detect frontal views of faces in gray-scale images.
The problem of blind signal separation arises in many areas such as speech recognition, data communication, sensor signal processing, and medical science.
For many learning tasks , it is important to produce classifiers that are not only highly accurate, but also easily understood by humans.
The basic element of a neural network, a neuron, takes in a number of real-valued input variables and produces a real-valued output.
Over the last few years the Fourier Transform (FT) representation of boolean functions has been an instrumental tool in the computational learning theory community.
The problem of learning multiple modes in a complex nonlinear system is increasingly being studied by various researchers [2, 3, 4, 5, 6], The use of a mixture of local experts [5, 6], and a conditional mixture density network [3] have been developed to model various modes of a system.
The ABBOT hybrid connectionist-HMM system performed competitively with many conventional hidden Markov model (HMM) systems in the 1994 ARPA evaluations of speech recognition systems (Hochberg, Cook, Renals, Robinson & Schechtman 1995).
Human vision relies extensively on the ability to make saccadic eye movements.
An unexpected breakdown of an electric induction motor can cause financial loss significantly in excess of the cost of the motor.
Many different learning models have been developed in the literature.
In natural contexts, visual object recognition in humans is remarkably fast, reliable, and viewpoint invariant.
The neuromuscular junction (NMJ) of mammalian skeletal muscle is one of the most extensively studied areas of the nervous system.
Recent developments in complexity theory have addressed the question of complexity of computation over the real numbers.
An important application of artificial neural networks is to obtain accurate solutions to pattern classification problems.
In the Poisson neuron model, the output is a rate-modulated Poisson process; the time varying rate parameter ret) is an instantaneous function G[.
The synaptic connections in most models of the cortex can be defined as either associative or self-organizing on the basis of a single feature: the relative infl uence of modifiable synapses on post-synaptic activity during learning (figure 1).
We consider networks that consist of a finite set V of neurons, a set E ~ V x V of synapses, a weightwu,v ~ 0 and a response junctioncu,v : R+ -+ R for each synapse W.
Neural networks are well established in the domains of pattern recognition and function approximation, where their properties and training algorithms have been well studied.
In this paper we consider the representational and inductive capabilities of timedelay neural networks (TDNN) [Waibel et al.
Although binocular rivalry leads to distinct perceptual distress, it is revealing about the mechanisms of visual information processing.
Decision theory is normative or prescriptive and can tell us how to be rational and behave optimally in a situation [French, 1988].
Reinforcement learning (RL) is widely-used for learning closed-loop control policies.
Are Hopfield networks faster than conventional computers? This apparently straightforward question is complicated by the fact that conventional computers are universal computational devices, that is, they are capable of simulating any discrete computational device including Hopfield networks.
To make predictions based on a set of training data, fundamentally we need to combine our prior beliefs about possible predictive functions with the data at hand.
Visual motion estimation is an area where spatiotemporal computation is of fundamental importance.
Problems of sequential decision-making under uncertainty have been studied extensively using the methodology of dynamic programming [Bertsekas, 1995].
The number and type of stationary points of the error surface provide insight into the difficulties of finding the optimal parameters ofthe network, since the stationary points determine the degree of the system[l].
The problem of non-linear interpolation arises frequently in image, speech, and signal processing.
In a communication network in which traffic sources can be dynamically added or removed, an access controller must decide when to accept or reject a new traffic source based on whether, if added, acceptable service would be given to all carried sources.
Decision trees are regression or classification models that are based on a nested decomposition of the input space.
One of the most powerful and commonly used methods for training large layered neural networks is that of on-line learning, whereby the internal network parameters {J} are modified after the presentation of each training example so as to minimize the corresponding error.
We consider two styles of on-line learning.
Both the classic experiments of Rubel & Wiesel [8] on neurons in visual cortex, and several decades of theorising about feature detection in vision, have left open the question most succinctly phrased by Barlow "Why do we have edge detectors?" That is: are there any coding principles which would predict the formation of localised, oriented receptive 832 A.
Bayes methods have become popular as a consistent framework for regularization and model selection in the field of neural networks (see e.
We consider a formal model SNN for a §piking neuron network that is basically a reformulation of the spike response model (and of the leaky integrate and fire model) without using 6-functions (see [Maass, 1996a] or [Maass, 1996b] for further backgrou nd).
Hybrid HMM/ANN systems deal with the optimal combination of artificial neural networks (ANN) and hidden Markov models (HMM).
In order to quantitatively predict the performance of methods such as the ultra-fast RAMnet, which are not trained by minimising a cost function, we develop a Bayesian formalism for estimating the generalisation cost of a wide class of algorithms.
Data from the financial markets has recently been of much interest to the neural computing community.
In many stereo algorithms, the local correlation between images from the two eyes is used to estimate relative depth (Jain, Kasturi, & Schunk [5]).
The analysis of online (gradient descent) learning, which is one of the most common approaches to supervised learning found in the neural networks community, has recently been the focus of much attention [1].
Researchers in pattern recognition, statistics, and machine learning often draw a contrast between linear models and nonlinear models such as neural networks.
Over the last few years, we have developed and implemented several analog VLSI building blocks that allow us to model parts of the auditory pathway [1], [2], [3].
Most sensory and motor variables in the brain are encoded with coarse codes, i.
The problem we will investigate in this work is how to develop a classifier with both good generalization performance and efficiency in space and time in a supervised learning environment.
Recently, there has been a vigorous debate concerning the nature of neural coding (Rieke et al.
Visual speech recognition is a challenging task in sensory integration.
Vector quantization (VQ) and principal component analysis (peA) are two widely used unsupervised learning algorithms, based on two fundamentally different ways of encoding data.
The problem of constructing a predictor can generally be viewed as finding the right combination of bias and variance (Geman, Bienenstock, Doursat, 1992) to reduce the expected error.
Tracking a moving object on a cluttered background is a difficult task.
We study the problems of hetero-associative trammg, linear discriminant analysis, generalized eigen-decomposition and their theoretical connections.
Training guided by empirical risk minimization does not always minimize the expected risk.
Automatic processing of visual scenes often begins by detecting regions of an image with common values of simple local features, such as texture, and mapping the pattern offeature activation into a predicted region label.
Hebb conjectured that information processing in the brain is achieved through the collective action of groups of neurons, which he called cell assemblies (Hebb, 1949).
Discovering high order structure in patterns is one of the keys to performing complex recognition and discrimination tasks.
Optimal Brain Damage (OBD) was introduced by Le Cun et al.
Tangent distance is a well known technique used for transformation invariant pattern recognition.
Some of the most successful research in machine perception of complex natural image objects (like faces), has relied heavily on reduction strategies that encode an object as a set of values that span the principal component sub-space of the object 's images [Cottrell and Metcalfe, 1991, Pentland et al.
Vision is an active process, and one of the earliest, preattentive actions in visual processing is the identification of the salient contours in a scene.
The Support Vector method is a universal tool for solving multidimensional function estimation problems.
Graphical models (see, e.
The Self-Organizing Map (SOM) algorithm of Kohonen (1982) represents a form of unsupervised learning in which a set of unlabelled data vectors tn (n = 1, .
Hebb learning, one of the main mechanisms of synaptic strengthening, is induced by cooccurrent activity of pre- and post-synaptic neurons.
A reassuring theory of asymptotic convergence is available for many reinforcement learning (RL) algorithms.
A variety of unsupervised connectionist models containing discrete-valued hidden units have been developed.
In this paper, we describe an analog implementation of a common signal processing block in pattern recognition systems: a hidden Markov model (HMM) state decoder.
Analog noise is a serious issue in practical analog computation.
The dominant approaches within the statistical community, such as multiple linear regression but even extending to advanced techniques such as generalised additive models (Hastie and Tibshirani, 1990), projection pursuit regression (Friedman and Stuetzle, 1981), and classification and regreSSion trees (Breiman et al.
There have been many attempts to interpret neural networks from the information theoretical point of view [2], [4], [5].
Independent Component Analysis (ICA) (Comon, 1994; Jutten and Herault, 1991) is a signal processing technique whose goal is to express a set of random variables as linear combinations of statistically independent component variables.
Simple and complex cells were first described in visual cortex by Hubel and Wiesel [4].
In the following, lower case bold characters represent vectors and upper case bold characters represent matrices.
Learning and generalization are important areas of research within the field of neural networks .
Reinforcement learning and related grid-based dynamic programming techniques are increasingly being applied to dynamic systems with continuous valued state spaces.
The following work explores two learning algorithms - Least Mean Squared (LMS) [1] and Exponentiated Gradient Descent (EG) [2] - in the context of text-based Information Retrieval (IR) systems.
Most neural networks learn iteratively by gradient descent.
One of the major difficulties in creating any statistical pattern recognition system is that the statistics of the training set is often different from the statistics in actual use.
Cervical carcinoma is the second most common cancer in women worldwide, exceeded only by breast cancer (Ramanujam et al.
Watkins [1989] has defined optimal learning as:" the process of collecting and using information during learning in an optimal manner, so that the learner makes the best possible decisions at all stages of learning: learning itself is regarded as a mUltistage decision process, and learning is optimal if the learner adopts a strategy that will yield the highest possible return from actions over the whole course of learning.
We have been conducting research on bottom-up classification techniques ba<;ed on trainable artificial neural networks (ANNs), in combination with comprehensive but weakly-applied language models.
Transistor mismatches and parameter variations cause unavoidable nonuniformities from sensor to sensor.
Population codes, where information is represented in the activities of whole populations of units, are ubiquitous in the brain.
Ghahramani (1995) advocated the use of mean field methods as a means to avoid the heavy computation involved in the E step of the EM algorithm used for estimating parameters within a certain latent structure model, and Ghahramani & Jordan (1995) used the same ideas in a more complex situation.
In many pattern analysis or synthesis tasks, the observed data are generated from the interaction of two underlying factors which we will generically call "style" and "content.
Stochastic (on-line) learning can speed learning over its batch training particularly ,,,,hen data sets are large and contain redundant information [M0l93J.
Long range horizontal connections form a ubiquitous structural element of intracortical circuitry.
The VC-formalism and its extensions link the generalisation capabilities of a binary valued neural network with its counting function l , e.
Traditionally, very simple statistical techniques are used in the analysis of epidemiological studies.
One of the most distinctive features of striate cortex neurons is their combined selectivity for stimulus orientation and the direction of motion.
Inductive supervised learning methods have reached a high level of sophistication.
Self-organizing maps (SaM) introduced by [Kohonen 84] are a very popular tool used for visualization of high dimensional data spaces.
Kriging is an interpolation method widely used in the earth sciences, which models the surface to be interpolated as a stationary random field (RF) and employs a linear model.
The goal in supervised learning is to learn functions that map inputs to outputs with high predictive accuracy.
In regression problems it is important not only to predict the output variables but also to have some estimate of the error bars associated with those predictions.
Consider the general autoregressive model of a noisy time series with both process and additive observation noise: x(k) y(k) I(x(k - 1), .
The weakly electric fish, Eigenmannia, generates a quasi sinusoidal, dipole-like electric field at individually fixed frequencies (250 - 600 Hz) by discharging an electric organ located in its tail (see Bullock and Heilgenberg, 1986 for reviews).
The capacity of the auditory system to represent the auditory scene is restricted by the finite number of cells and by intrinsic noise.
The visual system must group local elements in its input into meaningful global features to infer the visual objects in the scene.
The visual system is concerned with the perception of objects in a dynamic world.
In recent years, there has been an explosion of interest in "active" machine learning systems.
Combining a set of learned models l to improve classification and regression estimates has been an area of much research in machine learning and neural networks [Wolpert, 1992, Merz, 1995, Perrone and Cooper, 1992, Breiman, 1992, Meir, 1995, Leblanc and Tibshirani, 1993, Krogh and Vedelsby, 1995, Tresp, 1995, Chan and Stolfo, 1995].
In recent years much work has been done on how the structure of the visual system reflects properties of the visual environment (Atick and Redlich 1992; Attneave 1954; Barlow 1989).
There is now considerable interest in using ensembles or committees of learning machines to improve the performance of the system over that of a single learning machine.
Controlled Diffusion Processes (CDP) are the analogy to Markov Decision Problems in continuous state space and continuous time.
Area MT neurons can show precise and rapid modulation in response to dynamic noise stimuli (Bair and Koch, 1996); however, computational models of these neurons and their inputs (Adelson and Bergen, 1985; Heeger, 1987; Grzywacz and Yuille, 1990; Emerson et al.
We recently demonstrated that the human visual system can process previously unseen natural images in under 150 ms (Thorpe et aI, 1996).
Support Vector Machines are known to give good results on pattern recognition problems despite the fact that they do not incorporate problem domain knowledge.
Bayesian belief networks (BBN) are a powerful formalism for representing and reasoning under uncertainty.
The problem of tracking curves in dense visual clutter is a challenging one.
It is an ongoing debate how information in the nervous system is encoded and carried between neurons.
This article is based on the technical report [3] about speeding up the OBS algorithm.
Given some cost function C(x) with local minima, we may search for the optimal x in many ways.
We report progress towards our long-term goal of developing low-cost, low-power, lowcomplexity analog-VLSI processors for real-time applications.
The training of artificial neural networks (ANNs) is usually a stochastic and unstable process.
Recently, a new neural optimization algorithm has emerged for solving quadratic assignment like problems [4, 2].
Analog VLSI implementation of neural networks, such as silicon retinas and adaptive filters, has been the focus of much active research.
Graph matching [6, 5, 7, 2, 3, 12, 11J is a topic of central importance in pattern perception.
A typical problem in experiments performed at high energy accelerators aimed at studying novel effects in the field of Elementary Particle Physics is that of preselecting interesting interactions at as early a stage as possible, in order to keep the data volume manageable.
Statistical classifier design is fundamentally a supervised learning problem, wherein a decision function, mapping an input feature vector to an output class label, is learned based on representative (feature,class label) training pairs.
The edge detection mechanism in the primate visual cortex (VI) involves at least two fairly well characterized circuits.
The number of hidden layers is a crucial parameter for the architecture of multilayer neural networks.
In the study of temporal difference (TD) learning in continuous time and space (Doya, 1996b), an optimal nonlinear feedback control law was derived using the gradient of the value function and the local linear model of the system dynamics.
The unsupervised assignment of elements of a given set to groups or clusters of like points, is the objective of cluster analysis.
Results from statistical learning theory give bounds on the number of training examples that are necessary for satisfactory generalization performance in classification problems, in terms of the Vapnik-Chervonenkis dimension of the class of functions used by the learning system (see, for example, [13, 5]).
It took great chutzpah for Gerald Tesauro to start wasting computer cycles on temporal difference learning in the game of Backgammon (Tesauro, 1992).
In this work we pursue a new family of models for time series, substantially extending, but strongly related to and based on the classic linear autoregressive moving average (ARMA) family.
The nematode C.
Certain classification problems, such as recognizing the digits of a hand written zipcode, require the assignment of each object to a class.
Since the formulation of the reconstruction theorem by Takens [10] it has been clear that a nonlinear predictor of a dynamical system may be directly derived from a systems time series.
Much of the current work on visual cortex modeling has focused on the generation of coding which captures statistical independence and sparseness (Bell and Sejnowski 1996, Olshausen and Field 1996).
The Hierarchical Mixtures of Experts (HME) architecture [2,3,4] has proven useful for classification and regression tasks in small to medium sized applications with convergence times several orders of magnitude lower than comparable neural networks such as the multi-layer perceptron.
To someone training a neural network by maximizing the likelihood of a finite amount of data it makes no sense to use a network with an infinite number of hidden units; the network will "overfit" the data and so will be expected to generalize poorly.
Policy iteration, a widely used algorithm for solving problems in adaptive control, consists of repeatedly iterating the following policy improvement computation (Bertsekas, 1995): (1) First, a value function is computed that represents the longterm expected reward that would be obtained by following an initial policy.
Consider a data set D consisting of N sequences, D = {SI,"" SN}' Si = (.
Tick-by-tick, high frequency foreign exchange rates are extremely noisy and volatile, but they are not simply pure random walks (Moody & Wu 1996).
Recently there has been considerable interest among neural network community in techniques that integrate the collective predictions of a set of networks[l, 2, 3, 4].
Artificial neural networks have been found effective in learning input-outputmappings from noisy examples.
It has been found empirically that adding some penalty term to an objective function in the learning of neural networks can lead to significant improvements in network generalization.
The problem of predicting the expected long-term future cost (or reward) of a stochastic dynamic system manifests itself in both time-series prediction and control.
Until a few years ago neural network and other statistical learning techniques were not very popular in computer vision domains.
Dempster, Laird and Rubin's EM (expectation and maximisation) [1] algorithm was originally introduced as a means of finding maximum likelihood solutions to problems posed in terms of incomplete data.
Recognition of specific objects, such as recognition of a particular face, can be based on representations that are object centered, such as 3D structural models.
Neural networks provide powerful tools to capture the structure in data by learning.
Recently there has been much interest in the theoretical breakthrough in the understanding of the on-line learning dynamics of multi-layer feedforward perceptrons (MLPs) using a statistical mechanics framework.
Suppose that we have available training data (Xl, Yd, .
Combining the predictions of a set of learned models! to improve classification and regression estimates has been an area of much research in machine learning and neural networks [Wolpert, 1992, Merz and Pazzani, 1997, Perrone, 1994, Breiman, 1996, Meir, 1995].
When using a magnetoencephalographic (MEG) record, as a research or clinical tool, the investigator may face a problem of extracting the essential features of the neuromagnetic • Corresponding author R.
This paper presents a neural-model of pre-attentive visual processing.
In SDMA (spatial division multiple access) the purpose is to separate radio signals of interfering users (either intentional or accidental) from each others on the basis of the spatial characteristics of the signals using smart antennas, array processing, and beamforming [5, 8).
The relative gradient was introduced by (Cardoso and Laheld, 1996) to design multiplicative updating algorithms with equivariant property for blind separation problems.
Whilst it has long been known both that the hippocampus of the rat is needed for normal performance on spatial tasks l3 , 11 and that certain cells in the hippocampus exhibit place-related firing,12 it has not been clear how place cells are actually used for navigation.
Multivariate probability density estimation is a fundamental problem in exploratory data analysis, statistical pattern recognition and machine learning.
It is known that auditory neurons are tuned for a number of independent feature parameters of simple stimuli including frequency (Merzenich et al.
The self-organizing map (SOM; for a review, see [1]) forms a topographic mapping from the data space onto a (usually two-dimensional) output space.
Q-Iearning is a popular reinforcement learning (RL) algorithm whose convergence is well demonstrated in the literature (Jaakkola et al.
Understanding neural processing will ultimately require observing the response patterns and interactions of large populations of neurons.
In many physiological dynamical systems measurements are acquired at irregular intervals.
From a technical point of view neural associative memories (N AM) provide data storage and retrieval.
One of the most common assumptions made in the study of machine learning is that the examples are drawn independently from some joint input-output distribution.
In our environment objects are constantly in motion, and the visual system faces the task of identifying the motion of objects.
Let X be an input space, P a distribution on X, F a class of functions mapping X into Y (called the "environment"), Q a distribution on F and (J' a function (J': Y X Y -t [0 , .
A remarkably wide range of human visual thresholds for spatial patterns appears to be determined by the earliest stages of visual processing, namely, orientation- and spatial frequency-tuned visual filters and their interactions [18, 19, 3, 22, 9].
The usual approach to regression problems is to assume that the data are generated by some stochastic mechanism and make some, typically very restrictive, assumptions about that stochastic mechanism.
The estimation of transformational geometry is key to many problems of computer vision and robotics [10] .
In many real-world tasks, the extraneous information in the input can be easily confused with the important features, making the specific task much more difficult.
We consider the general problem of learning multi-category classification from labeled examples.
Standard state-of-the-art speech recognition systems utilize Hidden Markov Models (HMMs) to model the acoustic behavior of basic speech units like phones or words.
One ofthe drawbacks of applying the supervised learning model is that it is not always possible for a teacher to provide labeled examples for training.
Much of our empirical understanding of the systems-level functioning of the brain has come from a procedure called extracellular recording.
Gabor filters are used as preprocessing stages for different tasks in machine vision and image processing.
The analysis of online gradient descent learning, as one of the most common forms of supervised learning, has recently stimulated a great deal of interest [1, 5, 7, 3].
The basic information-theoretic quantity for continuous one-dimensional random variables is differential entropy.
Since Gardner's pioneering work on the storage capacity of a single layer perceptron[1], there have been numerous efforts to use the statistical mechanics formulation to study feed-forward neural networks.
Neurons in macaque inferotemporal cortex (IT) have been shown to respond to views of complex objects,8 such as faces or body parts, even when the retinal image undergoes size changes over several octaves, is translated by several degrees of visual angle 7 or rotated in depth by a certain amount 9 (see [13] for a review).
In many simulation studies it has been shown that combining the outputs of several trained neural networks yields better results than relying on a single model.
When people study a picture, they can judge whether it depicts a shaded , 3dimensional surface, or simply a flat surface with markings or paint on it.
Image classification algorithms often rely on distance metrics which are too sensitive to variations in the imaging environment or set up (e.
Object recognition is one of the most important functions in human vision.
Amplitude patterns of synchronized "gamma band" (30 to 80 Hz) oscillation have been observed in ilie ensemble activity (local field potentials) of vertebrate olfactory, visual, auditory, motor, and somatosensory cortex, and in the retlna, thalamus, hippocampus, reticular formation, and EMG.
With the proliferation of inexpensive multimedia computers and peripheral equipment, video conferencing nally appears ready to enter the mainstream.
Boltzmann Machines (BMs) [1], are networks of binary neurons with a stochastic neuron dynamics, known as Glauber dynamics.
Unsupervised learning addresses the problem to detect structure inherent in unlabeled and unclassified data.
The learning algorithm developed in this paper is quite different from the traditional family of derivative-based descent methods used to train Multilayer Perceptrons (MLPs) for function approximation.
Hidden Markov models (HMMs) for automatic speech recognition[l] rely on high dimensional feature vectors to summarize the short-time, acoustic properties of speech.
When we are trying to extract regularities from data, we often have additional knowledge about functions that we estimate.
Srinivasan and Zhang (1993) describe behavioral evidence for two distinct movement detecting systems in bee: (1) A direction selective pathway with low frequency response characteristics serving the optomotor response and (2) A non-direction selective movement system with higher frequency response serving functions of obstacle avoidance and the 'tunnel centering' response where the animal seeks a flight path along the centerline of a narrow corridor.
Whilst it has long been known both that the hippocampus of the rat is needed for normal performance on spatial tasks l3 , 11 and that certain cells in the hippocampus exhibit place-related firing,12 it has not been clear how place cells are actually used for navigation.
Noisy vector quantization is an important lossy coding scheme for data to be transmitted over noisy communication lines.
During the early development of the visual system in for instance rats, fish and chickens, retinal axons grow across the surface of the optic tectum and establish connections so as to form an ordered map.
Tools from estimation and infonnation theory have recently been applied by researchers (Bialek et.
The goal of this research is the development of a system to learn musical rules from examples of J.
Human brains are by far superior to computers in solving hard problems like combinatorial optimization and image and speech recognition, although their basic building blocks are several orders of magnitude slower.
The asymptotic dynamics of stochastic on-line learning and it's dependence on the annealing schedule adopted for the learning coefficients have been studied for some time in the stochastic approximation literature [1, 2] and more recently in the neural network literature [3, 4, 5].
Applications of Gaussian mixture models regularly appear in the neural networks literature.
Learning algorithms for sequential data modeling are important in many applications such as natural language processing and time-series analysis, in which one has to learn a model from one or more sequences of training data.
Bayesian techniques have been successfully applied to neural networks in the context of both regression and classification problems (MacKay 1992; Neal 1996).
Reinforcement learning has the ability to solve general control problems because it learns behavior through trial-and-error interactions with a dynamic environment.
An important motivation for studying the statistics of natural images is its relevance for the modeling of the visual system.
The human visual system possesses the remarkable ability to recognize objects despite the presence of distractors and occluders in the field of view.
In this paper we address the problem of catastrophic fusion in automatic multimodal recognition systems.
Recently several learning rules that develop simple cell-like receptive fields in a natural image environment have been proposed (Law and Cooper, 1994; Olshausen and Field, 1996; Bell and Sejnowski, 1997).
A popular approach to systems identification, i.
Object recognition is one of the most important functions in human vision.
As multi-media (compressed Variable Bit Rate (VBR) video, data and voice) traffic is expected to be the main loading component in future communication networks, accurate modeling of the multi-media traffic is crucial to many important applications such as video-conferencing and video-on-demand.
Optimal decision making in virtually all spheres of human activity is rendered intractable by the complexity of the task environment.
In reinforcement learning, there is a tradeoff between spending time acting in the environment and spending time planning what actions are best.
Flies rely heavily on visual motion information to survive.
Decision trees are widely used for pattern classification [2, 7].
In many learning applications, the cost of information can be quite high, imposing a requirement that the learning algorithms glean as much usable information as possible with a minimum of data.
Most previous work in inductive learning has concentrated on learning to classify.
Complex cells in the primary visual cortex (area VI in primates) are tuned to localized visual patterns of a given spatial frequency, orientation, color, and drift direction (De Valois & De Valois, 1990).
Bayesian belief networks can be regarded as a fully probabilistic interpretation of feedforward neural networks.
An emerging use of reinforcement learning (RL) is to approximate optimal policies for large-scale control problems through extensive simulated control experience.
Support Vector (SV) Machines for pattern recognition, regression estimation and operator inversion exploit the idea of transforming into a high dimensional feature space where they perform a linear algorithm.
Regression tasks involve mapping a n-dimensional continuous input vector x E ~n onto a m-dimensional output vector y E ~m • They form a ubiquitous class of problems found in fields including process control, sensorimotor control, coordinate transformations, and various stages of information processing in biological nervous systems.
Classical and instrumental conditioning experiments study the way that animals learn about the causal texture of the world (Dickinson, 1980) and use this information to their advantage.
In psychological or computational research on perceptual categorization, it is generally taken for granted that the perceiver has a priori access to a representation of stimuli in terms of some perceptually meaningful features that can support the relevant classification.
Intrusion detection schemes can be classified into two categories: misuse and anomaly intrusion detection.
Modeling dynamical systems through a measured time series is commonly done by reconstructing the state space with time-delay coordinates [10].
In a regression problem, we have a sequence of n-dimensional real valued inputs Zt E R n , t 1, .
Visual perception involves not only an operating visual sensory system, but also the ability to control eye movements.
We consider a learning scenario in which a feed-forward neural network model (the student) emulates an unknown mapping (the teacher), given a set of training examples produced by the teacher.
The investigation of neural information structures in music is a rather new, exciting research area bringing together different disciplines such as computer science, mathematics, musicology and cognitive science.
Radial Basis Function networks are popular regression and classification tools[lO].
This paper presents a neural-model of pre-attentive visual processing.
Traditionally, imbuing machines with human-like knowledge has relied primarily on explicit coding of symbolic facts into computer data structures and algorithms.
Model estimation involves building mathematical representations of physical processes using measured data.
Modem computer architectures provide semantics of execution equivalent to sequential execution of instructions one at a time.
The response of neurons to repeated stimuli is intrinsically noisy.
Without supplementary information, there exists no way to directly measure the similarity between the content of images.
In recent years, neuroscientists and modelers have made great strides towards illuminating structure and computational properties in biological motor systems.
AdaBoost [4, 5] (for Adaptive Boosting) constructs a composite classifier by sequentially training classifiers, while putting more and more emphasis on certain patterns.
For nearly 50 years, memory researchers have known that our ability to remember specific past episodes depends critically on the hippocampus.
A fundamental feature of a good model is the ability to uncover and exploit independencies in the data it is presented with.
Dietterich and Bakiri (1995) suggested the following method, motivated by Error Correcting Coding Theory, for solving k class classification problems using binary classifiers.
Optical Character Recognition (OCR) of machine-print document images ·has matured considerably during the last decade.
We consider the discrimination problem with J{ classes and N training observations.
The rectified Gaussian distribution is a modification of the standard Gaussian in which the variables are constrained to be nonnegative.
The inverse of the Fisher information matrix is required to find the Cramer-Rae lower bound to analyze the performance of an unbiased estimator.
Several researchers have recently derived results in analog computation theory in the setting of discrete-time dynamical systems(Siegelmann, 1994; Maass & Opren, 1997; Moore, 1996; Casey, 1996).
Several recent papers in machine learning have emphasized the importance of priors and domain-specific knowledge.
Suppose that we have available training data (Xl, Yd, .
Factor analysis is a probabilistic model for real-valued data which assumes that the data is a linear combination of real-valued uncorrelated Gaussian sources (the factors).
Many natural signals, like pixel gray-levels, line orientations, object position, velocity and shape parameters, are well described as continuous-time continuous-valued stochastic processes; however, the neural network literature has seldom explored the continuous stochastic case.
Analysis of rainfall data is important for many agricultural, ecological and engineering activities.
We are exploring the use of nonparametric models in robot learning (Atkeson et al.
A partially observable Markov decision process (POMDP) is a generalization of the standard completely observable Markov decision process that allows imperfect information about the state of the system.
We introduce an estimator of test error that takes into consideration the test inputs.
The Recurrent Cascade Correlation (RCC) network was proposed by Fahlman (1991) to offer a fast and efficient alternative to fully connected recurrent networks.
Reinforcement learning can be performed for fully continuous problems by discretizing state space and time, and then performing a discrete algorithm like Q-Iearning or RTDP (e.
Cortical modules were observed in the somatosensory and visual cortices a few decades ago.
The neural determinants of visual perception can be probed by subjecting the visual system to ambiguous viewing conditions - stimulus configurations that admit more E.
Regression tasks involve mapping a n-dimensional continuous input vector x E ~n onto a m-dimensional output vector y E ~m • They form a ubiquitous class of problems found in fields including process control, sensorimotor control, coordinate transformations, and various stages of information processing in biological nervous systems.
Eye movements, muscle noise, heart signals , and line noise often produce large and distracting artifacts in EEG recordings.
We consider the classic task of predicting a binary (0/1) target variable zo, based on the values of some n other binary variables ZI ••• Zft,.
In the developing nervous system, growing axons are guided to targets that may be some distance away.
Sleep staging is usually based on rules defined by Rechtschaffen and Kales (see [8]).
Asset allocation and portfolio management deal with the distribution of capital to various investment opportunities like stocks, bonds, foreign exchanges and others.
In this paper we describe a multi-scale structure of natural images across many it can be used to recognize novel images, tasks is reasonably efficient, requiring no workstation.
Given a data set (Xi, Yi), i = 1,2, .
Consider an image, I, that is the result of a stochastic image-formation process.
Humans experience the three-dimensional world not as it is seen by either their left or right eye, but from a position of a virtual cyclopean eye, located in the middle between the two real eye positions.
EVENT RELATED POTENTIALS Ever since Hans Berger's discovery that the electrical activity of the brain can be measured and recorded via surface electrodes mounted on the scalp, there has been major interest in the relationship between such recordings and brain function.
The hardware implementation of neural networks is crucial in order to realize the real-time operation of neural functions such as spatiotemporal filtering, learning and constraint processings.
Knowledge about independence or conditional independence between variables is most helpful in ''understanding'' a domain.
Data clustering is one of the core methods for numerous tasks in pattern recognition, exploratory data analysis, computer vision, machine learning, data mining, and in many other related fields.
For more than a century, it has been know that humans will adapt their reaches to altered visual feedback [8].
This is a continuation of our efforts (Manwani and Koch, 1998) to understand the information capacity ofa neuronal link (in terms of the specific nature of neural "hardware") by a systematic study of information processing at different biophysical stages in a model of a single neuron.
Within the neural networks community, there has in the last few years been a good deal of excitement about the use of Gaussian processes as an alternative to feedforward networks [lJ.
The structure in a visual scene can be described at many levels of granularity.
There are many practical applications of statistical learning where it is useful to characterize data hierarchically.
One general problem when constructing statistical pattern recognition systems is to ensure the capability to generalize well, i.
Synaptic input to neurons in the primary visual cortex is primarily recurrent, arising from other cortical cells.
Hidden Markov models (HMMs) are generally considered to be the state of the art in speech recognition (e.
Associative mUltiplication is defined as multiplication done without recourse to computational algorithms, and as such is mainly concerned with recalling the basic times table.
Analytically solvable models of large recurrent neural networks are bound to be simplified representations of biological reality.
Research on Reinforcement Learning (RL) problem for partially observable environments is gaining more attention recently.
We propose and test a new in-sample cross-validation based method for optimizing the biasvariance tradeoff in 'soft classification' (Wahba et al1994), called ranG ACV (randomized Generalized Approximate Cross Validation) .
We describe a real-time computer vision and machine learning system for modeling and recognizing human behaviors in two different scenarios: (1) complex, twohanded action recognition in the martial art of Tai Chi and (2) detection and recognition of individual human behaviors and multiple-person interactions in a visual surveillance task.
In many neural systems , information regarding sensory inputs or (intended) motor outputs is found to be distributed throughout a localized pool of neurons.
Animation based on physical principles has been an influential trend in computer graphics for over a decade (see, e.
The principal cues for human sound localization arise from time and intensity differences between the signals received at the two ears.
We propose and test a new in-sample cross-validation based method for optimizing the biasvariance tradeoff in 'soft classification' (Wahba et al1994), called ranG ACV (randomized Generalized Approximate Cross Validation) .
In this paper we consider the popular class of nonlinear autoregressive processes driven by additive noise, which are defined by stochastic difference equations of form (1) where ft is an iid.
Current approaches to image matching for visual object recognition and image database retrieval often make use of simple image similarity metrics such as Euclidean distance or normalized correlation, which correspond to a templatematching approach to recognition [2, 5].
Rotated text is commonly used in a variety of situations, ranging from advertisements, logos, official post-office stamps, and headlines in magazines, to name a few.
Visually guided navigation of autonomous vehicles requires robust measures of self-motion in the environment.
A majority of problems in science and engineering have to be modeled in a probabilistic manner.
Graphical models provide a powerful framework for probabilistic inference[l] but suffer intractability when applied to large scale problems.
In spite of its importance, we had been unable to obtain VC-dimension values for practical types of networks, until fairly tight upper and lower bounds were obtained ([6], [8], [9], and [10]) for linear threshold element networks in which all elements perform a threshold function on weighted sum of inputs.
It has been proposed by Mumford[3] that the distribution of illusory contour shapes can be modeled by particles travelling with constant speed in directions given by Brownian motions.
Classifier systems (CSs) (Holland 1986) have been among the most used in reinforcement learning.
In a previous paper [8] we presented a coarse-to-fine hierarchical pyramid/neural network (HPNN) architecture that combines multi-scale image processing tech-Applications of Multi-Resolution Neural Networks to Mammography 939 niques with neural networks to search for small targets in images (see figure IA).
Image-based view synthesis is.
The promise of neural computation lies in exploiting the information processing abilities of simple computing elements organized into large networks.
The spiral problem refers to distinguishing between a connected single spiral and disconnected double spirals, as illustrated in Fig.
Real-time (heuristic) search methods are domain-independent control methods that interleave planning and plan execution.
Principal component analysis (PCA) is a widely used technique for data analysis.
Visual search is closely related to visual segmentation, and therefore can be used to diagnose the mechanisms of visual segmentation.
The ability to learn concepts from examples is one of the core capacities of human cognition.
Support Vector (SV) machines comprise a new class of learning algorithms, motivated by results of statistical learning theory (Vapnik, 1995).
In this contribution we consider an unsupervised clustering task.
Clustering algorithms can be divided into two categories: those that require a vectorial representation of the data, and those which use only pairwise representation.
There has been recent interest in studying the statistical properties of the visual world.
Wavelet regression is a technique which attempts to reduce noise in a sampled function corrupted with noise.
In this paper we introduce a new class of image models, which we call dynamic trees or DTs.
This paper concerns an application of the Mixture of Gaussians (MoG) probabilistic model (Titterington et aI.
In pattern recognition, convolution is an important tool because of its translation invariance properties.
The task of calculating every joint angle that would result in a specific hand position is called the inverse kinematics problem.
A fundamental problem faced by both biological and machine vision systems is the recognition of familiar objects and patterns in the presence of transfonnations such as translations, rotations and scaling.
This paper addresses sequential training of neural networks using powerful sampling techniques.
In many real-life situations, we are faced with the task of inducing the dynamics of a complex stochastic process from limited observations about its state over time.
We consider a system of N neurons O'(t) = {ai(t) = ±1}, which can change their states collectively at discrete times (parallel dynamics).
Learning in biological systems is of great importance.
There are at least two different approaches to learning in Markov decision processes: indirect approaches, which use control experience (observed transitions and payoffs) to estimate a model, and then apply dynamic programming to compute policies from the estimated model; and direct approaches such as Q-Iearning [2], which use control Convergence Rates for Q-Leaming and Indirect Algorithms 997 experience to directly learn policies (through value functions) without ever explicitly estimating a model.
Face recognition has developed into a major research area in pattern recognition and computer vision.
Speech, vision , text and biosequence data can be difficult to deal with in the context of simple statistical classification problems.
Advances in sensing devices have fueled the deployment of multiple sensors in several computational vision systems [1, for example].
COllective INtelligences (COINs) are large, sparsely connected recurrent neural networks, whose "neurons" are reinforcement learning (RL) algorithms.
Measuring ways by which several neurons in the brain participate in a specific computational task can shed light on fundamental neural information processing mechanisms .
In spite of its importance, we had been unable to obtain VC-dimension values for practical types of networks, until fairly tight upper and lower bounds were obtained ([6], [8], [9], and [10]) for linear threshold element networks in which all elements perform a threshold function on weighted sum of inputs.
A fundamental problem in neural network research is to find a suitable representation for the data.
Support Vector Machines (SVMs) are learning systems designed to automatically trade-off accuracy and complexity by minimizing an upper bound on the generalisation error provided by VC theory.
Over the last decade there has been a growing interest in the Bayesian approach to regression problems, using both neural networks and Gaussian process (GP) prediction, that is regression performed in function spaces when using a Gaussian random process as a prior.
In the last few years, there has been a surge of interest in Support Vector Machines (SVMs) [1].
Combinatorial optimization is of great importance in computer science, engineering, and operations research.
Fraud is costly to a network carrier both in terms of lost income and wasted capacity.
We introduce a model for wind fields based on Gaussian Processes (GPs) with 'constrained discontinuities'.
The recognition process involves both identification and familiarity discrimination.
Agents which operate in the real world, such as mobile robots, must use sensors which at best give only partial information about the state of the environment.
Conditional densities have played an important role in statistics and their merits over joint density models have been debated.
A macro-action is a sequence of actions chosen from the primitive actions of the problem.
When reaching to an object, our brain transforms a visual stimulus on the retina into a finely coordinated motor act.
Several models have been proposed in the past to explain the activity depending development of ocular dominance (00) in the visual cortex.
Nonlinear state space models (NSSM) are a general framework for representing nonlinear time series.
Principal Components Analysis (PCA) is a widely used statistical technique for representing data with a large number of variables [1].
Variations in speaking rate currently present a serious challenge for automatic speech recognition (ASR) (Siegler & Stern, 1995).
Although high-level code is generally written as if it were going to be executed sequentially, many modern computers are pipelined and allow for the simultaneous issue of multiple instructions.
One of the strengths of Support Vector (SV) machines is that they are nonparametric techniques, where one does not have to e.
Over the past decade learning from data has become a highly active field of research distributed over many disciplines like pattern recognition, neural computation , statistics, machine learning, and data mining.
At the NIPS conference in 1996 [1], we introduced an algorithm for classifying acoustic transient signals using template correlation.
Many learning algorithms for pattern classification minimize some cost function of the training data, with the aim of minimizing error (the probability of misclassifying an example).
Hebbian modification of network interconnections plays a central role in the study of learning in neural networks (Rumelhart and McClelland, 1986; Hertz et al.
Bayesian probability theory gives a powerful framework for visual perception (Knill and Richards 1996).
In supervised learning, we have a set of explicative variables x from which we wish to predict a response variable y.
Pattern classification is one of most fundamental and important tasks, and a k-NN rule is applicable to a wide range of classification problems.
Both analog and digital implementations of neural networks have been reported.
This paper studies one of the fundamental puzzles in brain development: the massive synaptic pruning observed in mammals during childhood , removing more than half of the synapses until puberty (see [1] for review) .
Independent component analysis (ICA) (Comon, 1994; Jutten and Herault, 1991) has recently received much attention as a signal processing method which has been successfully applied to blind source separation and feature extraction.
Belief networks [1] are diagrammatic representations of joint probability distributions over a set of variables.
The concept of an internal model, a system for predicting behavior of a controlled process, is central to the current theories of motor control (Wolpert et al.
Nonparametric Bayesian models which are based on Gaussian priors on function spaces are becoming increasingly popular in the Neural Computation Community (see e.
In the field of unsupervised learning several information-theoretic objective functions (OFs) have been proposed to evaluate the quality of sensory codes.
This paper examines probabilistic modeling techniques for discriminating between five face orientations: left profile, left semi-profile, frontal, right semi-profile, and right profile.
Most learning rules are formulated in terms of mean firing rates, viz.
Many neuromorphic chips which mimic the analog and parallel characteristics of visual, auditory and cortical neural circuits have been designed [Mead, 1989; Koch, 1995] .
Many problems of neural networks, such as learning and pattern recognition, can be cast into a framework of statistical estimation problem.
Time series are often encoded by first dividing the signal into a sequence of blocks.
The paper presents a probabilistic framework for estimation (perception) and classification of complex time-varying signals, represented as temporal streams of states.
Radial basis functions (RBF) have been extensively used for problems in discrimination and regression.
An intriguing hypothesis for how the brain represents incoming sensory information holds that it constructs a hierarchical probabilistic model of the observed data.
Cells in the primary visual cortex have to encode a wide range of contrast levels, and they still need to be sensitive to small changes in the input intensities.
Simple connectionist models can fall prey to the "binding problem" .
Principal Components Analysis (PCA) is a widely used statistical technique for representing data with a large number of variables [1].
Simultaneous recording of activity from many neurons can greatly expand our understanding of how information is coded in neural systems[l].
Reinforcement learning (RL) methods (Barto et al.
Clustering and visualization are key issues in exploratory data analysis and are fundamental principles of many unsupervised learning schemes.
An event in High Energy Physics (HEP) is the experimental result of an interaction during the collision of particles in an accelerator .
Much progress has been made in solving the dynamics of supervised learning in layered neural networks, using the strategy of statistical mechanics: by deriving closed laws for the evolution of suitably chosen macroscopic observables (order parameters) in the limit of an infinite system size [1, 2, 3, 4].
Much progress has been made in solving the dynamics of supervised learning in layered neural networks, using the strategy of statistical mechanics: by deriving closed laws for the evolution of suitably chosen macroscopic observables (order parameters) in the limit of an infinite system size [1, 2, 3, 4].
The graph isomorphism problem is one of those few combinatorial optimization problems which still resist any computational complexity characterization [6].
Shawe-Taylor [8] demonstrated that the output margin can also be used as an estimate of the confidence with which a particular classification is made.
Scalp-recorded event-related potentials (ERPs) are voltage changes in the ongoing electroencephalogram (EEG) that are both time- and phase-locked to some experimental events.
In most areas of pattern recognition, machine learning, and neural computation it has become common practice to represent data as feature vectors in a Euclidean vector space.
Several t.
The Boundary Contour System (BCS) and Feature Contour System (FCS) combine models for processes of image segmentation, feature filling, and surface reconstruction in biological vision systems [1 ],[2].
Given the multitude of contexts within which we must act, there are two qualitatively distinct strategies to motor control and learning.
In order to approximate the value function (VF) of a continuous state-space and time reinforcement learning (RL) problem, we define a particular class of functions called the barycentric interpolator, that use some interpolation process based on finite sets of points.
Parti-game operates on goal problems by dynamically partitioning the space into hyperrectangular cells of varying sizes, represented using a k-d tree data structure.
Mixture density models, in particular normal mixtures, have been extensively used in the field of statistical pattern recognition [1].
Mixture models, in particular mixtures of Gaussians, have been a popular tool for density estimation, clustering, and un-supervised learning with a wide range of applications (see for instance [5, 2] and the references therein).
Many problems in vision, such as the detection of edges and object boundaries in noise/clutter, see figure (1), require the use of search algorithms .
Consider the classical Perceptron algorithm.
Hastie [2] and Hastie and Stuetzle [3] (hereafter HS) generalized the self consistency property of principal components and introduced the notion of principal curves.
What happens when we voluntarily focus our attention to a restricted part of our visual field? Focal attention is often thought as a gating mechanism, which selectively allows a certain spatial location and and certain types of visual features to reach higher visual processes.
Visualisation is a powerful tool in the exploratory analysis of multivariate data.
The W-S algorithm[5] is a simple Hebbian learning algorithm.
Implantable cardioverter defibrillators (ICDs) are devices used to monitor the electrical activity of the heart muscle and to apply appropriate levels of electrical stimulation if abnonnal conditions are detected.
In this work we propose a method for semi-supervised support vector machines (S3VM).
Reinforcement learning techniques have proven quite effective in solving Markov Decision Processes ("MOPs), control problems in which the exact state of the environment is available to the learner and the expected result of an action depends only on the present state [10].
The task in text retrieval is to find the subset of a collection of documents relevant to a user's information request, usually expressed as a set of words.
A fairly large literature (see [Omlin, Giles, 1996] and the references therein) is devoted to the construction of analog neural nets that recognize regular languages.
Lazy learning (Aha, 1997) postpones all the computation until an explicit request for a prediction is received.
A number of researchers have recently explored the application of reinforcement learning (RL) to resource allocation and admission control problems in telecommunications.
A large class of non-linear recurrent networks, including those studied by Grossberg,9 the Hopfield net,lO,l1 and many more recent proposals for the head direction system,27 orientation tuning in primarls visual cortex,25, 1,3, 18 eye position,20 and spatial location in the hippocampus 9 make a key simplifying assumption that the connections between the neurons are symmetric.
When applying a learning algorithm (or comparing several algorithms), one is typically interested in estimating its generalization error.
The technique of independent factor analysis (IFA) introduced in [1] provides a tool for modeling L'-dim data in terms of L unobserved factors.
Statistical modeling of complex sequences is a prominent theme in machine learning due to its wide variety of applications (see e.
Computational models that involve competitive stages have so far been neglected in computational complexity theory, although they are widely used in computational brain models, artificial neural networks, and analog VLSI.
It is well known that general inference and learning with graphical models is computationally hard [1] and it is therefore necessary to consider restricted architectures [13], or approximate algorithms to perform these tasks [3, 7].
Local linear regression has attracted considerable attention in both statistical and machine learning literature as a flexible tool for nonparametric regression analysis [Cle79, FG96, AMS97].
According to Vapnik [9], when solving a given problem one should avoid solving a more general problem as an intermediate step.
Current hyperspectral remote sensing technology can form images of ground surface reflectance at a few hundred wavelengths simultaneously, with wavelengths ranging from 0.
Spatial information is acquired by a process of exploration that is fundamentally temporal, whether it be on a small scale, such as scanning a picture, or on a larger one, such as physically navigating through a building, a neighborhood, or a city.
Voltage-gated ion channels undergo random transitions between different conformational states due to thermal agitation.
POMDPs address the problem of acting optimally in partially observable dynamic environment [6] .
Blind source separation (BSS) is capable of dramatic results when used to separate mixtures of independent signals.
Breaking up a complex task into many smaller and simpler subtasks facilitates its solution.
In density estimation, Gaussian mixtures provide flexible-basis representations for densities that can be used to model heterogeneous data in high dimensions.
During recent years, a new set of kernel techniques for supervised learning has been developed [8].
Gaussian Processes (GPs) are powerful regression models that have gained popularity recently, though they have appeared in different forms in the literature for years.
The ever increasing information transmission in the modern world is based on reliably communicating messages through noisy transmission channels; these can be telephone lines, deep space, magnetic storing media etc.
The multivariate Gaussian is the most elementary distribution used to model generic data.
Independent Components Analysis (lCA) has generated much recent theoretical and practical interest because of its successes on a number of different signal processing problems.
Noisy neural networks were recently examined, e.
Model predictive control has become the standard technique for supervisory control in the process industries with over 2,000 applications in the refining, petrochemicals, chemicals, pulp and paper, and food processing industries [1].
The general problem we address here is that of learning a set of basis functions for representing natural images efficiently.
In this paper, we are interested in the generalization performance of linear classifiers obtained from certain algorithms.
Segmenting figure from ground is one of the most important visual tasks.
The committee approach has been widely used to reduce model uncertainty and improve generalization performance.
Much recent attention has been focused on partially observable Markov decision processes (POMDPs) which have exponentially or even infinitely large state spaces.
Imagine what happens to the point in the N-dimensional space corresponding to an N-pixel image of an object, while the object is deformed by shearing.
One approach in the attempt of comprehending how the human brain works is the analysis of neural activation patterns in the brain for different stimuli presented to a sensory system.
Support vector machines (SVMs) have attracted wide interest as a means to implement structural risk minimization for the problems of classification and regression estimation.
Layered Sigmoid Belief Networks [1] are directed graphical models [2] in which the local conditional probabilities are parameterised by weighted sums of parental states, see fig ( 1).
The capacity of neurons to encode information is directly connected to the nature of spike trains as a code.
Spatial information is acquired by a process of exploration that is fundamentally temporal, whether it be on a small scale, such as scanning a picture, or on a larger one, such as physically navigating through a building, a neighborhood, or a city.
The problem of multiclass classificatIon, especially for systems like SVMs, doesn't present an easy solution.
The perceptron algorithm [10, 11] is well-known for its simplicity and effectiveness in the case of linearly separable data.
The computer-based analysis and organization of large document repositories is one oftoday's great challenges in machine learning, a key problem being the quantitative assessment of document similarities.
Decision trees have been proved to be a very popular tool in experimental machine learning.
Methods that aim to determine relevance of inputs have always interested researchers in various communities.
There has been considerable interest recently in voting methods for pattern classification, which predict the label of a particular example using a weighted vote over a set of base classifiers [10, 2, 6, 9, 16, 5, 3, 19, 12, 17, 7, 11, 8].
When we look at a painting or a visual scene, our eyes move around rapidly and constantly to look at different parts of the scene.
Learning is the stochastic process of generalizing from a random finite sample of data.
The Hebbian paradigm that serves as the basis for models of associative memory is often conceived as the statement that a group of excitatory neurons (the Hebbian cell assembly) that are coupled synaptically to one another fire together when a subset of the group is being excited by an external input.
Both physiological evidence and connectionist theory support the notion that in the brain, memories are stored in the pattern of learned synaptic weight values.
Real-world data sets are extremely large by the standards of the machine learning community.
Over the past decade or two mixture models have become a popular approach to clustering or competitive learning problems.
Visualization of input data and feature selection are intimately related.
Boosting and related Ensemble learning methods have been recently used with great success in applications such as Optical Character Recognition (e.
A fundamental approach in signal processing is to design a statistical generative model of the observed signals.
Urine is one of the most complex body fluid specimens: it potentially contains about 60 meaningful types of elements.
The problem of getting mobile robots to autonomously learn maps of their environment has been widely studied (see e.
Reinforcement learning (RL) has been applied to resource allocation problems in telecommunications, e.
In many classification tasks, recognition accuracy is low because input patterns are corrupted by noise or are spatially or temporally overlapping.
In the early nineties, Buntine and Weigend (1991) and Mackay (1992) showed that a principled Bayesian learning approach to neural networks can lead to many improvements [1 ,2] .
In many situations, precise timing of a motor output is essential for successful task completion.
Speech is seldom heard in isolation: usually, it is mixed with other environmental sounds.
In recent years, there has been growing interest in algorithms for approximate planning in (exponentially or even infinitely) large Markov decision processes (MDPs) and partially observable MDPs (POMDPs).
In the Bayesian framework predictions are made by integrating the function of interest over the posterior parameter distribution, the lattt~r being the normalized product of the prior distribution and the likelihood.
The human ear transduces airborne sounds into a neural signal using three stages in the inner ear's cochlea: (i) the mechanical filtering of the Basilar Membrane (BM), (ii) the transduction of membrane vibration into neurotransmitter release by the Inner Hair Cells (IHCs), and (iii) spike generation by the Spiral Ganglion Cells (SGCs), whose axons form the auditory nerve.
The problem of self-organization of the members of a set X based on the similarity of the conditional distributions of the members of another set, Y, {p(Ylx)}, was first introduced in [8] and was termed "distributional clustering" .
Sigmoidal neural networks are known to be universal approximators.
Let f (x, w) be an n input, m output, twice differentiable feedforward model parameterized by an input vector, x, and a weight vector w.
Understanding brain function requires knowing connections between neurons.
The vast majority of Reinforcement Learning (RL) [9J and Neuro-Dynamic Programming (NDP) [lJ methods fall into one of the following two categories: (a) Actor-only methods work with a parameterized family of policies.
Support Vector Machines (SVMs) implement the following idea: they map input vectors into a high dimensional feature space, where a maximal margin hyperplane is constructed [6].
Long term potentiation (LTP) is a neurophysiological phenomenon observed under laboratory conditions in which two neurons or neural populations are stimulated at a high frequency with a resulting measurable increase in synaptic efficacy between them that lasts for several hours or days [1]-[2] LTP thus provides direct evidence supporting the neurophysiological hypothesis articulated by Hebb [3].
Most work on hierarchical reinforcement learning has focused on temporal abstraction.
A new approach to learning has recently been proposed as an alternative to feedforward neural networks: the Support Vector Machines (SVM) [1].
Reinforcement learning (RL) addresses the problem of learning to act so as to maximize a reward signal provided by the environment.
Inspiration from biology has been recognized as a seminal approach to address some engineering challenges, particularly in the computational domain [1] .
Synapse-specific changes in synaptic efficacies, carried out by long-term potentiation (LTP) and depression (LTD) are thought to underlie cortical self-organization and learning in the brain.
For an operational mobile robot, it is essential to prevent becoming lost.
Bayesian techniques have been widely and successfully used in the neural networks and statistics community and are appealing because of their conceptual simplicity, generality and consistency with which they solve learning problems.
One of the major advantages in the Bayesian methodology is that "overfitting" is avoided; thus the difficult task of adjusting model complexity vanishes.
The controllability of VR makes it an excellent candidate for use in studying cognition.
Many approaches to object recognition in images estimate Pr(class I image).
Recently, SchOlkopf et al.
Noisy dynamical processes abound in the world - human speech, the frequency of sun spots, and the stock market are common examples.
We seek to capture the 3D motions of humans from video sequences.
Supervised learning is obtaining an underlying rule from training examples , and can be formulated as a function approximation problem.
We consider the problem of learning both the hidden states Xk and parameters w of a discrete-time nonlinear dynamic system, F(Xk , Vk, w) H(xk, nk, w), (1) (2) where Yk is the only observed signal.
Effective discrimination is essential in many application areas.
We examine a psychophysical law, named the Morton-Massaro law, and its implications to connectionist models of perception and neural information processing.
Lazzaro and colleagues (Lazzaro, 1988) were the first to implement a hardware model of a winner-take-all (WTA) network.
We focus on the following objective: given a set of experimental data and the assumption that it was produced by a deterministic chaotic system, find a set of model equations that will produce a time-series with identical chaotic characteristics, having the same chaotic attractor.
The first boosting algorithm appeared in Rob Schapire's thesis [1].
The dynamics of learning in neural computing is a complex multi-variate process.
Learning algorithms are designed to extract structure from data.
Recently blind separation/deconvolution has been recognized as an increasing important research area due to its rapidly growing applications in various fields, such as telecommunication systems, image enhancement and biomedical signal processing.
Perceptual organization refers to the ability of grouping similar features in sensory data.
A standard method to learn a graphical model 1 from data is maximum likelihood (ML).
The graphical models formalism provides an appealing framework for the design and analysis of network-based learning and inference systems.
Independent component analysis (ICA) methods are typically run in batch mode in order to keep the stochasticity of the empirical gradient low.
Humans are remarkably accurate in their ability to localize transient, broadband noise, an ability with obvious evolutionary advantages.
Population coding is a method to encode and decode stimuli in a distributed way by using the joint activities of a number of neurons (e.
Factor analysis (FA) is a method for modelling correlations in multidimensional data.
Facial expressions provide information not only about affective state, but also about cognitive activity, temperament and personality, truthfulness, and psychopathology.
Many problems of engineering interest can be formulated as sequential data problems in an abstract sense as supervised learning from sequential data, where an input vector (dimensionality D) sequence X = xf = {X!,X2, .
In the last four decades there has been a vivid and highly polarized discussion about the role of recurrent competition in the primary visual cortex (VI) (see [12] for review).
The Markov Decision Processes (MDP) framework [2] is a good way of mathematically formalizing a large class of sequential decision problems involving an agent that is interacting with an environment.
Recent efforts define the model selection problem as one of estimating the number of clusters[ 10, 17].
The curse of dimensionality hits particularly hard on models of high-dimensional discrete data because there are many more possible combinations of the values of the variables than can possibly be observed in any data set, even the large data sets now common in datamining applications.
Speech signals typically carry information about number of target sources such as linguistic message, speaker identity, and environment in which the speech was produced.
Current hyperspectral remote sensing technology can form images of ground surface reflectance at a few hundred wavelengths simultaneously, with wavelengths ranging from 0.
Tools from statistical mechanics have been used successfully over the last decade to study the dynamics of learning in layered neural networks (for reviews see e.
Conditioning experiments probe the ways that animals make predictions about rewards and punishments and how those predictions are used to their advantage.
Traditional theories of human vision considers two functionally distinct stages of visual processing [1].
In this paper, we present a new multi-class learning algorithm called Committee.
Classical linear discriminant analysis (LDA) projects N data vectors that belong to c different classes into a (c - 1)-dimensional space in such way that the ratio of between group scatter SB and within group scatter Sw is maximized [1].
Functional magnetic resonance imaging (tMRI) is a non-invasive technique that enables indirect measures of neuronal activity in the working human brain.
The use of hardware models to understand dynamical behaviors in biological systems is an approach that has a long and fruitful history [1 ][2].
Gaussian processes provide promising non-parametric Bayesian approaches to regression and classification [2, 1].
In recent years, neuroscientists and modelers have made great strides towards illuminating structure and computational properties in biological motor systems.
Growing interest in intelligent human computer interactions has motivated a recent surge in research on problems such as face tracking, pose estimation, face expression and gesture recognition.
This paper models speech using a stochastic state space model, where model parameters are estimated using the expectation-maximisation (EM) technique.
In domains ranging from reasoning to language acquisition, a broad view is emerging of cognition as a hybrid of two distinct modes of computation, one based on applying abstract rules and the other based on assessing similarity to stored exemplars [7].
A great number of scientific fields today benefit from being able to automatically estimate the probability of certain quantities of interest that may be difficult or expensive to observe directly.
The neocortex is characterized by an extensive system of recurrent excitatory connections between neurons in a given area.
Systems in which human users speak to a computer in order to achieve a goal are called spoken dialogue systems.
The ability to accurately estimate the location of a sound source has obvious evolutionary advantages in terms of avoiding predators and finding prey.
Working memory is usually defined as the capability to actively hold information in memory for short periods of time.
In a cell, genes can be turned "on" or "off' to varying degrees by the protein products of other genes.
Following [6] we consider a general scheme of transductive inference.
Cognitive linguistics has seen the development in recent years of two important, related trends.
The problem of multivariate density estimation is important for many applications, in particular, for speech recognition [1] [7].
Hierarchical learning machines such as multi-layer perceptrons, radial basis functions, and normal mixtures are non-regular and non-identifiable learning machines.
Due to the large amounts of imagery that can now be accessed and managed via computers, the problem of content-based image retrieval (CBIR) has recently attracted significant interest among the vision community [1, 2, 5].
Reinforcement Learning (RL) [7] is a learning paradigm based upon the framework of Markov decision process (MDP).
A commonly used assumption in nonlinear regression is that targets are disturbed by independent additive Gaussian noise.
Recent experiments have demonstrated types of synaptic ~re 11111111111 111111111111111111 plasticity that depend on the post 11111111111 11111111• •11.
Support vector machines (SVM) [1, 13] and boosting [10, 3, 4, 11] are highly popular and effective methods for constructing linear classifiers.
Model selection criteria based on the Vapnik-Chervonenkis (VC) dimension are known to be difficult to obtain, worst case, and often not very tight.
Reconstruction of statistically independent source signals from linear mixtures is an active research field.
Structural Risk Minimisation (SRM) in a learning system can be achieved using constraints on the parameter vectors, using regularization terms in the cost function, or using Support Vector Machines (SVM).
Density Estimation is a fundamental problem in statistics.
Recovering the intrinsic dimensionality of a data set is a classic and fundamental problem in data analysis.
Suppose we are given n objects, and for each pair (i,j) we have a measurement of the "dissimilarity" Oij between the two objects.
The problem of building a reliable switch arises in several different biological contexts.
Recently there has been an resurgence of interest in using optimal coding strategies to 'explain' the response properties of neuron in the primary sensory areas [1].
Researchers in the field of reinforcement learning have recently focused considerable attention on temporally abstract actions (e.
Adaptation was one of the first phenomena discovered when Adrian recorded the responses of single sensory neurons [1, 2].
Periodic structure in the time waveform conveys important cues for recognizing and understanding speech[I].
Sensory systems must take advantage of the statistical structure of their inputs in order to process them efficiently, both to suppress noise and to generate compact representations of seemingly complex data.
We consider the computational complexity of learning linear perceptrons for arbitrary (Le.
The last decade has seen increased application of statistical pattern recognition techniques to the problem of object recognition from images.
In this paper, we consider a learning problem that combines elements of regression and classification.
The modeling and tracking of human motion in video is important for problems as varied as animation, video database search, sports medicine, and human-computer interaction.
Gaussian processes are typically used for regression where it is assumed that the underlying function is generated by one infinite-dimensional Gaussian distribution (i.
The problem of multiclass categorization is about assigning labels to instances where the labels are drawn from some finite set.
Unsupervised learning algorithms such as principal components analysis and vector quantization can be understood as factorizing a data matrix subject to different constraints.
Reinforcement learning encompasses a class of machine learning problems in which an agent learns from experience as it interacts with its environment.
The recently introduced boosting approach to classification (e.
Competitive learning is a family of neural learning algorithms that has proved useful for training many classification and clustering networks [1].
Graphical models are an invaluable tool for defining and manipulating probability distributions.
How does the visual system represent its knowledge about objects so as to identify them? A largely unquestioned assumption in the study of object recognition has been that the visual system builds up a representation for an object by sequentially transforming an input image into progressively more abstract representations.
In multi-service communications networks, such as Asynchronous Transfer Mode (ATM) networks, resource control is of crucial importance for the network operator as well as for the users.
One of the major problems in automatic speech recognition (ASR) systems is their lack of robustness in noise, which severely degrades their usefulness in many practical applications.
Gaussian process (GP) models have gained considerable interest in the Neural Computation Community (see e.
We study the performance of eight different methods for developing image representations based on the statistical properties of the images at hand.
Gaussian processes have become popular because they allow exact Bayesian analysis with simple matrix manipulations, yet provide good performance.
The Support Vector Classifier (SVC) is a powerful tool to solve pattern recognition problems [13, 14] in such a way that the solution is completely described as a linear combination of several training samples, named the Support Vectors.
Compression algorithms for image and video signals often use transform coding as a low-complexity alternative to vector quantization (VQ).
It is commonly believed that large multi-layer perceptrons (MLPs) generalize poorly: nets with too much capacity overfit the training data.
When two people look at the same scene, do they see the same things? This basic question in the theory of knowledge seems to be beyond the scope of experimental investigation.
Occam's Razor is a well known principle of "parsimony of explanations" which is influential in scientific thinking in general and in problems of statistical inference in particular.
The last decade has seen much interest in structured approaches to solving planning problems under uncertainty formulated as Markov decision processes (MDPs).
Face recognition is difficult when the number of individuals is large and the test and training images of an individual differ in expression, pose, lighting or the date on which they were taken.
Circuit complexity theory is a classical area of theoretical computer science, that provides estimates for the complexity of circuits for computing specific benchmark functions, such as binary addition, multiplication and sorting (see, e.
In this paper, we consider the binary classification problem that is to determine a label y E {-1, 1} associated with an input vector x.
Independent component analysis (ICA) is a technique to transform a multivariate random signal into a signal with components that are mutually independent in complete statistical sense [1].
An important classification task is the ability to distinguish b etween new instances similar to m embers of the training set and all other instances that can occur.
We consider the so-called n e ural networks nonlinear least squares problem 1 wherein the objective is to optimize the n weight parameters of neural networks (NN) [e.
Support vector machines (SVMs) [23, 5, 14, 12] are powerful tools for data classification.
Independent Component Analysis (ICA) is a method for blind signal separation.
Recently, considerable attention has been paid to understanding and modeling the non-Gaussian or "higher-order" properties of natural signals, particularly images.
In many machine learning applications, the most time-consuming and costly task is the collection of a sufficiently large data set.
Minimization of regularized risk is a backbone of several recent advances in machine learning, including Support Vector Machines (SVM) [13], Regularization Networks (RN) [5] or Gaussian Processes [15].
In order to achieve spatial learning, both animals and artificial agents need to autonomously locate themselves based on available sensory information.
Data clustering is one of the most fundamental pattern recognition problems, with numerous algorithms and applications.
Recent studies of synapses between pyramidal neocortical and hippocampal neurons [1, 2, 3, 4] have revealed that changes in synaptic efficacy can depend on the relative timing of pre- and postsynaptic spikes.
Computer animation of realistic characters requires an explicitly defined model with control parameters.
One of the crucial ingredients of SVMs is the so-called kernel trick for the computation of dot products in high-dimensional feature spaces using simple functions defined on pairs of input patterns.
This paper focuses on pairwise (or similarity-based) clustering and image segmentation.
The Fisher information matrix determines a metric of the set of all parameters of a learning machine [2].
Filtering is the problem of estimating the states (parameters or hidden variables) of a system as a set of observations becomes available on-line.
Cats, many species of monkeys, and humans exibit ocular dominance stripes, which are alternating areas of primary visual cortex devoted to input from (the thalamic relay associated with) just one or the other eye (see Erwin et aI, 1995; Miller, 1996; Swindale, 1996 for reviews of theory and data).
Many unsupervised learning procedures can be viewed as trying to bring two probability distributions into alignment.
Classical neural network models approximate neurons as devices that sum their inputs and generate a nonzero output if the sum exceeds a threshold.
Modem robots still lag far behind animals in their capability for legged locomotion.
In the traditional reinforcement learning framework, the learning agent is given a single scalar value of reward at each time step.
Graphical probability models with hidden variables are capable of representing complex dependencies between variables, filling in missing data and making Bayesoptimal decisions using probabilistic inferences (Hinton and Sejnowski 1986; Pearl 1988; Neal 1992).
Research in video retrieval has traditionally focussed on the paradigm of query-byexample (QBE) using low-level features [2].
Figure 1 : Human meteorite search with snowmobiles on the Antarctic ice sheets, and on foot in the moraines.
Two great ideas have dominated recent developments in machine learning: the application of kernel methods and the popularisation of Bayesian inference.
Non-parametric kernel methods such as Gaussian Processes (GPs) and Support Vector Machines (SVMs) are closely related to neural networks (NNs).
The general problem we address here is that of learning efficient codes for representing natural images.
Tracking objects with flexible shapes in video sequences is currently an important topic in the vision community.
The CDMA (Code-Division-Multiple-Access) technique [1] is important as a fundamental technology of digital communications systems, such as cellular phones.
We perform spatial computations everday.
Causality plays a central role in human mental life.
Much evidence suggests that dopamine cells in the primate midbrain play an important role in reward and action learning.
In many situations it is necessary to make decisions that depend on the outcomes of several different classifiers in a way that provides a coherent inference that satisfies some constraints - the sequential nature of the data or other domain specific constraints.
Clustering is an ill-defined problem for which there exist numerous methods [1, 2].
In recent years, SVMs have been successfully applied to various tasks in computational face-processing.
Training a support vector machine (SVM) requires solving a quadratic programming (QP) problem in a number of coefficients equal to the number of training examples.
Principal component analysis (PCA) is a well-established technique for dimensionality reduction, and examples of its many applications include data compression, image processing, visualisation, exploratory data analysis, pattern recognition and time series prediction.
Imagine trying to plan a route from home to work that minimizes expected time.
Standard learning systems (like neural networks or decision trees) operate on input data after they have been transformed into feature vectors XI, ••• , Xl E X from an n dimensional space.
Multi-media signals pervade our environment.
In error-correcting codes [1] and image restoration [2], the choice of the so-called hyperparameters is an important factor in determining their performances.
Many unsupervised learning algorithms such as principal component analysis, vector quantization, self-organizing feature maps, and others use the principle of minimizing reconstruction error to learn appropriate features from multivariate data [1, 2].
It was suggested by Barlow [3] that the goal of early sensory processing is to reduce redundancy in sensory information and the activity of sensory neurons encodes independent features.
The field of perceptual learning in simple, but high precision, visual tasks (such as vernier acuity tasks) has produced many surprising results whose import for models has yet to be fully felt.
The hippocampal formation and adjacent cortical areas have long been believed to be involved in the acquisition and retrieval of long-term memory for events and other declarative information.
Neurons in higher regions of visual cortex have relatively large receptive fields (RFs): for example, neurons representing the central visual field in macaque area V4 have RFs up to 5° across (Desimone & Schein, 1987).
Kernel machines have recently gained a lot of attention due to the popularisation of the support vector machine (SVM) [13] with a focus on classification and the revival of Gaussian Processes (GP) for regression [15].
Linear classifiers are exceedingly popular in the machine learning community due to their straight-forward applicability and high flexibility which has recently been boosted by the so-called kernel methods [13].
Application of mean-field theory to solve the problem of inference in Belief Networks(BNs) is well known [1].
Modern speech recognition systems use cepstral features characterizing the short-term spectrum of the speech signal for classifying frames into phonetic classes.
Although there are convergent online algorithms (such as TD()') [1]) for learning the parameters of a linear approximation to the value function of a Markov process, no way is known to extend these convergence proofs to the task of online approximation of either the state-value (V*) or the action-value (Q*) function of a general Markov decision process.
A key issue in computational learning theory is to bound the generalization error of learning algorithms.
In the last few years there has been a large controversy about the significance of the attained margin, i.
We explored long-time constant adaptation mechanisms in a simple integrate-and-fire silicon neuron.
Bayesian approaches to machine learning have several desirable properties.
This paper presents a statistical-model-based algorithm for reconstructing a speech source from microphone signals recorded in a stationary noisy reverberant environment.
This paper is about accelerating a wide class of statistical methods that are naively quadratic in the number of datapoints.
Jet engines have a number of rigorous pass-off tests before they can be delivered to the customer.
Local "belief propagation" (BP) algorithms such as those introduced by Pearl are guaranteed to converge to the correct marginal posterior probabilities in tree-like graphical models.
We consider the binary classification problem: to determine a label y E {-1, 1} associated with an input vector x.
Structure from motion (SFM) addresses the problem of simultaneously recovering camera pose and a three-dimensional model from a collection of images.
Temporal-difference (TD) learning has been applied successfully to many real-world applications that can be formulated as discrete state Markov Decision Processes (MDPs) with unknown transition probabilities.
In the last years increasing interest has been devoted to the application of mean-field techniques to inference problems.
A standard problem in machine learning is to how to efficiently model correlations in multidimensional data.
In classification, a feature vector x = (Xl,···, Xqy E lRq, representing an object, is assumed to be in one of J classes {i}{=l' and the objective is to build classifier machines that assign x to the correct class from a given set of N training samples.
The field of reinforcement learning has recently adopted the idea that the application of prior knowledge may allow much faster learning and may indeed be essential if realworld environments are to be addressed.
The human figure exhibits complex and rich dynamic behavior.
Incorporating a priori knowledge of a particular task into a learning algorithm helps reducing the necessary complexity of the learner and generally improves performance, if the incorporated knowledge is relevant to the task and really corresponds to the generating process of the data.
More than two decades of research on artificial neural networks has emphasized the central role of synapses in neural computation.
Even simple nervous systems are capable of performing multiple and unrelated tasks, often in parallel.
The phenomenon of expl aining away is commonplace in inference in belief networks.
In many supervised learning problems feature selection is important for a variety of reasons: generalization performance, running time requirements, and constraints and interpretational issues imposed by the problem itself.
No text, no paper, no book can be isolated from the all-embracing corpus of documents it is embedded in.
In the last decade there has been a great deal of research focused on the problem of learning Bayesian networks (BNs) from data (e.
Recently, a number of authors have proposed deterministic methods for approximate inference in large graphical models.
Online Reinforcement Learning (RL) algorithms try to find a policy which maximizes the expected time-discounted reward provided by the environment.
A fundamental problem that makes language modeling and other learning problems difficult is the curse of dimensionality.
Hidden Markov models [9] have been a dominant paradigm in speech and handwriting recognition over the past several decades.
Hebbian plasticity, the major paradigm for learning in computational neuroscience, was until a few years ago interpreted as learning by correlated neuronal activity.
Gaussian processes (GP) [1; 15] provide promising non-parametric tools for modelling real-world statistical problems.
Graphical models have the capability to model a large class of probability distributions.
A great deal of effort has been devoted in recent years to the study of maximal margin classifiers.
In this study, we propose a new reinforcement learning paradigm that we call "Robust Reinforcement Learning (RRL).
Unsupervised learning of generative and feature-extracting models for continuous nonnegative data has recently been proposed [1], [2] .
Kernel functions are widely used in learning algorithms such as Support Vector Machines, Gaussian Processes, or Regularization Networks.
A large number of experimental studies have shown that biological synapses have an inherent dynamics, which controls how the pattern of amplitudes of postsynaptic responses depends on the temporal pattern of the incoming spike train.
Recent years have shown an enormous interest in kernel-based classification algorithms, primarily in Support Vector Machines (SVM) [2].
Recently, the demand for very fast image processing systems with real time operation capability has significantly increased.
Given the recent advances on video coding and streaming technology and the pervasiveness of video as a form of communication, there is currently a strong interest in the development of techniques for browsing, categorizing, retrieving and automatically summarizing video.
In many modern classification problems such as text categorization, very few labeled examples are available but a large number of unlabeled examples can be readily acquired.
Statistical model estimation and inference often require the maximization, evaluation, and integration of complicated mathematical expressions.
In recent years, there has been growing interest in the statistics of natural images (see Huang and Mumford [4] for a recent review).
This paper explores a framework for recognition of image sequences using partially observable stochastic differential equations (SDEs).
One of the most significant challenges in learning grammars from raw text is keeping the computational complexity manageable.
In classification tasks , learning enables us to predict the output y E {-1 , + 1} of some unknown system given the input a! E X based on the training examples {a!i ' y;}i=l' The purpose of a feature extractor f : X --+ ]RD is to convert the representation of data without losing the information needed for classification [3] .
Approximate expressions for generalization errors for finite dimensional statistical data models can be often obtained in the large data limit using asymptotic expansions.
Nearly fifty years ago, Barlow [1] and Attneave [2] suggested that the brain may construct a neural code that provides an efficient representation for the sensory stimuli that occur in the natural world.
An IR system consists of a collection of documents and an engine that retrieves documents described by users queries.
We integrate the portfolio optimization algorithm suggested by Black / Litterman [1] into a neural network architecture.
Two types of perceptual process might, in principle, map physically different visual stimuli onto the same percept.
The goal of the population dynamics approach is to model the time course of the collective activity of entire populations of functionally and dynamically similar neurons in a compact way, using a higher descriptionallevel than that of single neurons and spikes.
There has been a growing interest in direct policy-gradient methods for approximate planning in large Markov decision problems (MDPs).
Both intuition and theory [9] seem to support that the best linear separation between two classes is the one that maximizes the margin.
Statistical learning theory is mainly concerned with the study of uniform bounds on the expected error of hypotheses from a given hypothesis space [9, 1].
Principal component analysis (PCA) is a hugely popular dimensionality reduction technique that attempts to find a low-dimensional subspace passing close to a given set of points 	  	  .
Biological and artificial recognition systems face the challenge of grouping together differing proximal stimuli arising from the same underlying object.
While most applications of reinforcement learning (RL) to date have been to problems of control, game playing and optimization (Sutton and Barto1998), there has been a recent handful of applications to human-computer interaction.
Abrupt changes can occur in many different real-world systems like, for example, in speech, in climatological or industrial processes, in financial markets, and also in physiological signals (EEG/MEG).
Kernel based learning algorithms [1] are modular systems formed by a generalpurpose learning element and by a problem specific kernel function.
This paper addresses the problem of retrieving items in memory from partial information.
Support Vector Machine (SVM) [1] is one of the latest and most successful statistical pattern classifier that utilizes a kernel technique [2, 3].
'Place cells' recorded in the hippocampus of freely moving rats encode the rat's current location (O'Keefe & Dostrovsky, 1971; Wilson & McNaughton, 1993).
Seeking to replicate the representational and computational benefits that graphical models have provided to probabilistic inference, several recent works have introduced graph-theoretic frameworks for the study of multi-agent systems (La Mura 2000; Koller and Milch 2001; Kearns et al.
Automatic music transcription refers to extraction of a high level description from musical performance, for example in form of a music notation.
We study the problem of automatically synthesizing hierarchical classifiers by learning discriminative object parts in images.
We show convergence rates of ensemble learning methods such as AdaBoost [10], Logistic Regression (LR) [11, 5] and the Least-Square (LS) regression algorithm called LS-Boost [12].
Psychophysical studies as well as introspection indicate that we are not blind outside the focus of attention, and that we can perform simple judgments on objects not being attended to [1], though those judgments are less accurate than in the presence of attention [2, 3].
The analysis of a dynamic scene implies estimates of motion parameters to infer spatio-temporal information about the visual world.
In this paper, we study bootstrapping algorithms for learning from unlabeled data.
Information in the brain is not processed by a single neuron, but rather by a population of them.
Feature binding has been proposed to provide elegant solution strategies to the segmentation problem in perception [11, 12, 14].
Powerful systems for data inference, like neural networks implement complex inputoutput relations by learning from example data.
Grouping is often thought of as the process of finding intrinsic clusters or group structures within a data set.
Feature selection or feature transforms are important aspects of any pattern recognition system.
      	 	                   !"$#&%'  	(  ) In order to estimate a good predictor   from a set of training data     randomly drawn from  , it is necessary to start with a model of the functional relationship.
Narayanan and Jurafsky (1998) proposed that human language comprehension can be modeled by treating human comprehenders as Bayesian reasoners, and modeling the comprehension process with Bayesian decision trees.
A neural network is specified by a number of parameters which are synaptic weights and biases.
Based on the theory of hierarchical structure and related invariant decomposition of interactions by information geometry [3], the present paper briefly summarizes methods useful for systematically analyzing a population of neural firing [9].
Sequential Monte Carlo (SMC) particle methods go back to the first publically available paper in the modern field of Monte Carlo simulation (Metropolis and Ulam 1949) ; see (Doucet, de Freitas and Gordon 2001) for a comprehensive review.
The task of clustering raw data such as video frames and speech spectrograms is often obfuscated by the presence of random, but well-understood transformations in the data.
An aspect of delayed-reward reinforcement learning problem which has a long history of study in animal experiments, but has been overlooked by theorists, is the learning of the expected time to the reward.
Since the proposal of turbo codes[2], they have been attracting a lot of interests because of their high performance of error correction.
The Perceptron and Winnow algorithms are well known learning algorithms that make predictions using a linear function in their feature space.
Graph matching is a classic problem in computer vision and pattern recognition, instances of which arise in areas as diverse as object recognition, motion and stereo analysis [1].
Excitatory interactions between neurons are mediated by two classes of synapses: AMPA and NMDA.
Transaction data sets consist of records of pairs of individuals and events, e.
Many domains are naturally associated with a hierarchical taxonomy, in the form of a tree, where instances that are close to each other in the tree are assumed to be more “similar” than instances that are further away.
Continuous-valued latent representations of observed feature vectors can be useful for pattern classification via Bayes rule, summarizing data sets, and producing lowdimensional representations of data for later processing.
This paper explores the use of statistical learning methods and probabilistic inference techniques for modeling the relationship between the motion of a monkey’s arm and neural activity in motor cortex.
The approach in combinatorial optimization is traditionally single-instance and worst-case-oriented.
Many problems in Text Mining or Bioinformatics are multi-labelled.
A standard way of performing classification using a generative model is to divide the training cases into their respective classes and t hen train a set of class conditional models.
The main mathematical problem faced by actuaries is that of estimating how much each insurance contract is expected to cost.
The mutual information J (also called cross entropy) is a widely used information theoretic measure for the stochastic dependency of random variables [CT91, SooOO] .
It has been consistently observed that rapid human arm movements in both infants and adults often consist of several submovements, sometimes called “movement units” [21].
Sparse multinomial distributions arise in many statistical domains, including natural language processing and graphical models.
Classification with partially labeled examples involves a limited dataset of labeled examples as well as a large unlabeled dataset.
One of the main contributions of the recent statistical theories for regression and classification problems [21, 19] is the derivation of functionals of certain empirical quantities (such as the sample error or the sample margin) that provide uniform risk bounds for all the hypotheses in a certain class.
An important aspect of decision theory is multi-way pattern classification whereby one must determine the class  for a given data vector  that minimizes the overall risk:   argmin  	          where      is the loss in choosing   when the true class is  .
The individual and joint computational roles of neuromodulators such as dopamine, serotonin, norepinephrine and acetylcholine are currently the focus of intensive study.
How do groups of sensory neurons interact to code information and how do these interactions change along the ascending sensory pathways? According to the a common view, sensory systems are composed of a series of processing stations, representing more and more complex aspects of sensory inputs.
In the field of artificial grammar learning, people are known to be able to transfer grammatical knowledge to a new language which consists of a new vocabulary [6].
In the blind source separation problem an N-channel sensor signal x(~ ) is generated by M unknown scalar source signals s rn(~) , linearly mixed together by an unknown N x M mixing, or crosstalk, matrix A , and possibly corrupted by additive noise n(~): x(~) = As(~) + n(~ ).
Policy Gradient Reinforcement Learning (PGRL) algorithms have recently received attention because of their potential usefulness in addressing large continuous reinforcement Learning (RL) problems.
In some classification tasks, an a priori knowledge is known about the invariances related to the task.
In a widespread area of applications kernel based learning machines, e.
Virtually all existing work on value function approximation and policy-gradient methods starts with a parameterized formula for the value function or policy and then seeks to find the best policy that can be represented in that parameterized form.
Images of a class of objects are often not effectively characterized by a Gaussian distribution or even a mixture of Gaussians.
The question of how many Boolean primitives are needed to learn a logical formula is typically an NP-hard problem, especially when learning from noisy data.
Traditional information retrieval techniques can give poor results on the Web, with its vast scale and highly variable content quality.
Experiments in the last years have shown that in many cortical areas, the fluctuations in the responses of neurons to external stimuli are significantly correlated [1, 2, 3, 4], raising important questions regarding the computational implications of neuronal correlations.
ICA as a method for blind source separation has been proven very useful in a wide range of statistical data analysis.
The two most well-known approaches for reducing a multiclass classification problem to a set of binary classification problems are known as one-against-all and all-pairs.
Estimating the state of a dynamic system based on noisy sensor measurements is extremely important in areas as different as speech recognition, target tracking, mobile robot navigation, and computer vision.
Energetical considerations [1] and measurements [2] suggest , that sub-threshold inputs, i.
Dynamic programming offers a unified approach to solving problems of stochastic control.
Belief Propagation (BP) is a message passing scheme, which is known to yield exact inference in tree structured graphical models [1].
Many cognitive models posit mental representations based on discrete substructures.
The information bottleneck (IB) method of Tishby et al [14] is an unsupervised nonparametric data organization technique.
In recent years, particle filters [3, 7, 8] have found widespread application in domains with noisy sensors, such as computer vision and robotics [2, 5].
The ranking problem we discuss in this paper shares common properties with both classification and regression problems.
 	               	                     	! "#%$ &'")(*+&'")(-,.
Several network models have been designed to emulate the properties of head-direction neurons (HDNs) [Zhang, 1996, Redish et al.
Many learning problems in complex domains such as bioinformatics, vision, and information retrieval involve large collections of interdependent variables, none of which has a privileged status as a response variable or class label.
Recent biological experimental findings have indicated that the synaptic plasticity depends on the relative timing of the pre- and post- synaptic spikes which determines whether Long Term Potentiation (LTP) occurs or Long Term Depression (LTD) does [1, 2, 3].
Function approximation is essential for applying value-function-based reinforcement learning (RL) algorithms to solve large Markov decision problems (MDPs).
Recently a lot of work has been done around Support Vector Machines [9], mainly due to their impressive generalization performances on classification problems when compared to other algorithms such as artificial neural networks [3, 6].
Kernel machines, such as Support Vector machines or Gaussian processes, are powerful and frequently used tools for solving statistical learning problems.
The problem of searching for information in networks like the World Wide Web can be approached in a variety of ways, ranging from centralized indexing schemes to decentralized mechanisms that navigate the underlying network without knowledge of its global structure.
In semi-supervised classification tasks, a concept is to be learnt using both labeled and unlabeled examples.
K-means is a very popular method for general clustering [6].
Comparison of sequences of observations is a natural and necessary operation in speech applications.
Over the last decade, bounding techniques have become a popular tool to deal with graphical models that are too complex for exact computation.
Once a classifier is estimated from the training data, it can be used to label new examples, and in many application domains, such as character recognition, text classification and others, this constitutes the final goal of the learning stage.
Animal learning involves more than just predicting reinforcement.
Two of the most important goals in Computational Drug Design are to find active compounds in large databases quickly and (usually along the way) to obtain an interpretable model for what makes a specific subset of compounds active.
Neurons in the macaque inferior temporal visual cortex (IT) that respond to objects or faces have large receptive fields when a single object or image is shown on an otherwise blank screen [1, 2, 3].
Vector quantization (VQ) techniques are used in a wide range of applications, including speech and image processing, data compression.
In machine learning it is important to know the true error rate a classifier will achieve on future test cases.
Mixture models[11] have been widely applied to problems in classification.
Analog recurrent neural networks are known to have computational capabilities that exceed those of classical Turing machines (see, e.
The sheer number of cortical neurons and the vast connectivity within the cortex are difficult to duplicate in either hardware or software.
Automatic Speech Recognition systems perform reasonably well in controlled and matched training and recognition conditions.
Speech recognition is a complex, dynamic classification task.
The image flow can be considerably more complicated than merely an expanding pattern of motion vectors centered on the heading direction (Fig.
Markov decision processes (MDPs) form a foundation for control in uncertain and stochastic environments and reinforcement learning.
Recent progress on error-correcting codes has attracted much attention because their decoders, exhibiting performance very close to Shannon's limit, can be implemented as neural networks.
There has recently been a lot of interest in learning probability models for vision.
Consider a system where multiple agents, each with its own set of possible actions and its own observations, must coordinate in order to achieve a common goal.
Generative classifiers learn a model of the joint probability, p( x, y), of the inputs x and the label y, and make their predictions by using Bayes rules to calculate p(ylx), and then picking the most likely label y.
This paper considers an on-line learning problem for Markov decision processes with vector-valued rewards.
In order to produce movement, every vertebrate has to coordinate the large number of degrees of freedom in the musculoskeletal apparatus.
Speech recognition in noise has been considered to be essential for its real applications.
If robins didn’t have wings would they still be able to fly, eat worms or build nests? Previous work on counterfactual reasoning has tended to characterise the processes by which questions such as these are answered in purely qualitative terms, either focusing on the factors determining their onset and consequences (see Roese, 1997, for a review); the qualitative outline of their psychological characteristics (Kahneman and Miller, 1986; Byrne and Tasso, 1999); or else their logical or schematic properties (Lewis, 1973; Goodman, 1983).
Unsupervised learning is essentially concerned with finding alternative representations for unlabeled data.
Hidden Markov models (HMMs) are one of the most popular methods in machine learning and statistics for modelling sequences such as speech and proteins.
In many applications fast classification is almost as important as accurate classification.
We often take for granted the ease with which we can carryon a conversation in the proverbial cocktail party scenario: guests chatter, glasses clink, music plays in the background: the room is filled with ambient sound.
For many interesting models, exact inference is intractible.
Reinforcement learning (RL) has been applied to resource allocation problems in telecommunications.
Hebbian learning rules are widely used to model synaptic modification shaping the functional connectivity of neural networks [4, 5].
Two main approaches to robust speech recognition include "recognizer domain approaches" (Varga and Moore 1990; Gales and Young 1996), where the acoustic recognition model is modified or retrained to recognize noisy, distorted speech, and "feature domain approaches" (Boll 1979; Deng et al.
A number of learning algorithms rely on estimating spectral data on a sample of training points and using this data as input to further analyses.
Data clustering is a fundamental and challenging routine in information processing and pattern recognition.
The task of finding good clusters has been the focus of considerable research in machine learning and pattern recognition.
Reinforcement learning (RL 28) bears a tortuous relationship with historical and contemporary ideas in classical and instrumental conditioning.
Given a graphical model, one important problem is the computation of marginal distributions of variables at each node.
One of the challenges of Reinforcement Learning is learning in an unknown environment.
Although there is growing evidence of the role of contextual information in human perception [1], research in computational vision is dominated by object-based representations [5,9,10,15].
In virtually all listening situations, we are exposed to a mixture of sound energy from multiple sources.
Blind source separation (BSS) techniques have found wide-spread use in various application domains , e.
SVM training is a convex optimization problem which scales with the training set size rather than the input dimension.
Consider the problem of choosing a linear discriminant by minimizing the probabilities that data vectors fall on the wrong side of the boundary.
Cortical orientation selectivity, i.
There is an increasing interest in methods for approximate inference in probabilistic (graphical) models.
Kernel methods have been widely used to extend the applicability of many well-known algorithms, such as the Perceptron [1], Support Vector Machines [6], or Principal Component Analysis [15].
We consider the approximation, from a training sample, of real-valued functions, a task variously referred to as prediction, regression, interpolation or function approximation.
While kernel methods have proven to be successful in many batch settings (Support Vector Machines, Gaussian Processes, Regularization Networks) the extension to online methods has proven to provide some unsolved challenges.
Many applications in signal or image processing are hierarchical in the sense that a probabilistic model is built on top of variables that are the coefficients of some feature extraction technique.
The topic of learning in multi-agent environments has received increasing attention over the past several years.
Given a collection  of training data 	   , techniques such as linear SVMs [13] and PCA extract features from  by computing linear functions of this data.
Analog computational arrays [1, 2, 3, 4] for neural information processing offer very large integration density and throughput as needed for real-time tasks in computer vision and pattern recognition [5].
Recent years have seen the development and successful application of several latent factor models for discrete data.
The Hierarchical HMM [FST98] is an extension of the HMM that is designed to model domains with hierarchical structure, e.
Human motion detection and labeling is a very important but difficult problem in computer vision.
For many tasks of exploraty data analysis the Self-Organizing Maps (SOM), as introduced by Kohonen more than a decade ago, have become a widely used tool [1, 2].
It is well known that correct choices of hyperparameters in classification and regression tasks can optimize the complexity of the data model , and hence achieve the best generalization [1].
In graphical models, Bayesian belief propagation (BBP) algorithms often (but not always) yield reasonable estimates of the marginal probabilities at each node [6].
There has in the last few years been a good deal of excitement about the use of Gaussian processes (GPs) as an alternative to feedforward networks [1].
The goal of machine learning is to obtain a certain input/output functional relationship from a set of training examples.
In everyday life, humans can easily estimate body part locations (body pose) from relatively low-resolution images of the projected 3D world (e.
We discuss our continuing work in developing a computer system that plays the role of a musical accompanist in a piece of non-improvisatory music for soloist and accompaniment.
In a typical environment stimuli occur at various positions in space and time.
As the available bandwidth for wireless devices increases, new challenges are presented in the utilisation of such bandwidth.
Independent component analysis (ICA) is a statistical modelling technique which has attracted a significant amount of research interest in recent years (for a review, see Hyvärinen, 1999).
Several recent papers in statistics and machine learning have been devoted to the relationship between boosting and more standard statistical procedures such as logistic regression.
Gaussian Processes [Williams & Rasmussen, 1996] have proven to be a powerful tool for regression.
Hebbian learning rules modify weights of synapses according to correlations between activity at the input and the output of neurons.
Reinforcement learning (RL) is a way of learning how to behave based on delayed reward signals [12].
There is a lot of interest in designing discrete time dynamical systems for inference and learning (see, for example, [10], [3], [7], [13]).
It appears that in constructing the motor commands to guide the arm toward a target, the brain relies on an internal model (IM) of the dynamics of the task that it learns through practice [1].
Consider a collection of N data points Xi E ]RD.
The online analysis of single-trial electroencephalogram (EEG) measurements is a challenge for signal processing and machine learning.
The binding problem as it is classically conceived arises when different pieces of information are processed by entirely separate units.
Classical models for the direction selectivity in the primary visual cortex have assumed feed-forward mechanisms, like multiplication or gating of afferent thalamo-cortical inputs (e.
Assume that we model a complex decision-making problem under uncertainty by a finite MDP.
Digital music is becoming very widespread, as personal collections of music grow to thousands of songs.
	    In standard classification problems, we are given a set of training data , ,    , where the output is qualitative and assumes values in a finite set  .
There is a long history of research on neural networks inspired by the structure of visual cortex whose functions have been described as contour completion, saliency enhancement, orientation sharpening, or segmentation[6, 7, 8, 9, 12].
Sequential binding is a version of the binding problem requiring that the identity of an item and its position within a sequence be bound.
Support Vector Machines (SVMs) [6] are a very robust methodology for inference with minimal parameter choices.
Phase unwrapping is an easily stated, fundamental problem in image processing (Ghiglia and Pritt 1998).
Linear least squares function approximators offer many advantages in the context of reinforcement learning.
In a classification problem, we are given J classes and l training observations.
Kernel based learning provides a modular approach to learning system design [2].
Methods of data clustering are usually based on geometric or probabilistic considerations [1, 2, 3].
The multiple-instance (MI) learning model has received much attention.
Many problems in machine learning require the classifier to work with a set of discrete examples.
Both the difficulty and the fascination of the motor coordination problem lie in the apparent conflict between two fundamental properties of the motor system: the ability to accomplish its goal reliably and repeatedly, and the fact that it does so with variable movements [1].
A series of models [1, 2, 3, 4, 5] based on temporal-difference (TD) learning [6] has explained most responses of primate dopamine (DA) neurons during conditioning [7] as an error signal for predicting reward, and has also identified the DA system as a substrate for conditioning behavior [8].
Recently, Support Vector Machines (SVMs) [2, 11, 9] have attracted much interest in the machine-learning community and are considered state of the art for classification and regression problems.
Novelty detection is an important unsupervised learning problem in which test data are to be judged as having been generated from the same or a different process as that which generated the training data.
High-dimensional data sets have several unfortunate properties that make them hard to analyze.
 	 In framework of classification the ultimate goal of a classifier    the Bayesian  is to minimize the expected risk of misclassification which  measured by  denotes the loss for assigning a given feature vector to class , while it actually belongs to   !" class , with being the number of classes.
Let the m x n matrix X contain an independent sample from some unknown distribution.
It has long been recognized that feature extraction and feature selection are important problems in statistical learning.
Inference in most interesting machine learning algorithms is not computationally tractable, and is solved using approximations.
The number of possible images of an object or scene, even when taken from a single viewpoint with a fixed camera, is very large.
The area of information retrieval (IR) studies the representation, organization and access of information in an information repository.
Implementing machine-learning algorithms in VLSI is a logical step toward enabling realtime or mobile applications of these algorithms [1].
The identification of motif structures in biopolymer sequences such as proteins and DNA is an important task in computational biology and is essential in advancing our knowledge about biological systems.
There has been considerable recent interest in representational and algorithmic issues arising in multi-player game theory.
The functional role of simple and complex cells has puzzled scientists since their response properties were first mapped by Hubel and Wiesel in the 1950s (see, e.
While researchers have developed and successfully applied a myriad of boosting algorithms for classification and regression problems, boosting for density estimation has received relatively scant attention.
It was recently shown that the probability distribution of a belief network can be represented using a multi–linear function, and that most probabilistic queries of interest can be retriev ed directly from the partial deriv ativ es of this function [2].
The conventional role of analog circuits in mixed-signal VLSI is providing the I/O interface to the digital core of the chip —which realizes all the signal processing.
In [1], while attempting to better understand and bridge the gap between the good performance of the popular Support Vector Machines and the more traditional K-NN (K Nearest Neighbors) for classification problems, we had suggested a modified Nearest-Neighbor algorithm.
Increasing interest has been given to the use of overcomplete representations for natural scenes, where the number of basis functions exceeds the number of image pixels.
It is fair to say that difficulties with existing algorithms have so far precluded supervised training techniques for recurrent neural networks (RNNs) from widespread use.
Variations on “name that song”-types of games are popular on radio programs.
Most artificial neural network algorithms based on Hebbian learning use correlations of mean rate signals to increase the synaptic efficacies between connected neurons.
Recent results have demonstrated the feasibility of direct neural control of devices such as computer cursors using implanted electrodes [5, 9, 11, 14].
The widespread application of reinforcement learning is hindered by excessive cost in terms of one or more of representational resources, computation time, or amount of training data.
Many information-science studies are very similar to those of statistical physics.
Many modern classification problems are rife with unlabeled examples.
It has been suggested that the brain’s impressive functionality results from massively parallel processing using simple and efficient computational elements [1].
The problem of constructing a regression model can be posed as maximizing the minimum probability of future predictions being within some bound of the true regression function.
Reinforcement learning[8] is widely studied because of its promise to automatically generate controllers for difficult tasks from attempts to do the task.
In their natural environment, animals encounter highly complex, dynamically changing stimuli.
The Markov game framework has received increased attention as a rigorous model for defining and determining optimal behavior in multiagent systems.
One of the main objectives in time series analysis is forecasting and in many real life problems, one has to predict ahead in time, up to a certain time horizon (sometimes called lead time or prediction horizon).
Supervised and unsupervised learning problems have been extensively studied in the machine learning literature.
Cortical synaptic plasticity agrees with the Hebbian learning principle: Neurons that fire together, wire together.
With relatively few exceptions, relationships between development and learning have largely been ignored by the neural computation community.
A number of recent studies have suggested that spectrotemporal receptive field (STRF) models [1, 2], which are linear in the stimulus spectrogram, can describe the spiking responses of auditory cortical neurons quite well [3, 4].
Independent component analysis (ICA) is a popular enhancement over principal component analysis (PCA) and factor analysis.
Short-term synaptic dynamics have been observed in many parts of the cortical system [Stratford et al.
Natural sounds can be meaningfully represented as a superposition of translated and frequency-modulated versions of simple functions (atoms).
Currently active quantitative models of human causal judgment for single (and sometimes multiple) causes include conditional j}JJ [8], power PC [1], and Bayesian network structure learning [4], [9].
One of the major challenges in vision is how to derive from the retinal representation higher-order representations that describe properties of surfaces, objects, and scenes.
In this paper we discuss the problem of nonlinear dimensionality reduction (NLDR): the task of recovering meaningful low-dimensional structures hidden in high-dimensional data.
Particularly since the analysis of subject HM,1 the suggestion that human memories would consolidate,2 has gripped experimental and theoretical communities.
Models of individual neurons range from simple rate based approaches to spiking models and further detailed descriptions of protein dynamics within the cell[9, 10, 13, 6, 12].
The tangential neurons in the fly brain are known to respond in a directionally selective manner to wide-field motion stimuli.
Several models of neural computation use the precise timing of spikes to encode information.
Clustering is a notion that arises naturally in many fields; whenever one has a heterogeneous set of objects, it is natural to seek methods for grouping them together based on an underlying measure of similarity.
Learning control is formulated in one of the most general forms as learning a control policy u = π(x, t, w) that maps a state x, possibly in a time t dependent way, to an action u; the vector w denotes the adjustable parameters that can be used to optimize the policy.
In many applications such as speech recognition and computational biology, the objects to study and classify are not just fixed-length vectors, but variable-length sequences, or even large sets of alternative sequences and their probabilities.
Neurons exhibit an enormous variety of shapes and molecular compositions.
Bayes’ nets are widely used for data modeling.
The theory of pattern recognition is primarily concerned with the case of binary classification, i.
Feedback loops are prevalent in animal behaviour, where they are normally called a “reflex”.
While a number of different representational approaches to large Markov decision processes (MDPs) have been proposed and studied over recent years, relatively little is known about the relationships between them.
Consider the image patches in Figure 1.
Systems-level neuroscientists have a few favorite problems, the most prominent of which is the "what" part of the neural coding problem: what makes a given neuron in a particular part of the brain fire? In more technical language, we want to know about the conditional probability distributions P(spikelX = x), the probability that our cell emits a spike, given that some observable signal X in the world takes value x.
Multiclass classification is a central problem in machine learning, as applications that require a discrimination among several classes are ubiquitous.
The problem of annotating or segmenting observation sequences arises in many applications across a variety of scientific disciplines, most prominently in natural language processing, speech recognition, and computational biology.
The complexity of classifiers and the difficulty of learning their parameters is affected by the distribution of the input patterns.
The problem of inductive reasoning — in particular, how we can generalize after seeing only one or a few specific examples of a novel concept — has troubled philosophers, psychologists, and computer scientists since the early days of their disciplines.
Microarray technology (DNA chips) is quickly becoming a major data provider in the postgenomics era, enabling the monitoring of the quantity of messenger RNA present in a cell for several thousands genes simultaneously.
Algorithms for redundancy reduction and efficient coding have been the subject of considerable attention in recent years [6, 3, 4, 7, 9, 5, 11].
Many collections of data exhibit a common underlying structure: they consist of a number of parts or factors, each of which has a small number of discrete states.
Choosing suitable kernel functions for estimation using Gaussian Processes and Support Vector Machines is an important step in the inference process.
A statistical modeling of spectral features of speech (acoustic modeling) is one of the most crucial parts in the speech recognition.
Many inference problems in graphical models can be cast as determining the expected value of a random variable of interest,  , given observations drawn according to a target distribution  .
Regularization is essential when learning from finite data sets.
Unsupervised clustering is a central paradigm in data analysis.
Recently the string kernel [6] has been shown to achieve good performance on textcategorisation tasks .
Multiagent learning is a key problem in AI.
A brain-computer interface (BCI) is a system which translates a subject’s intentions into a control signal for a device, e.
In this paper we investigate the problem of inductive learning from the point of view of predicting variables of ordinal scale [3, 7,5], a setting referred to as ranking learning or ordinal regression.
 Let  	   be a jointly distributed pair of random variables.
In pattern recognition, the problem of selecting relevant variables is difficult.
Every image is the product of the characteristics of a scene.
        	 The goal of supervised learning is to predict an unobserved output value based on an observed input vector .
Probabilistic graphical models are powerful tools for learning and reasoning in domains with uncertainty.
Large Partially Observable Markov Decision Processes (POMDPs) are generally very difficult to solve, especially with standard value iteration techniques [2, 3].
The discovery of meaningful patterns and relationships from large amounts of multivariate data is a significant and challenging problem with close ties to the fields of pattern recognition and machine learning, and important applications in the areas of data mining and knowledge discovery in databases (KDD).
The classic notion of the basal ganglia as being involved in purely motor processing has expanded over the years to include sensory and cognitive functions.
Biologically-inspired computation paradigms take different levels of abstraction when modeling neural dynamics.
Partially observable Markov decision processes (POMDPs) provide a rich framework for modeling a wide range of sequential decision problems in the presence of uncertainty.
A fundamental goal of machine learning is to find regular structures in a given empirical data, and use it to construct predictive or comprehensible models.
In partitional clustering, each pattern is represented by a vector of features.
A good image segmentation must single out meaningful structures such as objects from a cluttered scene.
We consider the problem of semi-supervised learning, where one has usually few labeled examples and a lot of unlabeled examples.
One of the fundamental problems of learning theory is model assessment: Given a specific data set, how can one practically measure the generalization performance of a model trained to the data.
Pearl’s belief propagation [1] is a popular algorithm for inference in Bayesian networks.
Many cognitive capacities, such as memory and categorization, can be analyzed as systems for efficiently predicting aspects of an organism's environment [1].
Recommender systems attempt to automate the process of “word of mouth” recommendations within a community.
Automatic dimensionality reduction is an important “toolkit” operation in machine learning, both as a preprocessing step for other algorithms (e.
The set covering machine (SCM) has recently been proposed by Marchand and Shawe-Taylor (2001, 2002) as an alternative to the support vector machine (SVM) when the objective is to obtain a sparse classifier with good generalization.
The task in super-resolution is to combine a set of low resolution images of the same scene in order to obtain a single image of higher resolution.
During the past half century, a wealth of data has been collected on the response properties of cortical sensory neurons.
Citation matching is the problem currently handled by systems such as Citeseer [1].
Many properties of sets of objects can be described by matrices, whose rows and columns correspond to objects and whose elements describe the relationship between them.
Ever since the introduction of the Support Vector Machine (SVM) algorithm, the question of choosing the kernel has been considered as crucial.
This paper introduces a novel algorithm for learning complex binary classifiers by superposition of simpler hyperplane-type discriminants.
Consider a hidden Markov chain.
Dynamic Bayesian Networks are a powerful framework for temporal data models with widespread application in time series analysis[10, 2, 5].
Graphical models are a compact and efficient way of representing a joint probability distribution of a set of variables.
Multi-cellular organisms have solved the problem of efficient transport of nutrients and communication between their body parts by evolving spectacular networks: trees, blood vessels, bronchs, and neuronal arbors.
Reinforcement learning in the context of multi-agent interaction has attracted the attention of researchers in cognitive psychology, experimental economics, machine learning, artificial intelligence, and related fields for quite some time [8, 4].
From olfaction to vision and audition, there is an increasing need, and a growing number of experiments [1]-[8] that study responses of sensory neurons to natural stimuli.
Extracting individual sound sources from an additive mixture of different signals has been attractive to many researchers in computational auditory scene analysis (CASA) [1] and independent component analysis (ICA) [2].
Integer programming problems arise in various fields, including machine learning, statistical physics, communication theory, and error-correcting coding.
Historically, two different classes of statistical model have been used for natural images.
The pattern of connectivity between neurons in the brain is fundamental to understanding the function the brain’s neural networks.
A central problem for cognitive science is to understand the way people mentally represent stimuli.
Systems able to filter out unwanted pieces of information are of crucial importance for several applications.
A lot of learning machines with hidden parts such as multi-layer perceptrons [8], gaussian mixtures[2], Boltzman machines, and Bayesian networks with latent variables [4] are nonidentifiable statistical models.
Gaussian process (GP) models are powerful non-parametric tools for approximate Bayesian inference and learning.
There has been much recent work using probability models to search in optimization problems.
The demand for adaptive learning methods, e.
We investigate a general framework for modeling concurrent actions.
Figure 1a shows a simple image that evokes the percept of transparency.
The use of kernels is of increasing importance in machine learning.
In this article we consider the rather general learning problem of finding a dependency between inputs x E X and outputs y E Y given a training set (Xl,yl), .
Unsupervised grouping or clustering aims at extracting hidden structure from data (see e.
The strongest convergence guarantees for reinforcement learning (RL) algorithms are available for the tabular case, where temporal difference algorithms for both policy evaluation and the general control problem converge with probability one independently of the concrete sampling strategy as long as all states are sampled infinitely often and the learning rate is decreased appropriately [2].
Kernel-based algorithms exploit the information encoded in the inner-products between all pairs of data items (see for example [1]).
The classification of continuous temporal patterns is possible using Hopfield networks with asymmetric weights [2], but classification is restricted to periodic trajectories with a wellknown start and end point.
Some neurological diseases result in the so-called locked-in syndrome.
Variational methods [1, 2] have been used successfully for a wide range of models, and new applications are constantly being explored.
In this paper we describe and evaluate several algorithms to perform sound localization in a commercial entertainment robot.
Boosting is a method for incrementally building linearcombinations of “weak” models, 	 to generate a “strong” predictive model.
Many practical problems involve modeling large, high-dimensional data sets to uncover similarities or latent structure.
The problem of describing a class or a domain has recently gained a lot of attention, since it can be identified in many applications.
In large domains the determination of an optimal value function via a tabular representation is no longer feasible with respect to time and memory considerations.
We consider partitioning a weighted undirected graph— corresponding to a given dataset— into a set of discrete clusters.
The Good-Turing missing mass estimator was developed in the 1940s to estimate the probability that the next item drawn from a fixed distribution will be an item not seen before.
One of the fundamental problems in computational linguistics is adaptation of language processing systems to new languages with minimal reliance on human expertise.
From images, we want to estimate various low-level scene properties such as shape, material, albedo or motion.
Support vector machines (SVMs) currently provide state-of-the-art solutions to many problems in machine learning and statistical pattern recognition[18].
In this paper we consider data which are images containing views of multiple objects.
Multiple-instance learning (MIL) [4] is a generalization of supervised classification in which training class labels are associated with sets of patterns, or bags, instead of individual patterns.
Hidden Markov Models (HMMs) are statistical tools that have been used successfully in the last 30 years to model difficult tasks such as speech recognition [6) or biological sequence analysis [4).
Speech signal carries information about the linguistic message, the speaker, the communication channeL In the previous work [1, 2], we proposed analysis of information in speech as analysis of variability in a set of features extracted from the speech signal.
Most text retrieval or categorization methods depend on exact matches between words.
Neuronal activity in primary motor cortex (MI) during multi-joint arm reaching movements in 2D and 3-D [1, 2] and drawing movements [3] has been used extensively as a test bed for gaining understanding of neural computations in the brain.
An important issue in the Neurosciences is the investigation of the encoding properties of neural populations from their electrophysiological properties such as tuning curves, background noise, and correlations in the firing.
Calcium influx through NMDA receptors is essential for the induction of diverse forms of bidirectional synaptic plasticity, such as rate-based [1, 2] and spike timedependent plasticity (STDP) [3, 4].
Support vector machines (SVMs) have played a major role in classification problems [18,3, 11].
The curse of dimensionality prevents application of dynamic programming to most problems of practical interest.
Though fifty years have passed since the introduction of One Nearest Neighbour (1-NN) [1] it is still a popular algorithm.
Inference in graphical models scales exponentially with the number of variables.
The pitch of the human voice is one of its most easily and rapidly controlled acoustic attributes.
Sequence estimation is at the core of many problems in pattern recognition, most notably speech and language processing.
Good techniques for super-resolution are especially useful where physical limitations exist preventing higher resolution images from being obtained.
Tipping’s relevance vector machine (RVM) both achieves a sparse solution like the support vector machine (SVM) [2, 3] and the probabilistic predictions of Bayesian kernel machines based upon a Gaussian process (GP) priors over functions [4, 5, 6, 7, 8].
Much of the early work on computer vision applied to facial expressions focused on recognizing a few prototypical expressions of emotion produced on command (e.
Tracking human motion is an integral part of many proposed human-computer interfaces, surveillance and identification systems, as well as animation and virtual reality systems.
Real-time monitoring is important in many areas such as robot navigation or diagnosis of complex systems [1, 2].
The standard approach to Computational Learning Theory is usually formulated within the so-called frequentist approach to Statistics.
The performance of many learning and datamining algorithms depend critically on their being given a good metric over the input space.
Nonlinear classification of high dimensional data is a challenging problem.
A common experimental approach to the measurement of the stimulus-response function (SRF) of sensory neurons, particularly in the visual and auditory modalities, is “reverse correlation” and its related non-linear extensions [1].
Clustering is widely used in exploratory analysis for various kinds of data [6].
Gaussian process regression (GPR) has demonstrated excellent performance in a number of applications.
We recently developed Lifelong Planning A* (LPA*), a search algorithm for deterministic domains that combines incremental and heuristic search to reduce its search time [1].
Over recent years there has been a considerable amount of interest in kernel methods for supervised learning (e.
Data density estimation is an important step in many machine learning problems.
Consider the two cars in Figure 1.
People are remarkably good at inferring the causal structure of a system from observations of its behavior.
Recently, as the number of online documents has been rapidly increasing, automatic text categorization is becoming a more important and fundamental task in information retrieval and text mining.
In recent years, methods for reinforcement learning control based on approximating value functions have come under fire for their poor, or poorly-understood, convergence properties.
The £eld of astronomy is increasingly data-driven as new observing instruments permit the rapid collection of massive archives of sky image data.
Due to their sample-based representation, particle filters are well suited to estimate the state of non-linear dynamic systems.
This paper analyzes a class of optimization problems max G(q) + βD(q) q∈∆ (1) where ∆ is a linear constraint space, G and D are continuous, real valued functions of q, smooth in the interior of ∆, and maxq∈∆ G(q) is known.
Rheumatoid arthritis (RA) is the most common inflammatory arthropathy with 1–2% of the population being affected.
In many practical applications of data classification and data mining , one finds a wealth of easily available unlabeled examples , while collecting labeled examples can be costly and time-consuming .
Dimensionality reduction techniques such as principal component analysis [7] and factor analysis [1] linearly map high dimensional data samples onto points in a lower dimensional subspace.
Most text categorization systems use simple models of documents and document collections.
Syntactic structure has standardly been described in terms of categories (phrasal labels and word classes), with little mention of particular words.
The application of tools from Statistical Mechanics to analyzing the average case performance of learning algorithms has a long tradition in the Neural Computing and Machine Learning community [1, 2].
Diverse real-time information processing tasks are carried out by neural microcircuits in the cerebral cortex whose anatomical and physiological structure is quite similar in many brain areas and species.
   Classification is the task of associating a single label with a covariate .
Animals negotiating rich environments are faced with a set of hugely complex inference and learning problems, involving many forms of variability.
A large body of evidence suggests that the development of the retinogeniculocortical pathway, which leads in higher vertebrates to the emergence of eye-specific laminae in the lateral geniculate nucleus (LGN), the formation of ocular dominance columns (ODCs) in the striate cortex and the establishment of retinotopic representations in both structures, is a competitive, activity-dependent process (see Ref.
A fundamental problem in computational biology is the classification of proteins into functional and structural classes based on homology (evolutionary similarity) of protein sequence data.
The likelihood function of a mixture model often has a complex shape so that calculation of an estimator can be difficult, whether the maximum likelihood or Bayesian approach is used.
In many applications which involve modelling an unknown system  	  from observed data, model accuracy could be improved by using not only observations of  , but also observations of derivatives e.
Adaptation is one of the first facts with which neophyte neuroscientists and psychologists are presented.
Caching is ubiquitous in operating systems.
The focus of this paper is supervised learning from multiclass data.
'Danlage to the hippocampus (and nearby regions), often caused by lesiclns leaves normal cognitive function intact in the short term.
Clustering or grouping data is an important topic in machine learning and pattern recognition research.
A remarkable feature of the brain is its ability to adapt to, and learn from, experience.
Curve data arises naturally in a variety of applications.
We are interested in the on-line version of the problem of probability forecasting: we observe pairs of objects and labels sequentially, and after observing the nth object xn the goal is to give a probability measure pn for its label; as soon as pn is output, the label yn of xn is disclosed and can be used for computing future probability forecasts.
Donald Hebb [1] postulated half a century ago that the change of synaptic strength depends on the correlation of pre- and postsynaptic activity: cells which fire together wire together.
Common analytical tools of computational complexity theory cannot be applied to recurrent circuits with complex dynamic components, such as biologically realistic neuron models and dynamic synapses.
Using filters for image processing can be understood as a supervised learning method for classification and segmentation of certain image elements.
The EM (expectation-maximization) algorithm [1, 2] is a popular method for maximum likelihood learning in probabilistic models with hidden variables.
Belief propagation (BP) has become an important tool for approximate inference on graphs with cycles.
Consider a dynamical model in which a sequence of hidden states, x = (x0 , .
Learning in a single-agent stationary-environment setting can be a hard problem, but relative to the multi-agent learning problem, it is easy.
This paper introduces a new framework for understanding the scheduling of human eye movements.
Consider a sample space X and a measure λ on X (with respect to some σ-field).
Standard approaches to object detection (e.
The methodology of variational inference has developed rapidly in recent years, with increasingly rich classes of approximation being considered (see, e.
Quickly generating generating usable plans when the world abounds with uncertainty is an important and difficult enterprise.
In this paper, we are interested in data that lies on or close to a low dimensional manifold embedded, possibly non-linearly, in a Euclidean space of much higher dimension.
The reliable communication of information over noisy channels is a widespread issue, ranging from the construction of good error-correcting codes to feature extraction[3, 12].
Considerable progress has been made in the field of approximate inference using techniques such as variational methods [7], Monte-Carlo methods [5], mini-bucket elimination [4] and belief propagation (BP) [6].
The accurate perception of the relative depth of objects enables both biological organisms and artificial autonomous systems to interact successfully with their environment.
Given a probability distribution defined by a graphical model (e.
Clustering is one of the most widespread methods of data analysis and embodies strong intuitions about the world: Many different acoustic waveforms stand for the same word, many different images correspond to the same object, etc.
We focus on the online learning framework in which the learner has access to a set of experts but possesses no other a priori information relating to the observation sequence.
Inspired by events ranging from 9/11 to the collapse of the accounting firm Arthur Andersen, economists Kunreuther and Heal [5] recently introduced an interesting game-theoretic model for problems of interdependent security (IDS), in which a large number of players must make individual investment decisions related to security — whether physical, financial, medical, or some other type — but in which the ultimate safety of each participant may depend in a complex way on the actions of the entire population.
The estimation of a model underlying the production of noisy data becomes highly nontrivial when there exists more than one equally plausible model that could be responsible for the output data.
Synapses are a critical element in spike-based neural computation.
Traditionally, Hebbian learning algorithms have interpreted Hebb’s postulate in terms of coincidence detection.
We consider a finite-state and finite-action Markov decision problem in which the transition probabilities themselves are uncertain, and seek a robust decision for it.
Many statistical learning problems involve some form of dimensionality reduction.
What is it possible to discover from behind the interface of an unknown body, embedded in an unknown world? In previous work [4] we presented an algorithm that can deduce the dimensionality of the outside space in which it is embedded, by making random movements and studying the intrinsic properties of the relation linking outgoing motor orders to resulting changes of sensory inputs (the so called sensorimotor law [3]).
The simplest and most widely employed technique to reduce the dimensionality of a data distribution is to linearly project it onto the subspace of highest variation (principal components analysis or PCA).
As interests in interfacing electronic circuits to biological cells grows, an intelligent embedded system able to classify noisy and drifting biomedical signals becomes important to extract useful information at the bio-electrical interface.
Blur is one of the most common forms of image distortion.
Factor models are often natural in the analysis of multi-dimensional data.
People are extremely good at finding structure embedded in noise.
Connectionist models are popular in some areas of cognitive science, especially language processing.
Figure 1 illustrates the classical structure from motion (SFM) displays introduced by Ullman [13].
The fast Fourier transform is a fundamental component in any numerical toolbox.
Recent approaches to person detection and tracking exploit articulated body models in which the body is viewed as a kinematic tree in 2D [14], 2.
The goal of a binary classifier is to maximize the probability that unseen test data will be classified correctly.
The last decade has seen tremendous technological advances in neuroscience from the microscopic to the macroscopic scale (e.
In an archetypical regression situation, we are presented with a collection of N regressor/target pairs {φi ∈ <M , ti ∈ <}N i=1 and the goal is to find a vector of weights w such that, in some sense, ti ≈ φTi w, ∀i or t ≈ Φw, (1) T where t , [t1 , .
Hidden Markov Models (HMMs) are used in a wide variety of applications where a sequence of observable events is correlated with or caused by a sequence of unobservable underlying states (e.
This paper describes the navigation software of a deployed robotic system for mapping subterranean spaces such as abandoned mines.
Nonparametric models have become increasingly popular in the statistics communities and probabilistic AI communities.
The boosting method has become a powerful paradigm of machine learning.
In recent years, the design of cooperative multi-robot systems has become a highly active research area within robotics [1, 2, 3, 4, 5, 6].
This paper discusses supervised learning of label rankings – the task of associating instances with a total order over a predefined set of labels.
Recent years have witnessed a rapid development of multiagent learning theory.
Most theories of classical conditioning, exemplified by the classic model of Rescorla and Wagner [7], are wholly concerned with parameter learning.
In neurophysiology, the traditional approach to discover unknown information processing mechanisms is to compare neuronal activities with external variables, such as sensory stimuli or motor output.
Distance metrics are an essential component in many applications ranging from supervised learning and clustering to product recommendations and document browsing.
In previous work [1, 2] we developed a simple neural network algorithm that learned categories from co-occurences of patterns to different sensory modalities.
Spectral clustering has many applications in machine learning, exploratory data analysis, computer vision and speech processing.
Mechanism design (MD) is a subfield of economics that seeks to implement particular outcomes in systems of rational agents [1].
Human vision and neural systems possess very strong capabilities of identifying salient structures from various images.
Transcranial magnetic stimulation (TMS) is an experimental tool for stimulating neurons via brief magnetic pulses delivered by a coil placed on the scalp.
Convexity has played an increasingly important role in machine learning in recent years, echoing its growing prominence throughout applied mathematics (Boyd and Vandenberghe, 2003).
One of the central problems of pattern recognition is the exploitation of known invariances in the pattern domain.
The last decade brought us tremendous improvements in the performance and price of mass storage devices and network systems.
Hybrid neural-symbolic systems concern the use of problem-specific symbolic knowledge within the neurocomputing paradigm (d'Avila Garcez et al.
Historically, librarians have retrieved images by first manually annotating them with keywords.
The advent of functional Magnetic Resonance Imaging (fMRI) has made it possible to safely, non-invasively observe correlates of neural activity across the entire human brain at high spatial resolution.
We consider the problem of text-independent speaker verification.
Many real world domains are richly structured, involving entities of multiple types that are related to each other through a network of different types of links.
Our objective function to be minimized for optimizing the n-dimensional parameis  the sum over all the d data of squared ter vector „ of an F -output NN  model F 2 1 r = rk 22 .
A popular approach to artificial intelligence is to model an agent and its interaction with its environment through actions, perceptions, and rewards [10].
Microarray technology allows researchers to simultaneously measure expression levels associated with thousands or ten thousands of genes in a single experiment (e.
Generative classifiers learn a model of the joint probability, p(x, y), of the inputs x and the label y, and make their predictions by using Bayes rule to calculate p(y|x), and then picking the most likely label y.
In standard two-class classi£cation problems, we are given a set of training data (x 1 , y1 ), .
Gaussian processes (GPs) have been used successfully for regression and classification tasks.
Kernel-based methods are widely being used for data modeling and prediction because of their conceptual simplicity and outstanding performance on many real-world tasks.
Fast and robust face detection is an important computer vision problem with applications to surveillance, multimedia processing, and HCI.
Statistical pattern recognition is often based on Bayes decision theory [4], which aims to achieve minimum error rate classification.
It would be useful to have a system that could take large volumes of video data of people engaged in everyday activities and produce annotations of that data with statements about the activities of the actors.
Categorizing objects from their shape is an unsolved problem in computer vision that entails the ability of a computer system to represent and generalize shape information on the basis of a finite amount of prior data.
In statistical pattern recognition, the high-dimensionality is a major cause of the practical limitations of many pattern recognition technologies.
The question of how agents may adapt their strategic behavior while interacting with other arbitrarily adapting agents is a major challenge in both machine learning and multi-agent systems research.
Bayesian kernel methods, including the popular Relevance Vector Machine (RVM) [10, 11], have proved to be effective tools for regression and classification.
Real world motor tasks are inherently uncertain.
In supervised classification, our goal is to classify instances into some set of discrete categories.
Many animals – from flies to humans – are capable of visually detecting imminent collisions caused either by a rapidly approaching object or self-motion towards an obstacle.
Since the first results of Vapnik and Chervonenkis on uniform laws of large numbers for classes of {0, 1}-valued functions, there has been a considerable amount of work aiming at obtaining generalizations and refinements of these bounds.
Image denoising is an important subfield of computer vision, which has extensively been studied (e.
Speculation and data that the rules governing synaptic plasticity should include a very special role for timel,7,13,17,20,21,23,24,26,27,31,32,3S was spectacularly confirmed by a set of highly influential experiments4·S,ll,l6, 25 showing that the precise relative timing of pre-synaptic and post-synaptic action potentials governs the magnitude and sign of the resulting plasticity.
The aim of super-resolution is to take a set of one or more low-resolution input images of a scene, and estimate a higher-resolution image.
Markov decision processes (MDPs) offer an elegant mathematical framework for representing and solving decision problems in the presence of uncertainty.
We consider the general problem of learning from labeled and unlabeled data.
Experts algorithms.
Extracting relevant aspects of complex data is a fundamental task in machine learning and statistics.
This paper is motivated by the potential of policy gradient methods as a general approach to designing simple scalable distributed optimization protocols for networks of electronic devices.
Pyramidal cells in the CA3 and CA1 regions of the rat hippocampus have shown to be selectively active depending on the animal’s position within an environment[1].
The inference of hidden graphs from noisy edge appearance data is an important problem with obvious practical application.
AdaBoost is an algorithm for constructing a “strong” classifier using only a training set and a “weak” learning algorithm.
An important problem in approximate inference is improving the performance of belief propagation on loopy graphs.
In many applications of machine learning, it is inherently difficult or prohibitively expensive to generate large amounts of labeled training data.
Planning of behavior requires some knowledge about the consequences of actions in a given environment.
Legged locomotion is a system level behavior that engages most senses and activates most muscles in the human body.
Computer animated agents and robots bring a social dimension to human computer interaction and force us to think in new ways about how computers could be used in daily life.
The general transduction task is the following: given a training set of labelled data, and a working set of unlabelled data (also called transduction samples), estimate the value of a classification function at the given points in the working set.
Recently, there has been a great deal of interest in using the activity from a population of neurons to predict or reconstruct the sensory input [1, 2], motor output [3, 4] or the trajectory of movement of an animal in space [5].
One way to model the density of high-dimensional data is to use a set of parameters, Θ to deterministically assign an energy, E(x|Θ) to each possible datavector, x [2].
The brain’s neuronal activity generates weak magnetic fields (10 fT – 1 pT).
The task of selecting relevant features in classification problems can be viewed as one of the most fundamental problems in the field of machine learning.
In rating-based collaborative filtering, users express their preferences by explicitly assigning ratings to items that they have accessed, viewed, or purchased.
A number of data analysis methods are based on the spectral decomposition of large matrices.
Many unsupervised learning algorithms have been recently proposed, all using an eigendecomposition for obtaining a lower-dimensional embedding of data lying on a non-linear manifold: Local Linear Embedding (LLE) (Roweis and Saul, 2000), Isomap (Tenenbaum, de Silva and Langford, 2000) and Laplacian Eigenmaps (Belkin and Niyogi, 2003).
Sparse representation or sparse coding of signals has received a great deal of attention in recent years.
Learning in artificial systems can be formulated as optimization of an objective function which quantifies the system’s performance.
This paper examines a general problem: given a sparse graph of similarities between a set of objects, quickly assign each object a location in a low-dimensional Euclidean space.
No software is perfect, and debugging is a resource-consuming process.
Autonomous agents offer a wide range of possibilities to apply and test machine learning algorithms, for example in vision, locomotion, and localisation.
A central tenet of generative linguistics is that extensive innate knowledge of grammar is essential to explain the acquisition of language from positive-only data [1, 2].
Implementing language acquisition systems is one of the most diﬃcult problems, since not only the complexity of the syntactical structure, but also the diversity in the domain of meaning make this problem complicated and intractable.
Suppose we have a collection of data points of n-dimensional real vectors drawn from an unknown probability distribution.
Consider the set of points shown in figure 1a.
A predictive state representation, or PSR, captures the state of a controlled dynamical system not as a memory of past observations (as do history-window approaches), nor as a distribution over hidden states (as do partially observable Markov decision process or POMDP approaches), but as predictions for a set of tests that can be done on the system.
Efron’s bootstrap is a powerful tool for estimating various properties of a given statistic, most commonly its bias and variance (cf.
Finite state controllers (FSCs) provide a simple, convenient way of representing policies for partially observable Markov decision processes (POMDPs).
A hierarchical clustering of n data points is a recursive partitioning of the data into 2, 3, 4, .
Working with a time-frequency representation of speech can have many advantages over processing the raw amplitude samples of the signal directly.
The standard measure of accuracy for trained models is the prediction error (PE), i.
The development of new drugs by the pharmaceutical industry is a costly and lengthy process, with the time from concept to final product typically lasting ten years.
Normal visual acuity is limited by the photoreceptor distance on the retina to about 10 of visual angle, which is imposed by the neural nyquist sampling limit.
Color correction is an important preprocessing step for robust color-based computer vision algorithms.
During the last years Support Vector Machines (SVM’s) [1] have become extremely successful discriminative approaches to pattern classification and regression problems.
In this paper we describe and analyze several learning tasks through the same algorithmic prism.
We denote by Hk the RKHS associated with the kernel k(x, y) = φ(x)> φ(y), where φ(x) : X → Hk is a possible nonlinear mapping from input space X (assumed to be a nonempty set) to the possible infinite dimensional space Hk .
Semidefinite programming (SDP) is one of the most active research areas in optimisation.
Detecting curves in images is a fundamental visual task which requires combining local intensity cues with prior knowledge about the probable shape of the curve.
An important goal in image understanding is to detect, track and label objects of interest present in observed images.
The barn owl shows great accuracy in localizing sound sources using only auditory information [1].
A Gaussian process (GP) is an extremely concise and simple way of placing a prior on functions.
Environmental observation and forecasting systems (EOFS) gather, process, and deliver environmental information to facilitate sustainable development of natural resources.
The now commonplace ability to accurately and inexpensively log the activity of individuals in a digital environment makes available a variety of traces of user activity and with it the necessity to develop efficient representations, or profiles, of individuals.
People have remarkable abilities to learn concepts from very limited data, often just one or a few labeled examples per class.
Inference via Bayesian estimation can lead to optimization problems over rather large data sets.
Optimal search is often infeasible for real world problems, as we are given a limited amount of time for deliberation and want to find the best solution given the time provided.
The Google search engine [2] accomplishes web page ranking using PageRank algorithm, which exploits the global, rather than local, hyperlink structure of the web [1].
One of the major concerns in probabilistic reasoning using graphical models, such as Bayesian networks, is the computational complexity of inference.
Gaussian mixture densities are widely used to model complex, multimodal relationships.
One simple model of experimental design (we have neurophysiological experiments in mind, but our results are general with respect to the identity of the system under study) is as follows.
The goal of MEG/EEG localization is to identify and measure the signals emitted by electrically active brain regions.
Given a training set T = ((x1 , y1 ), .
Many quantum neural networks have been proposed [1], but very few of these proposals have attempted to provide an in-depth method of training them.
Although numerous studies have been undertaken on robust automatic speech recognition (ASR) in the real world, long reverberation is still a serious problem that severely degrades the ASR performance [1].
Model-based control of discrete-time non-linear dynamical systems is typically exacerbated by the existence of multiple relevant time scales: a short time scale (the sampling time) on which the controller makes decisions and where the dynamics are simple enough to be conveniently captured by a model learning from observations, and a longer time scale which captures the long-term consequences of control actions.
Variational and related mean field techniques have attracted much interest as methods for performing approximate Bayesian inference, see e.
Recent developments in kernel technology enable us to handle discrete structures, such as sequences, trees, and graphs.
Planning under uncertainty is a central problem in the field of robotics as well as many other AI applications.
Over the past few years, many machine learning methods have been successfully applied to Natural Language tasks in which phrases of some type have to be recognized.
We are used to thinking about learning from labels as supervised learning, and learning without labels as unsupervised learning, where ’supervised’ implies the need for human intervention.
Most existing learning algorithms perform a trade-off between fit of the data and ’complexity’ of the solution.
Ample evidence indicates that the maintenance of information in working memory (WM) is mediated by persistent neural activity in the prefrontal cortex (PFC) [9, 10].
A central problem in computational biology is the classification of proteins into functional and structural classes given their amino acid sequences.
The goal of a Brain-Computer Interface (BCI) is to establish a communication channel for translating human intentions – reflected by suitable brain signals – into a control signal for, e.
We can generally think of a non-rigid object’s motion as consisting of a rigid component plus a non-rigid deformation.
Complex probabilistic models are increasingly prevalent in domains such as bioinformatics, information retrieval, and vision.
Perceiving and analyzing human motion is a natural and useful task for our visual system.
Helicopters represent a challenging control problem with high-dimensional, complex, asymmetric, noisy, non-linear, dynamics, and are widely regarded as significantly more difficult to control than fixed-wing aircraft.
Cellular networks form the basis of modern wireless communication infrastructure.
The bootstrap method [1, 2] is a widely applicable approach to assess the expected qualities of statistical estimators and predictors.
The complete description of the morphology and synaptic connectivity of all 302 neurons in the nematode Caenorhabditis elegans [15] raised the prospect of the first comprehensive understanding of the neuronal basis of an animal’s entire behavioral repertoire.
This paper develops fast methods for detection of spatial overdensities: discovery of spatial regions with high scores according to some density measure, and statistical significance testing in order to determine whether these high-density regions can reasonably have occurred by chance.
The multi-class classification problem refers to assigning each of the observations into one of k classes.
Prediction, smoothing and filtering are traditional tasks applied to time series.
Visualisation of high dimensional data can be achieved through projecting a data-set onto a lower dimensional manifold.
Dynamic-programming approaches to finding optimal control policies in Markov decision processes (MDPs) [4, 14] using explicit (flat) state space representations break down when the state space becomes extremely large.
Policy search approaches to reinforcement learning represent a promising method for solving POMDPs and large MDPs.
Assume we have a classi£cation “learning” sample {x i , yi }ni=1 with yi ∈ {−1, +1}.
Potentially cancerous tissue samples are analyzed by staining them with a combination of two or more dyes.
Cells in the primary visual cortex perform nonlinear operations on visual stimuli.
In recent years, there has been a lot of interest in the study of kernel methods [1].
Message passing on junction trees is an efficient means of solving many probabilistic inference problems [1, 2].
We previously studied dyadic classification trees, equipped with simple binary decision rules at each leaf, in [1].
In a recent article in Nature [4], Lee and Seung proposed the notion of non-negative matrix factorization (NMF) as a way to find a set of basis functions for representing non-negative data.
For the analysis of natural images, it is important to use contextual information in the form of spatial dependencies in images.
We begin with an example of a rare-category-detection problem: an astronomer needs to sift through a large set of sky survey images, each of which comes with many numerical parameters.
Recently, the quality of research in Machine Learning has been raised by the sustained data sharing efforts of the community.
The formulation of recognition as a problem of statistical classification has enabled significant progress in the area, over the last decades.
The problem of graph inference, or graph reconstruction, is to predict the presence or absence of edges between a set of points known to form the vertices of a graph, the prediction being based on observations about the points.
We consider the multi-category classification problem, where we want to find a predictor p : X → Y, where X is a set of possible inputs and Y is a discrete set of possible outputs.
Autoassociative memory in recurrently coupled networks seems fondly regarded as having been long since solved, at least from a computational perspective.
The formation of covalent links among cysteine (Cys) residues with disulphide bridges is an important and unique feature of protein folding and structure.
In an online decision problem, an algorithm must choose from among a set of strategies in each of n consecutive trials so as to minimize the total cost of the chosen strategies.
Hidden Markov models (HMMs) and related probabilistic sequence models have been among the most accurate methods used for sequence-based prediction tasks in genomics, natural language processing and other problem domains.
Learning to select actions to achieve goals in a multiagent setting requires overcoming a number of key challenges.
The main challenges of visual tracking can be attributed to the difficulty in handling appearance variability of a target object.
Linear latent variable models such as factor analysis, principal component analysis (PCA) and independent component analysis (ICA) [1] are used in many applications ranging from engineering to social sciences and psychology.
While clustering is usually executed completely unsupervised, there are circumstances in which we have prior belief that pairs of samples should (or should not) be assigned to the same cluster.
We consider a problem of optimal aggregation (see [1]) of a finite set of base classifiers in a complex aggregate classifier.
This paper presents a new approach to feature selection for linear classification © ªnwhere the goal is to learn a decision rule from a training set of pairs Sn = x(i) , y (i) i=1 , where x(i) ∈ Rd are input patterns and y (i) ∈ {−1, 1} are the corresponding labels.
Recently, there have been advances in the machine learning community for developing effective and efficient algorithms for constructing nonlinear low-dimensional manifolds from sample data points embedded in high-dimensional spaces, emphasizing simple algorithmic implementation and avoiding optimization problems prone to local minima.
Clustering is one of the oldest forms of machine learning.
In many pattern classification problems, the acquisition of labelled training data is costly and/or time consuming, whereas unlabelled samples can be obtained easily.
Psychologists distinguish between extrinsic motivation, which means being moved to do something because of some specific rewarding outcome, and intrinsic motivation, which refers to being moved to do something because it is inherently enjoyable.
An important challenge in the problem of classification of high-dimensional data is to design a learning algorithm that can often construct an accurate classifier that depends on the smallest possible number of attributes.
Conditional random fields (CRFs) are a recently-introduced formalism [12] for representing a conditional model Pr(y|x), where both x and y have non-trivial structure (often sequential).
Linear Discriminant Analysis [2, 4] is a well-known scheme for feature extraction and dimension reduction.
Mechanism design (MD) is concerned with the problem of providing incentives to implement desired system-wide outcomes in systems with multiple self-interested agents.
The game of Go originated in China over 4000 years ago.
One of the most powerful techniques in protein analysis is the comparison of a target amino acid sequence with phylogenetically related or homologous proteins.
Rather than constructing particular microcircuit models that carry out particular computations, we pursue in this article a different strategy, which is based on the assumption that the computational function of cortical microcircuits is not fully genetically encoded, but rather emerges through various forms of plasticity (“learning”) in response to the actual distribution of signals that the neural microcircuit receives from its environment.
Reinforcement learning algorithms often face the problem of finding useful complex non-linear features [1].
Suppose we are given a finite sampling (in machine learning terms, training data) x1 , .
There is an inherent tension between the objectives in an expert setting and those in a reinforcement learning setting.
Accurate visual detection and tracking of three–dimensional articulated objects is a challenging problem with applications in human–computer interfaces, motion capture, and scene understanding [1].
The number of Support Vectors (SVs) has a dramatic impact on the efficiency of Support Vector Machines (Vapnik, 1995) during both the learning and prediction stages.
In many real-world application domains, the available training data sets are quite small, which makes learning and model selection difficult.
Modern embedded and portable electronic systems operate in unknown and mutating environments, and use adaptive filtering and machine learning techniques to discover the statistics of the data and continuously optimize their performance.
Semi-supervised learning has been the focus of considerable recent research.
The goal of de novo peptide sequencing is to reconstruct an amino acid sequence from a given mass spectrum.
It is a remarkable fact that pictures and their associated annotations are complementary.
In the area of learning from observations there are two main paths that are often mutually exclusive: (i) the design of learning algorithms, and (ii) the design of data representations.
In machine learning, it is often the case that unlabeled data is substantially cheaper and more plentiful than labeled data, and as a result a number of methods have been developed for using unlabeled data to try to improve performance, e.
Clustering is one of the building blocks of modern data analysis.
Independent component analysis (ICA) is a recently-developed method in the fields of signal processing and artificial neural networks, and has been shown to be quite useful for the blind separation problem [1][2][3] [4].
People's relationships are largely determined by their social interactions, and the nature of their conversations plays a large part in defining those interactions.
Neural networks learn from examples.
A wide range of psychophysical results have recently been successfully explained using Bayesian models [7, 8, 16, 19].
Biological organisms possess an enormous repertoire of genetic responses to everchanging combinations of cellular and environmental signals.
Nearest neighbor (KNN) is an extremely simple yet surprisingly effective method for classification.
The construction of 3D object models is a key task for many graphics applications.
The aim of research into brain-computer interfaces (BCIs) is to allow a person to control a computer using signals from the brain, without the need for any muscular movement—for example, to allow a completely paralysed patient to communicate.
One of the central problems in vision science is to identify the features used by human subjects to classify visual stimuli.
Partially observable Markov decision processes (POMDPs) provide a natural and expressive framework for decision making, but their use in practice has been limited by the lack of scalable solution algorithms.
Linear Discriminant Analysis [3] is a well–known method for dimension reduction.
Articulated object action modeling, tracking and recognition has been an important research issue in computer vision community for decades.
Biological auditory systems perform tasks that require exceptional sensitivity to both spectral and temporal acoustic structure.
This work is motivated by recent advances in object detection algorithms that use a cascade of rejectors to quickly detect objects in images.
There has recently been growing interest in models of causal learning formulated as probabilistic inference [1,2,3,4,5].
This paper presents three adaptivity properties of decision trees that lead to faster rates of convergence for a broad range of pattern classification problems.
Finding a representation for data in a low-dimensional Euclidean space is useful both for visualization and as prelude to other kinds of data analysis.
We introduce a novel methodology for the clustering and prediction of sets of smoothly varying curves while jointly allowing for the learning of sets of continuous curve transformations.
Estimating the time difference of arrival is crucial for binaural acoustic sound source localization[1].
Kernel methods provide efficient tools for nonlinear learning problems such as classification or regression.
Recently there has been great interest in algorithms for constructing low-dimensional feature-space embeddings of high-dimensional data sets.
A typical neuron in visual cortex receives most of its inputs from other visual cortical neurons.
Support Vector Machines [1] are powerful classification and regression tools, but their compute and storage requirements increase rapidly with the number of training vectors, putting many problems of practical interest out of their reach.
In the classical supervised learning classification framework, a decision rule is to be learned from a learning set Ln = {xi , yi }ni=1 , where each example is described by a pattern xi ∈ X and by the supervisor’s response yi ∈ Ω = {ω1 , .
Humans have a remarkable ability of perceiving 3D shape information - even from a single photograph.
The presence of multiple classes in a learning domain introduces interesting tasks besides the one to select the most appropriate class for an object, the well-known (single-label) multiclass problem.
Consider the following problem.
A one-class-classifier attempts to find a separating boundary between a data set and the rest of the feature space.
Loopy Belief Propagation (BP) [1] and its generalizations (such as the Cluster Variation Method [2]) are powerful methods for inference and optimization.
In the intelligent control of large systems, the multi-agent approach has the advantages of parallelism, robustness, scalability, and light communication overhead [1].
Principal component analysis (PCA) is widely used for dimensionality reduction with applications ranging from pattern recognition and time series prediction to visualization.
Our aim is to understand the properties of mixtures of speech signals within a generative statistical framework.
Let (xi , yi )i=1.
Gaussian process regression has many desirable properties, such as ease of obtaining and expressing uncertainty in predictions, the ability to capture a wide variety of behaviour through a simple parameterisation, and a natural Bayesian interpretation [15, 4, 9].
Suppose we are to deal with complex (e.
In recent years support vector machines (SVM’s) have been the subject of many theoretical considerations.
The segmentation of natural images into semantically meaningful subdivisions can be considered as one or more binary pixel classification problems, where two classes of pixels are characterized by some measurement data (features).
For motor response, the brain relies on several modalities.
One of the most common ways to define anomalies is by saying that anomalies are not concentrated (see e.
In many domains—including computer vision, databases and natural language processing—we find multiple views, descriptions, or names for the same underlying object.
There is a long history of research in economics on mathematical models for exchange markets, and the existence and properties of their equilibria.
The key to attaining autonomy in wireless sensory systems is to embed pattern recognition intelligence directly at the sensor interface.
Real-world environments require agents to choose actions sequentially.
Completely paralyzed patients cannot communicate despite intact cognitive functions.
There is broad agreement in the machine vision literature that objects and object categories should be represented as collections of features or parts with distinctive appearance and mutual position [1, 2, 4, 5, 6, 7, 8, 9].
The task of online document clustering is to group documents into clusters as long as they arrive in a temporal sequence.
In many systems dealing with speech or natural language, such as Automatic Speech Recognition and Statistical Machine Translation, a language model is a crucial component for searching in the often prohibitively large hypothesis space.
Neurons in the primate visual system exhibit a sparse distribution of firing rates.
The complexity of inference in graphical models is typically exponential in some parameter of the graph, such as the size of the largest clique.
Sparse signal representations from overcomplete dictionaries find increasing relevance in many application domains [1, 2].
A DA B OOST [1] is one of the machine learning algorithms that have revolutionized pattern recognition technology in the last decade.
The last decade has seen significant progress in online learning algorithms that perform well even in adversarial settings (e.
Many learning problems can be reduced to statistical inference of parameters.
Graphical models and message-passing algorithms defined on graphs are a growing field of research.
A constant stream of noisy and ambiguous sensory inputs bombards our brains, informing on-going inferential processes and directing perceptual decision-making.
A number of approaches and algorithms have been proposed for semi-supervised learning including parametric models [1], random field/walk models [2, 3], or discriminative (kernel based) approaches [4].
Embeddings of objects in a low-dimensional space are an important tool in unsupervised learning and in preprocessing data for supervised learning algorithms.
We consider semi-supervised classification problems on weighted directed graphs, in which some nodes in the graph are labeled as positive or negative, and where the task consists in classifying unlabeled nodes.
The problem that we address in this paper is that of learning object categories from supervised data.
In semi-supervised learning, we assume we have a sample (xi , yi , si )ni=1 , of i.
The problem of building a brain-computer interface (BCI) has received considerable attention in recent years.
Bayesian methods based on Gaussian process priors have recently become quite popular for machine learning tasks (1).
Though proposed more than fifty years ago [1, 7], the effectiveness of the policy improvement algorithm remains a mystery.
There is a consensus in the high-dimensional data analysis community that the only reason any methods work in very high dimensions is that, in fact, the data are not truly high-dimensional.
The problem of bias fields in magnetic resonance (MR) images is an important problem in medical imaging.
Tracking moving objects is an important and essential component of visual perception, and has been an active research topic in computer vision community for decades.
Recent progress in discriminative object detection, especially for faces, has yielded good performance and efficiency [1, 2, 3, 4].
Prediction suffix trees are elegant, effective, and well studied models for tasks such as compression, temporal classification, and probabilistic modeling of sequences (see for instance [13, 11, 7, 10, 2]).
Most machine learning (ML) algorithms assume that given instances are represented in numerical vectors.
In many learning problems, the goal is not simply to classify objects into one of a fixed number of classes; instead, a ranking of objects is desired.
Most learning algorithms have been developed to learn a vector of parameters from data.
The quality of beef meat is appreciated through sensory impressions, and therefore its assessment is very subjective.
Structured classification is the problem of predicting y from x in the case where y has meaningful internal structure.
In recent years, spectral clustering methods, i.
Nonnegative matrix factorization (NMF) is an unsupervised algorithm for learning the parts of complex objects [11].
Given a data sample (xi , yi )ni=1 (with xi ∈ Rp and yi ∈ R for regression, yi ∈ {±1} for classification), the generic regularized optimization problem calls for fitting models to the data while controlling complexity by solving a penalized fitting problem: X (1) C(yi , β 0 xi ) + λJ(β) β̂(λ) = arg min β i where C is a convex loss function and J is a convex model complexity penalty (typically taken to be the lq norm of β, with q ≥ 1).
First-order Markov models have enjoyed numerous successes in many sequence modeling and in many control tasks, and are now a workhorse of machine learning.
In the traditional formulation of supervised learning, we seek a predictor that maps input x to output y.
Principal component analysis (PCA) is a popular tool for data analysis and dimensionality reduction.
The development of tractable approximations for the statistical inference with probabilistic data models is of central importance in order to develop their full potential.
Imagine that a lazy graduate student has been asked to measure the pairwise distances among a group of objects in a metric space.
Statistical language models are essential components of natural language systems for human-computer interaction.
In this paper, we analyze a maximum-entropy procedure for ensemble learning in the online learning model.
Online learning algorithms provide a powerful set of tools for automatically fine-tuning a controller to optimize performance while in operation, or for automatically adapting to the changing dynamics of a control problem.
Our long-term goal is to build a vision system that can examine an image and describe what objects are in it, and where.
A word can appear in a sentence for two reasons: because it serves a syntactic function, or because it provides semantic content.
Using artificial neural networks for problem solving immediately raises the issue of their general trainability and the appropriate learning strategy.
The main goal of learning from examples is to infer an estimator, given a finite sample of data drawn according to a fixed but unknown probabilistic input-output relation.
The estimation of discrete distributions given finite data — “histogram smoothing” — is a canonical problem in statistics and is of fundamental importance in applications to language modeling, informatics, and safari organization (1–3).
Figure 1 shows six cars: the two leftmost cars were captured by one camera; the right four cars were seen later by another camera from a different angle.
Balancing hypothesis-space generality with predictive power is one of the central tasks in inductive learning.
The detection of human faces in natural images and videos is a key component in a wide variety of applications of human-computer interaction, search and indexing, security, and surveillance.
In a recent paper [1] we introduced M ED B OOST, a boosting algorithm that trains base regressors and returns their weighted median as the final regressor.
The Gaussian mixture model (MoG) is a flexible and powerful parametric framework for unsupervised data grouping.
Unlike in the linear case the nonlinear Blind Source Separation (BSS) problem can not be solved solely based on the principle of statistical independence [1, 2].
Consider an agent situated in a partially observable domain: It executes an action; the action may change the state of the world; this change is reflected, in turn, by the agent’s sensors; the action may have some associated cost, and the new state may have some associated reward or penalty.
Neurons transmit information through spikes, but how the information is encoded remains a matter of debate.
Many cellular functions are carried out through physical interactions between proteins.
Kernel based methods, including support vector machines [16], regularization networks [5] and Gaussian processes [18], have attracted much attention in machine learning.
Proximity-based, or pairwise, data clustering techniques are gaining increasing popularity over traditional central grouping techniques, which are centered around the notion of “feature” (see, e.
We describe a framework for learning to accurately discriminate between two target classes of objects (e.
Advances in network technology, like peer-to-peer networks on the Internet or sensor networks, have highlighted the need for efficient ways to deal with large amounts of data that are distributed over a set of nodes.
An increasingly common phenomenon in classification tasks is that unlabeled data is abundant, whereas labels are considerably harder to come by.
Clustering algorithms partition a given data set into several groups based on some notion of similarity between objects.
Many real-world planning problems involve concurrent optimization of a set of prioritized subgoals of the problem by dynamically merging a set of (previously learned) policies optimizing the subgoals.
One of the core goals of modern statistical inference and data mining is to discover patterns and relationships in data.
Recent years have see a combinatorial explosion of results on kernels for structured and semi-structured data, including trees, strings, graphs, transducers and dynamical systems [6, 8, 15, 13].
Over the past few years, there has been a growing interest in linear programming (LP) approaches to approximate dynamic programming (DP).
This paper describes our approach to the beat tracking problem.
A central issue of generalization is how information from the training examples can be used to make predictions about new examples, and without strong prior assumptions, i.
Spike-timing dependent plasticity (STDP) has been intensively studied during the last decade both experimentally and theoretically (for reviews see [1, 2]).
Semantic taxonomies and thesauri such as WordNet [5] are a key source of knowledge for natural language processing applications, and provide structured information about semantic relations between words.
The traditional machine learning classification problem involves a set of input vecT T tors X = [x1 .
Neural spike activity is recorded with a micro-electrode which normally picks up the activity of multiple neurons.
In this paper, we consider learning binary classification tasks with partially labelled data via selective sampling.
Schema learning1 is a data-driven, constructivist approach for discovering probabilistic action models in dynamic controlled systems.
It has been proposed that extensive computational capabilities are achieved by systems whose dynamics is neither chaotic nor ordered but somewhere in between order and chaos.
In many real world applications we wish to learn from data which is not labeled, to find clusters or some structure within the data.
Chorale harmonisation is a traditional part of the theoretical education of Western classical musicians.
Neuroimaging experimenters usually report their results in the form of 3dimensional coordinates in the standardized stereotaxic Talairach system [1].
Random 3-SAT is a classic problem in combinatorics, at the heart of computational complexity studies and a favorite testing ground for both exactly analyzable and heuristic solution methods which are then applied to a wide variety of problems in machine learning and arti£cial intelligence.
It has been shown that support vector machines (SVMs) provide state-of-the-art accuracies in object detection.
Classical conditioning experiments probe how organisms learn to predict significant events such as the receipt of food or shock.
One of the most significant conceptual and practical tools in the Bayesian paradigm is the notion of a hierarchical model.
The paper focuses on the problem of tracking hand movements, which constitute smooth spatial trajectories, from spike trains of a neural population.
“Collaborative filtering” refers to the general task of providing users with information on what items they might like, or dislike, based on their preferences so far and how they relate to the preferences of other users.
The problem of recovering signals from linear mixtures, with only partial knowledge of the mixing process and the signals—a problem often referred to as blind source separation— is a central problem in signal processing.
The k-nearest-neighbor searching problem is to find the k nearest points in a dataset X ⊂ RD containing n points to a query point q ∈ RD , usually under the Euclidean distance.
Experimental studies have observed synaptic potentiation when a presynaptic neuron fires shortly before a postsynaptic neuron, and synaptic depression when the presynaptic neuron fires shortly after.
Conjoint analysis (also called trade-off analysis) is one of the most popular marketing research technique used to determine which features a new product should have, by conjointly measuring consumers trade-offs between discretized1 attributes.
Efficient coding theory posits that one of the primary goals of sensory coding is to eliminate redundancy from raw sensory signals, ideally representing the input by a set of statistically independent features [1].
Denote by (x, y) ∈ X ×Y patterns with corresponding labels.
Graphical models have become the basic framework for generative approaches to probabilistic modelling.
Generative models provide a powerful and intuitive way to analyse images or video sequences.
The Bradley-Terry model [2] for paired comparisons has been broadly applied in many areas such as statistics, sports, and machine learning.
There is rapidly growing interest in multi-agent systems, and in particular in learning algorithms for such systems.
The goal of neuromorphic engineering is to design large-scale sensory information processing systems that emulate the brain.
We have a set of n training pairs xi , yi , where xi ∈ Rp is a p-vector of real valued predictors (attributes) for the ith observation, yi ∈ {−1, +1} codes its binary response.
Fitting a target matrix Y with a low-rank matrix X by minimizing the sum-squared error is a common approach to modeling tabulated data, and can be done explicitly in terms of the singular value decomposition of Y .
This paper addresses the problem of learning a vector–valued function f : X → Y, where X is a set and Y a Hilbert space.
Graph-based algorithms have long been popular, and have received even more attention recently, for two of the fundamental problems in machine learning: clustering [1–4] and manifold learning [5–8].
Consider a network of n nodes Pnin which the ith node observes a number yi ∈ [0, 1] and aims to compute the average i=1 yi /n.
In the multiclass text classification task, we are given a training set of documents, each labeled as belonging to one of K disjoint classes, and a new unlabeled test document.
In [TRW05] various on-line updates were generalized from vector parameters to matrix parameters.
Humans and other primates can perform fast and efficient object recognition.
This paper addresses the problem of data association in online object tracking [6].
We consider (regression) estimation of a function x 7→ u(x) from noisy observations.
Functional Magnetic Resonance Imaging (fMRI) has enabled scientists to look into the active human brain [1] by providing sequences of 3D brain images with intensities representing blood oxygenation level dependent (BOLD) regional activations.
The field of social network analysis (SNA) has developed mathematical models that discover patterns in interactions among entities.
Audio content analysis is an important area in music research.
Semi-supervised learning has received significant attention in machine learning in recent years, see, for example, [2, 3, 4, 8, 9, 16, 17, 18] and references therein.
In many circumstances the human mind has to make decisions when time and knowledge are limited.
Quite demanding real-time computations with fading memory1 can be carried out by generic cortical microcircuit models [1].
Models of the visual system often examine steady-state levels of neural activity during presentations of visual stimuli.
The k-nearest neighbors (kNN) rule [3] is one of the oldest and simplest methods for pattern classification.
The statistical models typically used in unsupervised learning draw upon a relatively small repertoire of representations.
Probabilistic methods have become well-established in the analysis of learning algorithms over the past decade, drawing largely on classical Gaussian statistical theory [21, 2, 28].
The cerebellum is involved in various types of motor learning.
The usual tradeoff in parameter estimation for single neuron models is between realism and tractability.
Consider the model fn (x) = n n 1 X 1 X vj h(x; uj ) ≡ vj hj (x) sn j=1 sn j=1 (1) which can be viewed as a multi-layer perceptron with input x, hidden functions h, weights uj , output weights vj , and sn a sequence of normalizing constants.
Policy gradient reinforcement learning (RL) methods train controllers by estimating the gradient of a long-term reward measure with respect to the parameters of the controller [1].
Clustering has been widely applied in data analysis to group similar objects.
Images, when represented as a collection of pixel values, exhibit a high degree of redundancy.
The goal of active learning is to learn a classifier in a setting where data comes unlabeled, and any labels must be explicitly requested and paid for.
In many supervised learning tasks the input is represented by a very large number of features, many of which are not needed for predicting the labels.
Building and controlling fast biped robots demands a deeper understanding of biped walking than for slow robots.
Brain-Computer Interface (BCI) research aims at the development of a system that allows direct control of, e.
The ability to construct and use a map of the environment is a critical enabling technology for many important applications, such as search and rescue or extraterrestrial exploration.
Spectral graph methods have been used both in clustering and in semi-supervised learning.
In general it is important for models used in unsupervised learning to be able to describe the gross statistical properties of the data they are intended to learn from, otherwise these properties may distort inferences about the parameters of the model.
Kernel methods enable us to work with high dimensional feature spaces by defining weight vectors implicitly as linear combinations of the training examples.
A brain-computer interface (BCI) is a communication system that relies on the brain rather than the body for control and feedback.
The Gaussian process (GP) is a popular and elegant method for Bayesian non-linear nonparametric regression and classification.
Electromagnetic source imaging, the reconstruction of the spatiotemporal activation of brain sources from MEG and EEG data, is currently being used in numerous studies of human cognition, both in normal and in various clinical populations [1].
In a eukaryotic cell, a variety of DNA switches—promoters, enhancers, silencers, etc.
There is currently a great deal of interest in appearance-based approaches to face recognition [1], [5], [8].
In multi-agent systems, individuals within a group coordinate and interact to achieve a goal.
Data clustering is unsupervised classification of objects into groups based on their similarity [1].
Distributed communication, sensing, and computing [13, 17] are emerging fields with numerous promising applications.
Undirected graphical models are powerful statistical tools having a wide range of applications in diverse fields such as image analysis [1, 2], conditional random fields [3], neural models [4] and epidemiology [5].
While obtaining labeled data remains a time and labor consuming task, acquisition and storage of unlabelled data is becoming increasingly cheap and easy.
Recent results [1, 2] indicate that the current VLSI paradigm based on CMOS technology can be hardly extended beyond the 10-nm frontier: in this range the sensitivity of parameters (most importantly, the gate voltage threshold) of silicon field-effect transistors to inevitable fabrication spreads grows exponentially.
Figure/ground organization, the binding of contours to surfaces, is a classical problem in vision.
There is a growing number of studies supporting the classical view of perception as probabilistic inference [1, 2].
The goal of nonlinear dimensionality reduction (NLDR) [1, 2, 3] is to find low-dimensional manifold descriptions of high-dimensional data.
Neurons in the primary visual cortex (V1) display what is often referred to as “size tuning”, i.
An active area of research in machine transcription of handwritten documents is reducing the amount and expense of supervised data required to train prediction models.
The availability and use of unstructured historical collections of documents is rapidly growing.
One of the major issues in present wireless communications is how users share the resources.
The brain does not have the computational capacity to fully process the massive quantity of information provided by the eyes.
Learning algorithms try to produce classifiers with small prediction error by trying to optimize some function that can be computed from a training set of examples and a classifier.
One of the most powerful mechanisms of learning in humans is learning by watching.
The firing patterns of cortical neurons look very noisy [1].
Bayesian Gaussian processes provide a promising probabilistic kernel approach to supervised learning tasks.
Given a data set (x1 , y1 ), (x2 , y2 ), .
In practical applications, one is frequently confronted with situations in which multiple tasks must be solved.
In many scientific and engineering problems where one is modeling some function over an experimental space, one is not necessarily interested in the precise value of the function over an entire region.
Given n i.
In this paper, we analyze the risk of hypotheses selected from the ensemble obtained by running an arbitrary on-line learning algorithm on an i.
In this paper, we show that support vector machines (SVMs) are the solution of a relaxed maximum a posteriori (MAP) estimation problem.
In recent years, there has been growing interest in large scale analyses of brain activity with respect to associated behavioral variables.
Efficient coding hypothesis has been proposed as a guiding computational principle for the analysis of early visual system and motivates the search for good statistical models of natural images.
Information Extraction (IE) is an important task in natural language processing, with many practical applications.
Recently there has been great interest in distributed reinforcement learning problems where a collection of agents with independent action choices attempts to optimize a joint performance metric.
Suppose {Xi }ni=1 are i.
Feature selection methods can be classified into “wrapper” methods and “filter” methods [4].
Batch learning is probably the most common supervised machine-learning setting.
Directional edge detection in an input image is the most essential operation in early visual processing [1, 2].
An important problem in machine learning is how to generalize between multiple related tasks.
Consider a set of acoustic sensors (microphones) for detecting acoustic events in the environment (e.
The Octopus arm is one of the most sophisticated and fascinating appendages found in nature.
Recurrent networks that perform a winner-take-all computation are of great interest because of the computational power they offer.
Contextual effect plays an essential role in the linguistic behavior of humans.
Detecting objects rapidly in images is very important.
The computation of normalizing constants plays an important role in statistical inference.
In a recent paper, Feng [1] was questioning the “goodness” of the Integrate-and-Fire model (I&F).
A unifying theme in the recent literature on classification is the notion of a surrogate loss function—a convex upper bound on the 0-1 loss.
The setting of manifold learning we consider is the following.
Clustering and low dimensional representation of high dimensional data are important problems in many diverse fields.
Kernel based methods such as Support Vector Machines (SVMs) have proven to be powerful for a wide range of different data analysis problems.
Consider the problem of joint learning (parameter estimation) and prediction in a Markov random field (MRF): in the learning phase, an initial collection of data is used to estimate parameters, and the fitted model is then used to perform prediction (e.
We consider the aggregation problem (cf.
Two issues, (i) efficient representation, and (ii) efficient inference, are of central importance in the area of statistical modeling of vision problems.
Many machine learning (ML) applications deal with the problem of bipartite ranking where the goal is to find a function which orders relevant elements over irrelevant ones.
Time lags in acoustic wave propagation provide cues to localize an acoustic source from observations across an array.
A variety of "brain tasks" can be tersely posed as transformation-discovery problems.
In recent years, there has been considerable interest in finding sparse signal representations from redundant dictionaries [1, 2, 3, 4, 5].
In a neonatal intensive care unit (NICU), an infant’s vital signs, including heart rate, blood pressures, blood gas properties and temperatures, are continuously monitored and displayed at the cotside.
Given a probabilistic model with discrete-valued hidden variables, Belief Propagation (BP) and related graph-based algorithms are commonly employed to solve for the Maximum APosteriori (MAP) assignment (i.
A banal, but nonetheless valid, behaviorist observation is that hungry animals work harder to get food [1].
There has been increased interest in applying tools from supervised learning to problems in reinforcement learning.
PCA is indispensable as a basic tool for factor analysis and modeling of data.
Engineers have long admired the brain’s ability to effortlessly adapt to novel situations without instruction, and sought to endow digital computers with a similar capacity for unsupervised self-organization.
In this paper we address the theoretical capabilities of active learning for estimating functions in noise.
The Address-Event-Representation (AER) is an event-driven asynchronous inter-chip communication technology for neuromorphic systems [1][2].
A very large fraction of the recent work in statistical machine learning has been focused on non-parametric learning algorithms which rely solely, explicitly or implicitely, on the smoothness prior, which says that we prefer as solution functions f such that when x ≈ y, f (x) ≈ f (y).
The problem of learning patterns of human behavior from sensor data arises in many areas and applications of computer science, including intelligent environments, surveillance, and assistive technology for the disabled.
Consider the problem of detecting faint asteroids from a series of images collected on a single night.
Within and across instances of a certain class of a natural signal, such as a facial image, a bird song recording, or a certain type of a gene, we find many repeating fragments.
The recent interest in manifold learning algorithms is due, in part, to the multiplication of very large datasets of high-dimensional data from numerous disciplines of science, from signal processing to bioinformatics [6].
Since natural language data take the form of sequences of words and are generally analyzed into discrete structures, such as trees (parsed trees), discrete kernels, such as sequence kernels [7, 1] and tree kernels [2, 5], have been shown to offer excellent results in the natural language processing (NLP) field.
The inference of depth information from single images is typically performed by devising models of image formation based on the physics of light interaction and then inverting these models to solve for depth.
It is well known that neurons can suddenly change firing statistics to reflect a macroscopic change of a nervous system.
The scope of discriminative learning methods has been expanding to encompass prediction tasks with increasingly complex structure.
Object detection is one of our most common visual operations.
Given a probability measure P and a reference measure µ, the minimum volume set (MV-set) with mass at least 0 < α < 1 is G∗α = arg min{µ(G) : P (G) ≥ α, G measurable}.
Representations of state in dynamical systems fall into three main categories.
What do Jesus and Darwin have in common? Other than being associated with two different views on the origin of man, they also have colleges at Cambridge University named after them.
A lot of recent work on semi-supervised learning is based on regularization on graphs [5].
In this paper, we develop methods for analyzing the features composing a visual scene, thereby localizing and categorizing the objects in an image.
A great deal of attention has been paid to the curious phenomenon of illusory contours in visual scenes [5].
Discriminative methods, such as Boosting and Support Vector Machines have significantly advanced the state of the art for classification.
Recovering 3-D depth from images is a basic problem in computer vision, and has important applications in robotics, scene understanding and 3-D reconstruction.
The Variational Bayesian (VB) framework has been widely used as an approximation of the Bayesian learning for models involving hidden (latent) variables such as mixture models[2][4].
Value iteration (Bellman, 1957) has proven its worth in a variety of sequential-decisionmaking settings, most significantly single-agent environments (Puterman, 1994), team games, and two-player zero-sum games (Shapley, 1953).
Automated reasoning and learning theory have been the subject of intensive investigation since the early developments in computer science [14].
Building appropriate object models is central to object recognition, which is a fundamental problem in computer vision.
Noise has an important impact on information processing of the nervous system in vivo.
Value function approximation (VFA) is a well-studied problem: a variety of linear and nonlinear architectures have been studied, which are not automatically derived from the geometry of the underlying state space, but rather handcoded in an ad hoc trial-and-error process by a human designer [1].
Theoretical approaches to understanding neuromodulatory systems are plagued by the latter’s neural ubiquity, evolutionary longevity, and temporal promiscuity.
Principal Component Analysis (PCA for short in the sequel) is a widely used tool for data dimensionality reduction.
There has long been interest in the nature of eye movements and fixation behavior following early studies by Buswell [I] and Yarbus [2].
In neural systems, the representational capacity of a single neuron is estimated to be as low as 1 bit/spike [1, 2].
Device and sensor networks are shaping many activities in our society.
Pattern recognition has so far mainly focused on the following task: given many training examples labelled with their classes (the object they display), guess the class of a new sample which was not available during training.
We study an online (sequential) prediction setting in which, at each timestep, the learner is given some input from the set X , and the learner must predict the output variable from the set Y.
The objective of this paper is not to present yet another learning algorithm, but rather to point to a previously unnoticed relation between multi-layer neural networks (NNs),Boosting (Freund and Schapire, 1997) and convex optimization.
Kernel canonical correlation analysis (kernel CCA) has been proposed as a nonlinear extension of CCA [1, 11, 3].
We consider multivariate Gaussian distributions defined on graphs.
Diffusion weighted Magnetic Resonance Imaging (DT-MRI) enables the measurement of the apparent water self-diffusion along a specified direction [1].
Modeling population activity is central to many problems in the analysis of neural data.
Many machine learning applications depend on accurately ordering the elements of a set based on the known ordering of only some of its elements.
The traditional view on Hebbian plasticity is that the correlation between pre- and postsynaptic events will drive learning.
There are many striking similarities between results in the standard batch learning setting, where labeled examples are assumed to be drawn independently from some distribution, and the more difficult online setting, where labeled examples arrive in an arbitrary sequence.
Ideal Observers give fundamental limits for performing visual tasks (somewhat similar to Shannon’s limits on information transfer).
The introduction of the Support Vector Machine (SVM) [8] sparked a widespread interest in kernel methods as a means of solving (binary) classification problems.
Fisher linear discriminant analysis (LDA), a widely-used technique for pattern classification, finds a linear discriminant that yields optimal discrimination between two classes which can be identified with two random variables, say X and Y in Rn .
Interaction between brain sources, phase synchrony or coherent states of brain activity are believed to be fundamental for neural information processing (e.
Most experimental studies of Spike-Timing Dependent Plasticity (STDP) have focused on the timing of spike pairs [1, 2, 3] and so do many theoretical models.
Clustering has found increasing attention in the past few years due to the enormous information flood in many areas of information processing and data analysis.
Many machine learning tasks involve clustering data using a mixture model, so that the data in each cluster is accurately described by a probability model from a pre-defined, possibly parameterized, set of models [1].
Finding common structure between two or more concepts lies at the heart of analogical reasoning.
Finding a small subset of most predictive features in a high dimensional feature space is an interesting problem with many important applications, e.
This paper was motivated by recent joint work with Ole Winther on approximate inference techniques (the expectation consistent (EC) approximation [1] related to Tom Minka’s EP [2] approach) which allows us to tackle high–dimensional sums and integrals required for Bayesian probabilistic inference.
When researchers use machine learning for object detection, they need to know the location and size of the objects, in order to generate positive examples for the classification algorithm.
Here we focus on the task of spatial cluster detection: finding spatial regions where some quantity is significantly higher than expected.
Optimal resource allocation is a well known problem in the area of distributed computing [1, 2] to which significant effort has been dedicated within the computer science community.
A number of regularities have been empirically observed for the motion of the end-point of the human upper-limb during curved and drawing movements.
Data clustering has been an active research area with a long history.
A central difficulty in modeling time-series data is in determining a model that can capture the nonlinearities of the data without overfitting.
Topology what for? Given a set of points in a high-dimensional euclidean space, we intend to extract the topology of the manifolds from which they are drawn.
A central objective of statistical machine learning is to discover structure in the joint distribution between random variables, so as to be able to make predictions about new combinations of values of these variables.
Detecting a yellow tiger among distracting foliage in different shades of yellow and brown requires efficient top-down strategies that select relevant visual cues to enable rapid and reliable detection of the target among several distractors.
Social network analysis is becoming increasingly important in many fields besides sociology including intelligence analysis [2], marketing [3] and recommender systems [4].
When segmenting a visual scene, the brain utilizes a variety of visual cues.
Statistical machine learning methods are making increasing inroads in the area of biological data analysis, particularly in the context of genome-scale data, where computational efficiency is paramount.
A major challenge in auditory neuroscience is to understand how cortical neurons represent the acoustic environment.
Bayesian methods have become quite popular in psychophysics and neuroscience (1–5); in particular, a recent trend has been to interpret observed biases in perception and/or behavior as optimal, in a Bayesian (average) sense, under ecologically-determined prior distributions on the stimuli or behavioral contexts under study.
Statistical tests are often used in machine learning in order to assess the performance of a new learning algorithm or model over a set of benchmark datasets, with respect to the state-of-the-art solutions.
To date, only a few planning tools have attempted to handle general probabilistic temporal planning problems.
Power-law distributions of event sizes have been observed in a number of seemingly diverse systems such as piles of granular matter [8], earthquakes [9], the game of life [1], friction [7], and sound generated in the lung during breathing.
Approximate inference techniques are enabling sophisticated new probabilistic models to be developed and applied to a range of practical problems.
Let x ∈ R|x| be a visible pattern, and y ∈ {y1 , .
At present, the best view of the activity of a neural circuit is provided by multiple-electrode extracellular recording technologies, which allow us to simultaneously measure spike trains from up to a few hundred neurons in one or more brain areas during each trial.
Stochastic symbol sequences with memory effects are frequently modelled by training hidden Markov models with the Baum-Welch variant of the EM algorithm.
Traditional motion representations, based on optical flow, are inherently local and have significant difficulties when faced with aperture problems and noise.
Nonnegative matrix approximation (NNMA) is a method for dimensionality reduction and data analysis that has gained favor over the past few years.
A large part of learning theory deals with methods that bound the generalization error of hypotheses in terms of their empirical errors.
The support vector regression (SVR) is a popular tool for function estimation problems, and it has been widely used on many real applications in the past decade, for example, time series prediction [1], signal processing [2] and neural decoding [3].
It is important to understand the relationship between the Rescorla-Wagner (RW) algorithm [1,2] and theories of learning based on maximum likelihood (ML) estimation of the parameters of generative models [3,4,5].
In many natural machine learning settings, one is not only faced with data that may be corrupted or deficient in some way (classification noise or other label errors, missing attributes, and so on), but with data that is not uniformly corrupted.
Estimating a high dimensional regression function is notoriously difficult due to the “curse of dimensionality.
Stone’s celebrated theorem proves that given a large enough training sequence, even naive algorithms such as the k-nearest neighbors can be optimal.
Machine learning techniques based on similarity metrics have gained wide acceptance over the last few years.
Autonomous off-road vehicles have vast potential applications in a wide spectrum of domains such as exploration, search and rescue, transport of supplies, environmental management, and reconnaissance.
In recent years, there has been an enormous interest in developing technologies for measuring range.
In recent times, the possibility of accessing, handling and mining large-scale networks datasets has revamped the interest in their investigation and theoretical characterization along with the definition of new modeling frameworks.
In recent years, spectral clustering based on graph partitioning theories has emerged as one of the most effective data clustering tools.
We study the problem of estimating a probability distribution, particularly in the context of species habitat modeling.
The clustering of data is a problem found in many pattern recognition tasks, often in the guises of unsupervised learning, vector quantization, dimensionality reduction, etc.
Numerous experimental data show that STDP changes the value wold of a synaptic weight after pairing of the firing of the presynaptic neuron at time tpre with a firing of the postsynaptic neuron at time tpost = tpre + ∆t to wnew = wold + ∆w according to the rule  min{wmax , wold + W+ · e−∆t/τ+ } , if ∆t > 0 wnew = (1) max{0, wold − W− · e∆t/τ− } , if ∆t ≤ 0 , with some parameters W+ , W− , τ+ , τ− > 0 (see [1]).
A woman is walking down the street.
Gaussian process (GP) models are powerful tools for regression, function approximation, and predictive density estimation.
In the last few years, considerable progress has been made in finding good controllers for helicopters.
The human object detection literature, also known as visual search, has long struggled with how best to conceptualize the role of bottom-up (BU) and top-down (TD) processes in guiding search behavior.
A significant development in machine learning for classification has been the emergence of boosting algorithms [1].
In a diagnosis problem, one wishes to select the best test (or observation) to make in order to learn the most about a system of interest.
In this paper we consider a class of infinite horizon non-zero sum stochastic dynamic games.
The central aspect of quantitative approaches in psychology is to adequately model human behaviour.
Preliminaries Definition 1.
Policy Gradient (PG) methods are Reinforcement Learning (RL) algorithms that maintain a parameterized action-selection policy and update the policy parameters by moving them in the direction of an estimate of the gradient of a performance measure.
Search engines are invaluable tools for society.
Consider the probability distribution with density p(x) depicted in Fig.
We present a new method, XORSample, for uniformly sampling from the solutions of hard combinatorial problems.
In a network such as hippocampal area CA3 that shows prominent oscillations during memory retrieval and other functions [1], there are apparently three, somewhat separate, ways in which neurons might represent information within a single cycle: they must choose how many spikes to fire; what the mean phase of those spikes is; and how concentrated those spikes are about that mean.
In functional Magnetic Resonance Imaging, or fMRI, an MR scanner measures a physiological signal known to be correlated with neural activity, the blood-oxygenation-level dependent (BOLD) signal [12].
In traditional supervised learning, an object is represented by an instance (or feature vector) and associated with a class label.
Most approaches to semi-supervised learning (SSL) see the problem from one of two (dual) perspectives: supervised classification with additional unlabelled data (see [20] for a recent survey); clustering with prior information or constraints (e.
In the last years several new methods have been developed in the machine learning community which are based on the assumption that the data lies on a submanifold M in Rd .
The default assumption in many learning scenarios is that training and test data are independently and identically (iid) drawn from the same distribution.
There is a long and successful history of decomposing collections of documents into factors or clusters to identify “similar” documents and principal themes.
Conditional Random Sampling (CRS) is a sketch-based sampling technique that effectively exploits data sparsity.
Partial Least Squares (PLS) is, in its general form, a family of techniques for analyzing relations between data sets by latent variables.
Evidenced by three recent workshops1 , nonparametric Bayesian methods are gaining popularity in the machine learning community.
Linear implementations of Barlow’s efficient encoding hypothesis1 , such as ICA [1] and sparse coding [2], have been used to explain the very first layers of auditory and visual information processing in the cerebral cortex [1, 2, 3].
Multi-frame image super-resolution refers to the process by which a group of images of the same scene are fused to produce an image or images with a higher spatial resolution, or with more visible detail in the high spatial frequency features [7].
The rate of spike occurrence, or the firing rate, of a neuron can be captured by the (peri-stimulus) time-histogram (PSTH) [1, 2], which is constructed easily as follows: Align spike sequences to the onset of stimuli, divide time into discrete bins, count the number of spikes that enter each bin, and divide the counts by the bin size and the number of sequences.
Shape-based object recognition is a key problem in machine vision and content-based image retrieval (CBIR).
Structural equation modelling (SEM) is a technique widely used in the behavioural sciences.
Design and analysis of most machine learning algorithms are based on the assumption that the training data be drawn independently and from the same stationary distribution that the resulting model will be exposed to.
We address the problem of comparing samples from two probability distributions, by proposing a statistical test of the hypothesis that these distributions are different (this is called the two-sample or homogeneity problem).
In the multi-class clustering problem, we are given n data points, x1 , .
A major line of research on extending SVMs to handle partially labeled datasets is based on the following idea: solve the standard SVM problem while treating the unknown labels as additional optimization variables.
In [4, 3] Haussler, Littlestone and Warmuth proposed the one-inclusion prediction strategy as a natural approach to the prediction (or mistake-driven) model of learning, in which a prediction strategy maps a training sample and test point to a test prediction with hopefully guaranteed low probability of erring.
Spectral clustering methods are common graph-based approaches to (unsupervised) clustering of data.
Will that berry taste good? Is that table strong enough to sit on? Predicting whether an object has an unobserved property is among the most basic of all inductive problems.
The human visual system samples images through saccadic eye movements, which rapidly change the point of fixation.
Principal Components Analysis (PCA) is a standard linear technique for dimensionality reduction.
We introduce and analyze a theoretical model for the problem of learning from multiple sources of “nearby” data.
Autonomous helicopter flight represents a challenging control problem with high-dimensional, asymmetric, noisy, nonlinear, non-minimum phase dynamics.
Recombinations between ancestral chromosomes during meiosis play a key role in shaping the patterns of linkage disequilibrium (LD)—the non-random association of alleles at different loci—in a population.
In recent years many hard problems have been transformed into easier problems that can be solved ef ciently via linear methods [1] or convex optimization [2].
Magnetoencephalography (MEG) and electroencephalography (EEG) use an array of sensors to take EM field measurements from on or near the scalp surface with excellent temporal resolution.
A fundamental and versatile mechanism for learning in humans is imitation.
Various emerging quantitative measurement technologies in the life sciences are producing genome, transcriptome and proteome-wide data collections which has motivated the development of data integration methods within an inferential framework.
Large-scale networks of sensing devices have become increasingly pervasive, with applications ranging from sensor networks and mobile robot teams to emergency response systems.
Biological systems interact with the outside world in real-time, reacting to complex stimuli in few milliseconds.
Supervised learning, or learning from examples, refers to the task of training a system by a set of examples which are specified by input-output pairs.
A number of approximate Bayesian methods have been proposed to offset the high computational cost of exact Bayesian calculations.
Many computer vision tasks require the use of appearance and shape models to represent objects in the scene.
In a sequence of trials we must pick hypotheses y1 , y2 , .
In many real-world applications, collection of labeled data is both time-consuming and expensive.
Gene prediction is the task of labeling nucleotide sequences to identify the location and components of genes (Figure 1).
Motion blur is the result of the relative motion between the camera and the scene during image exposure time.
We consider the fundamental problem of making inference about an unknown function f that predicts an output Y using a p dimensional vector of inputs x when Y = f (x) + ,  ∼ N (0, σ 2 ).
There is an increasing body of evidence supporting the hypothesis that recurrent cooperative competitive neural networks play a central role in cortical processing [1].
Undirected graphical models, such as Markov networks or log-linear models, have been used in an ever-growing variety of applications, including computer vision, natural language, computational biology, and more.
The description of factorization properties of families of probabilities using graphs (i.
In this paper, we address the problem of grasping novel objects that a robot is perceiving for the first time through vision.
Kernels provide a general framework of statistical learning that allows for integrating problemspecific background knowledge via the geometry of a feature space.
The task of data clustering is important in many fields such as artificial intelligence, data mining, data compression, computer vision, and others.
A decision list is a Boolean function defined over n Boolean inputs of the following form: if `1 then b1 else if `2 then b2 .
Probabilistic models of language make two kinds of substantive assumptions: assumptions about the structures that underlie language, and assumptions about the probabilistic dependencies in the process by which those structures are generated.
The last several years have seen significant activity in geometrically motivated approaches to data analysis and machine learning.
Unsupervised learning methods are often used to produce pre-processors and feature extractors for image analysis systems.
Humans can reliably analyze visual motion under a diverse set of conditions, including textured as well as featureless objects.
We model the multi-agent planning problem as a general-sum stochastic game with cheap talk: the agents observe the state of the world, discuss their plans with each other, and then simultaneously select their actions.
The problem of reconstructing a surface from a set of points frequently arises in computer graphics.
A good measure for similarity between signals is necessary in many machine learning problems.
The Information Bottleneck (IB) approach and independent component analysis (ICA) have both attracted substantial interest as general principles for unsupervised learning [1, 2].
Recently, graph based semi-supervised learning algorithms have been used successfully in various machine learning problems including classification, regression, ranking, and dimensionality reduction.
Hierarchical Dirichlet processes (DPs) (Teh et al.
The problem of partitioning data points into a number of distinct sets, known as the clustering problem, is central in data analysis and machine learning.
Sparse coding of natural images has led to models that resemble the receptive fields in the primate primary visual cortex area V1 (see e.
Our nervous system is constantly integrating information from many different sources into a unified percept.
Several recent developments such as the growth of the world wide web and the maturation of genomic technologies, have brought new domains of application to machine learning research.
Behind all variational methods for inference in probabilistic models lies a basic principle: treat the quantities of interest, which amount to moments of the random variables, as the solution to an optimization problem obtained via convex duality.
In many modern large-scale learning applications, the amount of unlabeled data far exceeds that of labeled data.
Brain-Computer-Interfaces (BCIs) allow communication without using the peripheral nervous systems by detecting intentional changes in the mental state of a user (see [1] for a review).
In the traditional formulation of supervised learning, data instances are viewed as vectors of features in some high-dimensional space.
A key part of many reinforcement learning algorithms is a policy evaluation process, in which the value function of a policy is estimated online from data.
The area of distributed computing systems provides a promising domain for applications of machine learning methods.
In Single-Class Classification (SCC) the learner observes a training set of examples sampled from one target class.
Given two views of the same scene, this paper addresses the dual objectives of inferring depth and segmentation in scenes with perceptually distinct foreground and background layers.
Kernel methods have shown to be competitive with other techniques in classification or regression tasks where the input data lie in a vector space.
In the past decade, modern computational Bayesian methods have been productively applied to the modeling of many core psychological phenomena.
Learning dynamical systems is an important problem in several fields including engineering, physical science and social science.
Kernel machines use a kernel function as a non-linear mapping of the original data into a highdimensional feature space; this mapping is often referred to as empirical kernel map [6, 11, 8, 9].
One of the goals of unsupervised learning is to discover the latent structure expressed in observed data.
Machine learning in domains such as bioinformatics, drug discovery, and web data mining involves the study of relationships between objects.
When a single data object is described by a set of feature vectors, it is often useful to consider the matching or “correspondence” between two sets’ elements in order to measure their overall similarity or recover the alignment of their parts.
As a result of many years of widespread use, continuous density hidden Markov models (CDHMMs) are very well matched to current front and back ends for automatic speech recognition (ASR) [21].
In many areas of science and engineering, the problem arises how to discover low dimensional representations of high dimensional data.
Recent advances in motion capture technology have fueled interest in the analysis and synthesis of complex human motion for animation and tracking.
In the situation of information explosion that characterizes todays world, the need for automatic tools for data analysis is more than obvious.
Our visual system is remarkably good at producing consistent, crisp percepts of the world around us, in the process hiding interpretation uncertainty.
In machine learning problem settings, we generally assume pairwise relationships among the objects of our interest.
Anomaly detection and localization are important but notoriously difficult problems.
Human categorization is fundamentally hierarchical, where categories are organized in tree-like hierarchies.
Establishing 3D correspondence between surfaces such as human faces is a crucial element of classspecific representations of objects in computer vision and graphics.
Multinomial logistic and probit regression are perhaps the classic statistical methods for multi-class pattern recognition problems (for a detailed introduction, see e.
Bayesian approaches have become an important modeling paradigm in machine learning.
In many statistical learning problems, it is useful to obtain an estimate of the underlying probability density given a set of observations.
In many computer aided diagnosis applications, the goal is to detect potentially malignant tumors and lesions in medical images (CT scans, X-ray, MRI etc).
In this paper, we study the problem of object-level figure/ground segmentation in images and video sequences.
Hidden Markov SVMs are a recently-proposed method for predicting a label sequence given the input sequence [3, 17, 18, 1, 2].
In many inference tasks, the cost function1 used to assess the final quality of the system is not the one used during training.
Joint pattern alignment attempts to remove from an ensemble of patterns the effect of nuisance transformations of a systematic nature.
Registration of point sets is an important issue for many computer vision applications such as robot navigation, image guided surgery, motion tracking, and face recognition.
The L1 Support Vector Machine (L1-SVM or SVM for short) [1, 2, 3] is a powerful technique for learning binary classifiers from examples.
Data in high dimensions is becoming ubiquitous, from image analysis and finances to computational biology and neuroscience.
Researchers in AI and cognitive science [1, 7] have proposed that hierarchies are useful for representing and reasoning about the objects in many real-world domains.
Animals interacting with a changeable, potentially adversarial environment need to excel in the detection of changes in its sensory inputs.
Support vector machines (SVM) implement linear classifiers in a high-dimensional feature space using the kernel trick to enable a dual representation and efficient computation.
The World Wide Web and other textual databases provide a convenient platform for exchanging opinions.
BCI systems typically require training on the subject side and on the decoding side (e.
Design of cortically implanted neural prosthetic sensors (CINPS)is an active area of research in the rapidly emerging field of brain machine interfaces (BMI) [1, 2].
Real brain structures employ inference algorithms as a basis of decision making.
The network models described in the neuroscience literature have frequently used rate equations to avoid the difficulties of formulating mathematical descriptions of spiking behaviors; and also to avoid the excessive computational resources required for simulating spiking networks.
The problem of nonlinear dimensionality reduction is to find the meaningful low-dimensional structure hidden in high dimensional data.
A Markov Decision Process (MDP) models a situation in which an agent interacts (by performing actions and receiving rewards) with an environment whose dynamics is Markovian, i.
Both nonnegative and sparse decompositions of data are desirable in domains where the underlying factors have a physical interpretation: In economics, sparseness increases the efficiency of a portfolio, while nonnegativity both increases its efficiency and reduces its risk [7].
Visual categorization is a difficult task in large part due to the large variation seen between images belonging to the same class.
Recent advances in molecular biology have brought about a revolution in our understanding of cellular processes.
Learning multiple related tasks simultaneously has been empirically [2, 3, 8, 9, 12, 18, 19, 20] as well as theoretically [2, 4, 5] shown to often significantly improve performance relative to learning each task independently.
Discriminative machine learning methods have led to better solutions for many problems in natural language processing (NLP), such as various kinds of sequence labeling.
The technique of k-nearest neighbor (kNN) is one of the most popular classification algorithms.
Blind source separation techniques, such as Independent Components Analysis (ICA), have received great interest in many domains including neuroscience [3; 19; 2], machine learning [12; 11], and speech and signal processing [25].
One of the central problems in cognitive science is determining the mental representations that underlie human inferences.
Recent analyses (Bengio, Delalleau, & Le Roux, 2006; Bengio & Le Cun, 2007) of modern nonparametric machine learning algorithms that are kernel machines, such as Support Vector Machines (SVMs), graph-based manifold and semi-supervised learning algorithms suggest fundamental limitations of some learning algorithms.
Optimising the performance of existing road networks is a cheap way to reduce the environmental, social, and financial impact of ever increasing volumes of traffic.
Random field models are a popular probabilistic framework for representing complex dependencies in natural image data.
The Internet triggered many opportunities for cooperative computing in which buyers and sellers can meet to buy and sell goods, information or knowledge.
Go is an ancient board game–over 3,000 years old [6, 5]–that poses unique opportunities and challenges for artificial intelligence and machine learning.
The goal of active learning is to select training data points so that the number of required training data points for a given performance is smaller than the number which is required when randomly sampling those points.
In many real-world statistical problems, we would like to fit a model with a large number of dependent variables to a training sample with very many cases.
We consider the machine vision task of pose estimation from static images, specifically for the case of articulated objects.
In Principal Component Analysis the n-dimensional data instances are projected into a kdimensional subspace (k < n) so that the total quadratic approximation error is minimized.
The temporal coding hypothesis states that information is encoded in the precise timing of action potentials sent by neurons.
Determining the direction from which a sound originated using only two microphones is a difficult problem.
Most vertebrates, including humans, can move their eyes.
Mapping functional brain activity is an important problem in basic neuroscience research as well as clinical use.
Sequence labeling, the task of assigning labels y = y1 , .
As sensor and storage technologies continue to improve in terms of both cost and performance, increasingly rich data sets are becoming available that characterize the rhythms of human activity over time.
Gaussian data is pervasive in all walks of life and many learning algorithms—e.
The ultimate goal of brain-computer interfacing (BCI) is to translate human intentions into a control signal for a device, such as a computer application, a wheelchair or a neuroprosthesis (e.
We are all familiar with the situation in which someone learns to perform a task on training examples drawn from some domain (the source domain), but then needs to perform the same task on a related domain (the target domain).
Since Hodgkin and Huxley [1] described the ionic ﬂux across the neuronal membrane with four nonlinear differential equations more than half a century ago, continuous efforts have been made either to extract an essence of the nonlinear dynamical aspect by simplifying the model, or construct ever more realistic models by including more ionic channels in the model.
Markov random fields (MRFs) [12] have been applied to a wide variety of real-world problems.
Sparse representations of signals have received a great deal of attentions in recent years.
Identification of clusters is the most basic tool for data analysis and unsupervised learning.
Transcriptional control relies in part on coordinate operation of DNA binding regulators and their interactions with various co-factors.
Neural motor prostheses (NMPs) aim to restore lost motor function to people with intact cerebral motor areas who, through disease or injury, have lost the ability to control their limbs.
What are the computational goals of the retina? The retina has numerous specialized classes of retinal ganglion cells (RGCs) that are likely to subserve a variety of different tasks [1].
The study of complex networked systems is an emerging field impacting nearly every area of engineering and science, including the important domains of biology, cognitive science, sociology, and telecommunications.
Consider a p-dimensional discrete random variable X = (X1 , X2 , .
There is a common presumption in developing supervised methods that the distribution of training points used for learning supervised models will match the distribution of points seen in a new test scenario.
Boosting algorithms are an important recent development in classification.
Remarkable progress in mathematics and computer science of probability is leading to a revolution in the scope of probabilistic models.
Principal Component Analysis (PCA) [1] refers to the problem of fitting a linear subspace S ⊂ RD of unknown dimension d < D to N sample points X = {xi ∈ S}N i=1 .
In many decision problems the parameters of the problem are inherently uncertain.
Online learning refers to a paradigm where, at each time t, an instance xt ∈ X is presented to a learner, which uses its parameter vector ft to predict a label.
Frequency scaling on silicon—the ability to drive chips at ever higher clock rates—is beginning to hit a power limit as device geometries shrink due to leakage, and simply because CMOS consumes power every time it changes state [9, 10].
Many problems of interest in Computer Vision and Machine Learning can be formulated as a problem of correspondence: finding a mapping between one set of points and another set of points.
A set F of classifiers and a probability distribution D over their domain induce a metric in which the distance between classifiers is the probability that they disagree on how to classify a random object.
The ability to operate on sequential data is a vital prerequisite for application of machine learning techniques in many challenging domains.
In this paper we discuss and analyze a framework for devising efficient online learning algorithms for complex prediction problems such as multiclass categorization.
Skill ratings in competitive games and sports serve three main functions.
In graph-based methods, one often constructs similarity graphs by linking similar data points that are close in the feature space.
Bayesian networks with discrete random variables form a very general and useful class of probabilistic models.
Animals choose their actions based on reward expectation and motivational drives.
The performance of kernel methods, such as Support Vector Machines, Gaussian Processes, etc.
The theoretical understanding of support vector machines (SVMs) and related kernel-based methods has been substantially improved in recent years.
Measurements from sensor systems typically serve as a proxy for latent variables of interest.
The PAC-Bayes approach, initiated by [1], aims at providing PAC guarantees to “Bayesian-like” learning algorithms.
We work on a type of supervised learning problems called ranking or ordinal regression, where examples are labeled by an ordinal scale called the rank.
Human detection and localization from a single image is an active area of research that has witnessed a surge of interest in recent years [9, 18, 6].
We study the problem of robust online learning over a graph.
The goal of Brain-Computer Interface (BCI) research [1, 2, 3, 4, 5, 6, 7] is to provide a direct control pathway from human intentions reflected in brain signals to computers.
Relational learning concerns the modeling of physical, social, or other phenomena, where rich types of entities interact via complex relational structures.
Sparse coding provides a class of algorithms for finding succinct representations of stimuli; given only unlabeled input data, it learns basis functions that capture higher-level features in the data.
Bayesian methods are widely used throughout engineering for estimating quantities from corrupted measurements.
“Imitation learning” of control or navigational behaviors is important in many application areas.
Consider the general SVM classifier model in which, given n training examples {(x i , yi )}ni=1 , the primal problem consists of solving the following problem: n X 1 min kwk2 + C l(oi , yi ) (1) (w,b) 2 i=1 where l denotes a loss function over labels yi ∈ {+1, −1} and the outputs Pn oi on the training set.
Data transformation is of fundamental importance in machine learning, and may greatly improve and simplify tasks such as clustering.
Data clustering, the unsupervised classification of samples into groups, is an important research area in machine learning for several decades.
It has recently been argued that the most fundamental aspects of computations in visual cortex are still unknown [1].
Many of the most popular current methods for image classification represent images as collections of independent patches characterized by local visual descriptors.
Saccades are rapid eye movements that shift the direction of gaze from one target to another.
A number of techniques have been developed for dealing with high dimensional data sets that fall on or near a smooth low dimensional nonlinear manifold.
Planning in partially observable domains is a notoriously difficult problem.
Research over the last decade from fields as diverse as biology, sociology, economics and computer science has established the frequent empirical appearance of certain structural properties in naturally occurring networks.
Human motion perception can be generally thought of as the result of interaction of two factors, traditionally termed content and style.
One consequence of the expressive richness of natural languages is that usually more than one means exists of expressing the same (or approximately the same) message.
Density estimation is a popular unsupervised learning technique for recovering distributions from data.
Brain-Computer Interfaces (BCIs) translate the intent of a subject measured from brain signals directly into control commands, e.
The general ranking problem has widespread applications including commercial search engines and recommender systems.
The soldiers on a parade ground form a neat rectangle by interacting with their neighbors.
Machine learning has been applied to a number of tasks involving an input domain with a special topology: one-dimensional for sequences, two-dimensional for images, three-dimensional for videos and for 3-D capture.
Dimension reductions in the lα norm (0 < α ≤ 2) have numerous applications in data mining, information retrieval, and machine learning.
Determining the assumptions that guide human learning and inference is one of the central goals of cognitive science.
Human visual activity often involves search.
Active learning addresses the issue that, in many applications, labeled data typically comes at a higher cost (e.
In the so-called Ensemble Clustering problem, the target is to ‘combine’ multiple clustering solutions or partitions of a set into a single consolidated clustering that maximizes the information shared (or ‘agreement’) among all available clustering solutions.
A central tenet of the Bayesian program is the representation of beliefs by distributions, which assign probability to each of a set of hypotheses.
Why are we concerned by deviations? The efficiency of an algorithm can be summarized by its expected risk, but this does not precise the fluctuations of its risk.
In recent years optimization methods for convex models have seen significant progress.
Spike driven synaptic plasticity mechanisms have been thoroughly investigated in recent years to solve two important problems of learning: 1) how to modify the synapses in order to generate new memories 2) how to protect old memories against the passage of time, and the overwriting of new memories by ongoing activity.
When an agent is faced with the task of learning how to behave in a stochastic environment, a common approach is to model the situation using a Markov Decision Process.
We present an optimal way to combine binary classifiers in the Neyman-Pearson sense: for a given upper bound on false alarms (false positives), we find the set of combination rules maximizing the detection rate (true positives).
One quintessential problem in statistical learning [9, 20] is to construct a classifier from labeled training data (xi , yi ) ∼iid pX,Y (x, y).
Plotting a peristimulus time histogram (PSTH), or a spike density function (SDF), from spiketrains evoked by and aligned to a stimulus onset is often one of the first steps in the analysis of neurophysiological data.
Measurement of similarity is a critical asset of state of the art in computer vision.
Most problems in machine intelligence can be formulated as probabilistic inference using probabilistic models defined on structured knowledge representations.
The human visual system provides an arena in which objects compete for our visual attention, and a given object may win the competition with support from a number of influences.
Binocular disparity, the displacement between the image locations of an object between two eyes or cameras, is an important depth cue.
In this paper we consider apprenticeship learning in the setting of large, complex domains.
An important problem in statistics and machine learning consists in testing whether the distributions of two random variables are identical under the alternative that they may differ in some ways.
In recent years maximum variance unfolding (MVU), introduced by Saul et al.
Many applications of graphical models have traditionally dealt with discrete state spaces, where each variable is multinomial distributed given its parents [1].
Synchrony is an important topic in neuroscience.
Learning a good classifier requires a sufficient number of labeled training instances.
In many practical situations, a classification task can often be divided into related sub-tasks.
Movement planning and control is a very difficult problem in real-world applications.
Here, we present an algorithm for support vector machine (SVM) classification using indefinite kernels.
Matrix analysis techniques, e.
A frequently encountered problem in many fields is the analysis of histogram data to extract meaningful latent factors from it.
Semi-supervised inductive learning concerns the problem of automatically learning a decision rule from a set of both labeled and unlabeled data, which has received a great deal of attention due to enormous demands of real world learning tasks ranging from data mining to medical diagnosis [1].
In unsupervised problems where observed data has sequential, recursive, spatial, relational, or other kinds of structure, we often employ statistical models with latent variables to tease apart the underlying dependencies and induce meaningful semantic parts.
Boosting algorithms use simple “base classifiers” to build more complex, but more accurate, aggregate classifiers.
In recent years, latent annotation of PCFG has been shown to perform as well as or better than standard lexicalized methods for treebank parsing [1, 2].
A key requirement of biological or artificial agents acting in a random dynamical environment is estimating the state of the environment based on noisy observations.
In many real world systems, uncertainty can arise in both the prediction of the system’s behavior, and the observability of the system’s state.
A major puzzle for understanding learning in biological organisms is the relationship between experimentally well-established learning rules for synapses (such as STDP) on the microscopic level and adaptive changes of the behavior of biological organisms on the macroscopic level.
Background.
Semantic memory refers to our ability to learn and retrieve facts and relationships about concepts without reference to a specific learning episode.
Domain adaptation addresses a common situation that arises when applying machine learning to diverse data.
Markov Random Field (MRF) based exponential family of distribution allows for representing distributions in an intuitive parametric form.
Part-of-speech tagging is a basic problem in natural language processing and a building block for many components.
Two issues facing the use of statistical learning methods in applications are scale and privacy.
Gaussian processes (GP’s) are a widely used method for Bayesian non-linear non-parametric regression and classification [13, 16].
In recent years, the recognition of object categories has become a major focus of computer vision and has shown substantial progress, partly thanks to the adoption of techniques from machine learning and the development of better probabilistic representations [1, 3].
Clustering is one of the most common tools of unsupervised data analysis.
Graphical models are widely used in many areas, including statistical machine learning, computer vision, bioinformatics, and communications.
Suppose that we have labeled data L = {(X 1 , Y1 ), .
Most contemporary SMT systems view parallel data as independent sentence-pairs whether or not they are from the same document-pair.
Protein structure prediction is one of the most important unsolved problems in biology today.
Computational models of visual cortex, and in particular those based on sparse coding, have recently enjoyed much attention.
Neuronal activity, particularly in cerebral cortex, is highly variable.
Hierarchically structured data abound across a wide variety of domains.
Loopy Belief Propagation (LBP) and its variants [6, 9, 13] have been shown empirically to be effective in solving many instances of hard problems in a wide range of fields.
In the field of reinforcement learning, perhaps the most popular way to estimate the future discounted reward of states is the method of temporal difference learning.
The notion of algorithmic stability has been used effectively in the past to derive tight generalization bounds [2–4,6].
A common problem in speech processing is to identify properties of a speech segment such as the language, speaker, topic, or dialect.
Cortical neural networks are characterized by a large degree of recurrent excitatory connectivity, and local inhibitory connections.
Markov jump processes (MJPs) underpin our understanding of many important systems in science and technology.
In this paper we provide provably privacy-preserving versions of belief propagation, Gibbs sampling, and other local message-passing algorithms on large distributed networks.
Visual input to the retina consists of complex light intensity patterns.
Image motion is an important cue used by both biological and artificial visual systems to extract information about the environment.
The max-weight independent set (MWIS) problem is the following: given a graph with positive weights on the nodes, find the heaviest set of mutually non-adjacent nodes.
The goal of transfer learning [1] is to learn new tasks with fewer examples given information gained from solving related tasks, with each task corresponding to the distribution/probability measure generating the samples for that task.
Web search engine evaluation is an expensive process: it requires relevance judgments that indicate the degree of relevance of each document retrieved for each query in a testing set.
The state of the art in real-time face detection has progressed rapidly in recently years.
Major strides have been made in understanding the detailed dynamics of decision making in simple two-alternative forced choice (2AFC) tasks, at both the behavioral and neural levels.
Microarray and other high-throughput measurement devices have been applied to examine specimens such as cancer tissues of biological and/or clinical interest.
Many clustering frameworks have already been proposed, with numerous applications in machine learning, exploratory data analysis, computer vision and speech processing.
Traditionally, meteorological radars, such as the National Weather Service NEXRAD system, are tasked to always scan 360 degrees.
To arrive at a better understanding of human brain function, functional neuroimaging traditionally studies the brain’s responses to controlled stimuli.
Competitive games and sports can benefit from statistical skill ratings for use in matchmaking as well as for providing criteria for the admission to tournaments.
Actor-critic (AC) algorithms are based on the simultaneous online estimation of the parameters of two structures, called the actor and the critic.
Clustering is the problem of discovering “meaningful” groups in given data.
One of the most popular approaches to collaborative filtering is based on low-dimensional factor models.
Many fields have developed heuristic methods for preprocessing data to improve performance.
Although understanding attention is interesting purely from a scientific perspective, there are numerous applications in engineering, marketing and even art that can benefit from the understanding of both attention per se, and the allocation of resources for attention and eye movements.
What use is an episodic memory? It might seem that the possibility of a fulminant recreation of a former experience plays a critical role in enabling us to act appropriately in the world [1].
Current reinforcement-learning (RL) techniques hold great promise for creating a general type of artificial intelligence (AI), specifically autonomous (software) agents that learn difficult tasks with limited feedback (Sutton & Barto, 1998).
Decision making under uncertainty is one of the principal concerns of Artificial Intelligence and Machine Learning.
A computer graphics artist sits down to use a simple renderer to find appropriate surfaces for a typical reflectance model.
We consider a setting where a number of agents need to repeatedly make decisions in the face of uncertainty.
In many real world problems, the proportion of data points in different classes is highly skewed: some classes dominate the data set (majority classes), and the remaining classes may have only a few examples (minority classes).
Energy consumption is a major and growing concern throughout the IT industry as well as for customers and for government regulators concerned with energy and environmental matters.
Consider a training dataset D = {(xi , yi )}, i = 1 .
Subspace-based techniques for face recognition, such as Eigenfaces [1] and Fisherfaces [2], take advantage of the large redundancy present in most images to compute a lowerdimensional representation of their input data and stored patterns, and perform classification in the reduced subspace.
Social tags are a key part of “Web 2.
Stochastic processes defined on graphs offer a compact representation for the Markov structure in a large collection of random variables.
Supervised learning has proven an effective technique for learning a classifier when the quantity of labeled data is large enough to represent a sufficient sample from the true labeling function.
The so-called cocktail party problem refers to a situation where several sound sources are simultaneously active, e.
Graphical models [14, 8] are a powerful tool for probabilistic reasoning over sets of random variables.
Computing an optimal policy for a partially observable Markov decision process (POMDP) is an intractable problem [10, 9].
Neuromorphic analog, VLSI devices [12] try to derive organizational and computational principles from biologically plausible models of neural systems, aiming at providing in the long run an electronic substrate for innovative, bio-inspired computational paradigms.
Over the last decade, mathematical explorations into the statistics of natural scenes have led to the observation that these scenes, as complex and varied as they appear, have an underlying structure that is sparse [1].
Generative systems that model the relationship between face and speech offer a wide range of exciting prospects.
Very accurate pedestrian detectors are an important technical goal; approximately half-a-million pedestrians are killed by cars each year (1997 figures, in [1]).
Substantial progress has been made recently on the problem of fitting high dimensional linear regression models of the form Yi = X iT β + i , for i = 1, .
Recently, we have witnessed a tremendous increase in the sizes of data sets generated and processed by acquisition and computing systems.
Regression data are often modelled as noisy observations of an underlying process.
Measuring dependence of random variables is one of the main concerns of statistical inference.
Suppose that a medical center has decided to use machine learning techniques to induce a diagnostic tool from records of previous patients.
Permutations arise naturally in a variety of real situations such as card games, data association problems, ranking analysis, etc.
Many machine learning methods have computational bottlenecks in the form of nested summations that become intractable for large datasets.
We are interested in learning controllers for high-dimensional, highly non-linear dynamical systems, continuous in state, action, and time.
Bioinformatics provides a rich source for the application of techniques from machine learning.
Survey propagation (SP) is an algorithm for solving k-SAT recently developed in the physics community [1, 2] that exhibits excellent empirical performance on “hard” instances.
Hidden Markov Models (HMMs) assume a generative model for sequential data whereby a sequence of states (or sample path) is drawn from a Markov chain in a hidden experiment.
An efficient optimization algorithm is one that quickly finds a good minimum for a given cost function.
Classical time-series forecasting models, such as ARMA models [6], assume that forecasting is performed at a fixed horizon, which is implicit in the model.
In tasks such as sensor placement for environmental temperature monitoring or experimental design, one has to select among a large set of possible, but expensive, observations.
The psychophysics of visual saliency and attention have been extensively studied during the last decades.
Functional Magnetic Resonance Imaging (fMRI) poses a large-scale, noisy and altogether difficult problem for machine learning algorithms.
Since the pioneering work of Shannon [1], Markov models have not only been taught in elementary information theory classes, but also served as indispensable tools and building blocks for sequence modeling in many fields, including natural language processing, bioinformatics [2], and compression [3].
Speech dereverberation, which may be viewed as a denoising technique, is crucial for many speech related applications, such as hands-free teleconferencing and automatic speech recognition.
In visual scene interpretation the goal is to assign image pixels to one of several semantic classes or scene elements, thus jointly performing segmentation and recognition.
People with severe motor disabilities (spinal cord injury (SCI), amyotrophic lateral sclerosis (ALS), etc.
Recent years have seen widespread application of reproducing kernel Hilbert space (r.
Many applications for autonomous decision making (e.
Many problems in natural language, vision, and computational biology require the joint modeling of many dependent variables.
Computational models for sensory processing are still in their infancy, but one promising approach has been to compare aspects of sensory processing with aspects of machine-learning algorithms crafted to solve the same putative task.
Continuous state-space Markov Decision Processes (MDPs) are notoriously difficult to solve.
There is a growing need to analyze large collections of electronic text.
Very large data sets, such as collections of images, text, and related data, are becoming increasingly common, with examples ranging from digitized collections of books by companies such as Google and Amazon, to large collections of images at Web sites such as Flickr, to the recent Netflix customer recommendation data set.
Boosting provides a ready method for improving existing learning algorithms for classification.
The last few years have seen significant interest in “deep” learning algorithms that learn layered, hierarchical representations of high-dimensional data.
The problem of on-line learning linear-threshold functions from labeled data is one which have spurred a substantial amount of research in Machine Learning.
Utility of discriminant analysis in EEG Brain computer interface (BCI) algorithms [1][2][3][4] aim to decode brain activity, on a singletrial basis, in order to provide a direct control pathway between a user’s intentions and a computer.
Optimal control provides a potentially useful methodology to design nonlinear control laws (policies) u = u(x) which give the appropriate action u for any state x.
An important class of “distances” between multivariate probability distributions P and Q are the AliSilvey or f -divergences [1, 6].
Here we will be concerned with the generative model y = Φx + , (1) where Φ ∈ R is a dictionary of features, x ∈ R is a vector of unknown weights, y is an observation vector, and  is uncorrelated noise distributed as N (; 0, λI).
Structured prediction models commonly involve complex inference problems for which finding exact solutions is intractable [1].
Kernel machines such as the Support Vector Machine are attractive because they can approximate any function or decision boundary arbitrarily well with enough training data.
Collaborative filtering has gained much attention in the machine learning community due to the need for it in webshops such as those of Amazon, Apple and Netflix.
We address the problem of marker-less articulated pose and shape estimation of the human body from images using a detailed parametric body model [3].
Invariances are one of the most powerful forms of prior knowledge in machine learning; they have a long history [9, 1] and their application has been associated with some of the major success stories in pattern recognition.
Let {yi,j : 1 ≤ i < j ≤ n} denote data measured on pairs of a set of n objects or nodes.
Extensive games are a natural model for sequential decision-making in the presence of other decision-makers, particularly in situations of imperfect information, where the decision-makers have differing information about the state of the game.
In many supervised learning methods, overfitting is controlled through the use of regularization penalties for limiting model complexity.
Multi-task learning is an area of active research in machine learning and has received a lot of attention over the past few years.
Large repositories of private or public software source code, such as the open source projects available on the Internet, create considerable new opportunities and challenges for statistical machine learning, information retrieval, and software engineering.
The recognition of objects in a scene often consists of matching representations of image regions to an object model while rejecting background regions.
Value function representations are dominant in algorithms for dynamic programming (DP) and reinforcement learning (RL).
In many scenarios the data of interest consist of relational observations on the edges of networks.
Rankers such as humans, search engines, and classifiers, output full or partial rankings representing preference relations over n items.
Most of the research on Reinforcement Learning (RL) [13] has studied solutions to finite Markov Decision Processes (MDPs).
Let us examine the resource bottlenecks of SVMs in a binary classification setting to explain our proposed solution.
There has been much interest in developing machine learning methods involving complex loss functions beyond those used in regression and classi£cation problems [13].
Data samples may sometimes be characterized in multiple ways, e.
Applications in various domains such as text/web mining and bioinformatics often lead to very highdimensional data.
In regression, we want to explain or to predict a response variable y from a set of explanatory variables x = (x1 , .
The Information Bottleneck (IB) approach [2] allows the investigation of learning algorithms for unsupervised and semi-supervised learning on the basis of clear optimality principles from information theory.
We consider inference based on a countable set of models (sets of probability distributions), focusing on two tasks: model selection and model averaging.
Let P be a distribution on X × Y , where X is an arbitrary set and Y ⊂ R is closed.
In many applications, e.
In recent times, one of the major challenges in kernel methods has been design of kernels on structured data e.
Graphical models such as Markov Random Fields (MRFs) have been successfully applied to a wide variety of fields, from computer vision to computational biology.
Handwriting recognition is traditionally divided into offline and online recognition.
Nearest neighbor (NN) searching is a fundamental operation in machine learning, databases, signal processing, and a variety of other disciplines.
Dynamic Bayesian networks (DBNs) are graphical model representations of discrete-time stochastic processes.
Discrete random fields are a powerful tool to obtain a probabilistic formulation for various applications in Computer Vision and related areas [3].
Change-points detection tasks are pervasive in various fields, ranging from audio [10] to EEG segmentation [5].
In recent years semi-supervised learning, i.
Partially Observable Markov Decision Processes (POMDPs) provide a powerful model for sequential decision making under state uncertainty.
The problem of online convex optimization can be formulated as a repeated game between a player and an adversary.
Languages evolve over time, with words changing in form, meaning, and the ways in which they can be combined into sentences.
The experience of the passage of time, as well as the timing of events and intervals, has long been of interest in psychology, and has more recently attracted attention in neuroscience as well.
Clustering is one of the most basic problems of unsupervised learning with applications in a wide variety of fields.
Recently, there has been renewed interest in the problem of multi-task learning, see [2, 4, 5, 14, 16, 19] and references therein.
Continuous-time diffusion processes, described by stochastic differential equations (SDEs), arise naturally in a range of applications from environmental modelling to mathematical finance [13].
Bayesian networks remain the cornerstone of modern AI.
One of the basic problems in modeling controlled, partially observable, stochastic dynamical systems is representing and tracking state.
Energy minimization is the problem of finding a maximum a posteriori (MAP) configuration in a Markov random field (MRF).
From online auctions to Texas Hold’em, AI is captivated by multi-agent interactions based on competition.
Gaussian processes are a flexible and popular approach to non-parametric modelling.
A central goal of systems neuroscience is to identify the functional relationship between environmental stimuli and a neural response.
The application of system identification techniques to the study of sensory neural systems has a long history.
Boosting methods have been used with great success in many applications like OCR, text classification, natural language processing, drug discovery, and computational biology [13].
Multivariate real-valued data appears in many real-world data sets, and a lot of research is being focused on the development of multivariate real-valued distributions.
Products in today’s e-market are described using both visual and textual information.
Dimensionality reduction is a two-step process: 1) Transform the data so that more information will survive the projection, and 2) project the data into fewer dimensions.
Although a great deal is known about the low-level features computed by the human visual system, determining the information used to make high-level visual classifications is an active area of research.
The quest for efficient machine learning techniques which (a) have favorable generalization capacities, (b) are flexible for adaptation to a specific task, and (c) are cheap to implement is a pervasive theme in literature, see e.
The curse of dimensionality has traditionally been the bane of nonparametric statistics, as reflected for instance in convergence rates that are exponentially slow in dimension.
Few algorithms are better known in machine learning and statistics than expectation-maximization (EM) [5].
One of the main purposes of unsupervised learning is to produce good representations for data, that can be used for detection, recognition, prediction, or visualization.
A core issue in sensory coding is to seek out and model statistical regularities in high-dimensional data.
Survival analysis is a well-established field in medical statistics concerned with analyzing/predicting the time until the occurrence of an event of interest–e.
In the online linear optimization problem (as in Kalai and Vempala [2005]), at each timestep the learner chooses a decision xt from a decision space D ⊂ Rn and incurs a cost Lt · xt , where the loss vector Lt is in Rn .
Semi-supervised learning has attracted an increasing amount of research interest recently [3, 15].
Latent Dirichlet Allocation (LDA) [1] is a language model which clusters co-occurring words into topics.
A limitation of supervised learning is that it requires a set of instance labels which are often difficult or expensive to obtain.
Functional Magnetic Resonance Imaging (fMRI) is a non-invasive imaging technique that can quantify changes in cerebral venous oxygen concentration.
Many problems in machine learning involve sequences of real-valued multivariate observations.
The standard k-armed bandits problem has been well-studied in the literature (Lai & Robbins, 1985; Auer et al.
A central goal of computational neuroscience is to understand how the brain transforms sensory input into spike trains, and considerable effort has focused on the development of statistical models that can describe this transformation.
Internet content providers, such as MSN, Google and Yahoo, all depend on the correct functioning of the wide-area Internet to communicate with their users and provide their services.
Conditional random fields (CRFs) are undirected graphical models that have been successfully applied to the classification of relational and temporal data [1].
Unsupervised learning using mixture models assumes that one latent cause is associated with each data point.
Kernel independence measures have been widely applied in recent machine learning literature, most commonly in independent component analysis (ICA) [2, 11], but also in fitting graphical models [1] and in feature selection [22].
Barn owls, the champions of sound localization, show systematic errors when localizing sounds.
The noisy-or and noisy-and-not conditional probability distributions are frequently studied in cognitive science for modeling causal reasoning [1], [2],[3] and are also used as probabilistic models for artificial intelligence [4].
A common assumption in supervised learning is that training and test samples follow the same distribution.
Learning algorithms need to make assumptions about the problem domain in order to generalise well.
Modelling hierarchical aspects in complex stochastic processes is an important research issue in many application domains ranging from computer vision, text information extraction, computational linguistics to bioinformatics.
We consider the problem of modeling correlated outputs from a single Gaussian process (GP).
Language model (LM) adaptation is crucial to automatic speech recognition (ASR) as it enables higher-level contextual information to be effectively incorporated into a background LM improving recognition performance.
Relational data are observations of relationships between sets of objects and it is therefore natural to consider representing relations1 as arrays of random variables, e.
Recent years have seen a surge of readily available data for complex and varied domains.
Online learning methods for linear classifiers, such as the perceptron and passive-aggressive (PA) algorithms [4], have been thoroughly analyzed and are widely used.
Traditional machine learning relies on the availability of a large amount of labeled data to train a model in the same feature space.
A recent line of research in machine learning has focused on regularization based on block-structured norms.
Markov Decision Processes (MDPs) are a very popular formalism for decision making under uncertainty (Puterman, 1994).
A long standing problem in neuroscience has been collecting enough data to robustly estimate the response function of a neuron.
Offline handwriting recognition is generally observed to be harder than online handwriting recognition [14].
Factor analysis is the task of explaining data by means of a set of latent factors.
Causal relationships are fundamental to science because they enable predictions of the consequences of actions [1].
Neural responses are typically studied by averaging noisy spiking activity across multiple experimental trials to obtain firing rates that vary smoothly over time.
Batch learning (also called statistical learning) and online learning are two different supervised machine-learning frameworks.
A common inference task in graphical models is finding the most likely setting of the values of the variables (the MAP assignment).
Matching shapes in images has many applications, including image retrieval, alignment, and registration [1, 2, 3, 4].
Understanding neural coding is at the heart of theoretical neuroscience.
We study the problem of predicting the labelling of a graph in the online learning framework.
Policy search, also known as policy learning, has become an accepted alternative of value functionbased reinforcement learning [2].
In a Markov decision process (MDP) M with finite state space S and finite action space A, a learner in state s ∈ S needs to choose an action a ∈ A.
A key attribute of visual perception is the ability to extract invariances from visual input.
Probabilistic graphical models gained popularity in the recent decades due to their intuitive representation and because they enable the user to query about the value distribution of variables of interest [19].
Multi-class classification is a problem that arises in many applications of machine learning.
A key idea in reinforcement learning (RL) is to learn an action-value function which can then be used to derive a good control policy [15].
Given a reproducing kernel Hilbert space (RKHS) of a kernel k : X × X → R and training set D := ((x1 , y1 ), .
We present the Gaussian Process Density Sampler (GPDS), a generative model for probability density functions, based on a Gaussian process.
Machine learning builds models of the world using training data from the application domain and prior knowledge about the problem.
Most learning theory models such as the standard PAC learning framework [13] are based on the assumption that sample points are independently and identically distributed (i.
Hierarchical probabilistic models of discrete data have emerged as powerful tool for large-scale text analysis.
We address the problem of unsupervised learning of object classifiers for visually polysemous words.
As an observer moves through the environment, the retinal image changes over time to create multiple complex motion flows, including translational, circular and radial motion.
Real-time extraction of information from composite neural recordings is a significant challenge in neural interfacing.
Modeling sequences is an important problem since there is a vast amount of natural data, such as speech and videos, that is inherently sequential.
In the binary classification problem we are given realizations (x1 , y1 ), .
We consider a Partially Observable Markov Decision Problem (POMDP) (see e.
Visual attention plays an important role in the human visual system.
We address a supervised learning problem over a set of training data {xi , yi }ni=1 where xi ∈ X ⊂ Rp is a p-dimensional input vector and yi is a univariate response.
Principal component analysis (PCA) is a popular change of variables technique used in data compression, predictive modeling, and visualization.
Performing a behavioral action such as picking up a sandwich and bringing it to one’s mouth is a motor control task achieved easily every day by millions of people.
The problem of “holistic scene understanding” encompasses a number of notoriously difficult computer vision tasks.
We consider the question of determining a real-valued function on the space of permutations of n elements with very limited observations.
Animal behaviour is guided by rewards that can be received in different situations and by modulatory factors, such as stress and motivation.
We address the problem of finding taxonomies in data: that is, to cluster the data, and to specify in a systematic way how the clusters relate.
It is often the case that an observed waveform is the superposition of elementary waveforms, taken from a limited set and added with variable latencies and variable but positive amplitudes.
In the earliest days of artificial intelligence, the bottom-most layer of neural networks consisted of randomly connected “associator units” that computed random binary functions of their inputs [1].
Boosting algorithms are efficient procedures that can be used to convert a weak learning algorithm (one which outputs a weak hypothesis that performs only slightly better than random guessing for a binary classification task) into a strong learning algorithm (one which outputs a high-accuracy classifier).
In applications such as medical diagnosis, credit risk screening or information retrieval, one aims at ordering instances under binary label information.
In decision problems where errors incur a severe loss, one may have to build classifiers that abstain from classifying ambiguous examples.
Kernel learning [5, 9, 7] has received a lot of attention in recent studies of machine learning.
The Farwell-Donchin speller [4], also known as the “P300 speller,” is a Brain-Computer Interface which enables users to spell words provided that they can see sufficiently well.
Consider the task of naming the sum of two numbers, e.
Random Fields (RFs) have played an increasingly important role in the fields of image denoising, texture discrimination, image segmentation and many other important problems in computer vision.
In this paper we consider linear regression problems with least-square error.
In the last two decades, kernel methods have been a prolific theoretical and algorithmic machine learning framework.
Researchers in biological vision have long hypothesized that image contours (ordered sets of edge pixels, or contour points) are a compact yet descriptive representation of object shape.
Matching pairs of objects is a fundamental operation of unsupervised learning.
Learning to rank is aimed at constructing a model for ordering objects by means of machine learning.
Gaussian processes (GPs) are used for Bayesian non-parametric estimation of unobserved or latent functions.
The Shannon/Nyquist sampling theorem tells us that in order to preserve information when uniformly sampling a signal we must sample at least two times faster than its bandwidth.
Approximate dynamic programming methods often offer surprisingly good performance in practical problems modeled as Markov Decision Processes (MDP) [6, 2].
Current machine learning is frequently concerned with the estimation of functions with multivariate output.
Humans daily perform sequential decision-making under uncertainty to choose products, services, careers, and jobs; and to mate and survive as species.
Understanding the meaning of a sentence involves both syntactic and semantic analysis.
Ranking is the central problem for many information retrieval applications such as web search, collaborative filtering and document retrieval [8].
We study a problem setting of transfer learning in which classifiers for multiple tasks have to be learned from biased samples.
Linear prediction is the cornerstone of an extensive number of machine learning algorithms, including SVM’s, logistic and linear regression, the lasso, boosting, etc.
Understanding how the dynamics of a neural network is shaped by the network structure, and consequently facilitates the functions implemented by the neural system, is at the core of using mathematical models to elucidate brain functions [1].
Building models that make good predictions about the world can be a complicated task.
Humans demonstrate a variety and versatility of movements far beyond the reach of current anthropomorphic robots.
Constructing models of biological systems, e.
Neural activities are highly non-stationary and vary from time to time according to stimuli and internal state changes.
Column generation (CG) [3] is a technique widely used in linear programming (LP) for solving large-sized problems.
Brain-Computer Interfaces (BCIs) are devices that enable a subject to communicate without utilizing the peripheral nervous system, i.
Most accounts of the processes underlying human learning, decision-making, and perception assume that stimuli have fixed sets of features.
It was some fifty years after James (1950)’s famously poetic description of our capacities for attention that more analytically-directed experiments began, based originally on dichotic listening Cherry (1953).
Designing markets to achieve certain goals is gaining renewed importance with the prevalence of many novel markets, ranging from prediction markets [13] to markets for e-services [11].
A major goal of current information retrieval research is to develop algorithms that can improve retrieval effectiveness by inferring a more complete picture of the user’s information need, beyond that provided by the user’s query text.
Ten years ago, an eight-year lasting collaborative effort resulted in the first completely sequenced genome of a multi-cellular organism, the free-living nematode Caenorhabditis elegans.
We consider the problem of (approximately) minimizing a stochastic objective F (w) = Eθ [f (w; θ)] (1) where the optimization is with respect to w ∈ W, based on an i.
A recent trend in statistical machine translation (SMT) has been the use of synchronous grammar based formalisms, permitting polynomial algorithms for exploring exponential forests of translation options.
The fact that most visual learning problems deal with high dimensional data has made dimensionality reduction an inherent part of the current research.
Model-free policy gradient algorithms allow for the optimization of control policies on systems which are impractical to model effectively, whether due to cost, complexity or uncertainty in the very structure and dynamics of the system (Kohl & Stone, 2004; Tedrake et al.
The work we report here falls under the general heading of state estimation, i.
The area of high-dimensional statistical inference is concerned with the behavior of models and algorithms in which the dimension p is comparable to, or possibly even larger than the sample size n.
Recognizing human actions from videos is a task of obvious scientific and practical importance.
Evolution is likely to favor those biological organisms which are able to maximize the chance of achieving correct decisions in response to multiple unreliable sources of evidence.
The problem of modeling relational information among objects, such as pairwise relations represented as graphs, arises in a number of settings in machine learning.
The binary classification of examples x is usually performed with recourse to the mapping ŷ = sign[f (x)], where f is a function from a pre-defined class F, and ŷ the predicted class label.
Structured estimation [18, 20] and related techniques has proven very successful in many areas ranging from collaborative filtering to optimal path planning, sequence alignment, graph matching and named entity tagging.
This paper presents an algorithm for solving the following class of online resource allocation problems.
Mammalian brains consist of billions of neurons, each capable of independent electrical activity.
The goal of most machine learning problems is to generalize from a limited number of training examples.
Trust tasks such as the Dictator, Ultimatum and Investor-Trustee games provide an empirical basis for investigating social cooperation and reciprocity [11].
Signals may be manipulated, transmitted or stored more efficiently if they are transformed to a representation in which there is no statistical redundancy between the individual components.
The stochastic approximation method supplies the theoretical underpinnings behind many well-studied algorithms in machine learning, notably policy gradient and temporal differences for reinforcement learning, inference for tracking and filtering, on-line learning [1, 17, 19], regret minimization in repeated games, and parameter estimation in probabilistic graphical models, including expectation maximization (EM) and the contrastive divergences algorithm.
Online advertisements (ads) are a rapidly growing source of income for many Internet content providers.
In many domains, researchers are interested in predicting the effects of interventions, or manipulating variables, on other observed variables.
Regularization has emerged as a dominant theme in machine learning and statistics, providing an intuitive and principled tool for learning from high-dimensional data.
Active learning is a paradigm in which the learner has the ability to sequentially select examples for labeling.
The expectation propagation (EP) message passing algorithm is often considered as the method of choice for approximate Bayesian inference when both good accuracy and computational efficiency are required [5].
Actor-critic (AC) algorithms [22] were probably among the first algorithmic approaches to reinforcement learning (RL).
Multi-armed bandit problems describe typical situations where learning and optimization should be balanced in order to achieve good cumulative performances.
Time series classification arises in diverse application.
Problem statement This paper is motivated by the problem fitting of binary distributions to experimental data.
Kullback-Leibler divergence, mutual information and differential entropy are central to information theory [5].
Recent studies have shown that mapping random variables into a suitable reproducing kernel Hilbert space (RKHS) gives a powerful and straightforward method of dealing with higher-order statistics of the variables.
Inferring structured predictions based on high-dimensional, often multi-modal and hybrid covariates remains a central problem in data mining (e.
Sparse approximation is a key technique developed in engineering and the sciences which approximates an input signal, X, in terms of a “sparse” combination of fixed bases B.
When modeling discrete time series data, the hidden Markov model [1] (HMM) is one of the most widely used and successful tools.
It is accepted that neural activity self-regulates to prevent neural circuits from becoming hyper- or hypoactive by means of homeostatic processes [14].
Consider a set of input vectors x1 , .
For many important problems in machine learning, we have a limited amount of labeled training data and a very high-dimensional feature space.
A typical mixture model is composed of a number of separately parameterized density models each of which has two important properties: 1.
A natural scene contains several multi-modal sensory cues to the true underlying values of its physical properties.
One common error human subjects make in statistical inference is that they detect hidden patterns and causes in what are genuinely random data.
Pattern clustering is a classic topic in pattern recognition and machine learning.
Probabilistic topic models provide a suite of algorithms for finding low dimensional structure in a corpus of documents.
Matching pursuit refers to a family of algorithms that generate a set of bases for learning in a greedy fashion.
The problem we address in this paper is, broadly speaking, function approximation.
With the advent of the Internet, it is now possible to use huge training sets to address challenging tasks in machine learning.
In the context of ranking, several performance measures may be considered.
The need to partition a sequence of observations into several homogeneous segments arises in many applications, ranging from speaker segmentation to pop song indexation.
In many real-world classification problems, it is not equally easy or affordable to verify membership in different classes.
The human immunodeficiency virus (HIV) has one of the highest levels of genetic variability yet observed in nature.
Despite of its widespread success in regression problems, Gaussian process (GP) has two limitations.
Algorithms for automatically discovering hierarchical structure from data play an important role in machine learning.
Most popular optimization algorithms, like the Levenberg-Marquardt algorithm (LMA) use simple “controllers” that modulate the behavior of the optimization algorithm based on the state of the optimization process.
Regularized Least Squares (RLS) algorithms have been drawing people’s attention since they were proposed due to their ability to avoid over-fitting problems and to express solutions as kernel expansions in terms of the training data [4, 9, 12, 13].
The web has become the central distribution channel for information from traditional sources such as news outlets as well as rapidly growing user-generated content.
Privacy-preserving machine learning is an emerging problem, due in part to the increased reliance on the internet for day-to-day tasks such as banking, shopping, and social networking.
Barn owl is a nocturnal predator with strong able auditory and visual localization system.
A critical problem in machine learning is that of scaling: Algorithms should be effective computationally and statistically as various dimensions of a problem are scaled.
Metal ions play important roles in protein function and structure and metalloproteins are involved in a number of diseases for which medicine is still seeking effective treatment, including cancer, Parkinson, dementia, and AIDS [10].
In the context of importance sampling, the ratio of two probability density functions is called the importance.
In many real-world prediction problems, the response variable of interest is clustered hierarchically.
An important step toward understanding the neural basis of vision is to develop computational models that describe how complex visual stimuli are mapping onto evoked neuronal responses.
Phylogenetic analysis plays a significant role in modern biological applications such as ancestral sequence reconstruction and multiple sequence alignment [1, 2, 3].
The problem of visualizing high dimensional data often arises in the context of exploratory data analysis.
In his highly influential paper, [1], Kleinberg advocates the development of a theory of clustering that will be “independent of any particular algorithm, objective function, or generative data model.
Statistical dependence measures have been proposed as a unifying framework to address many machine learning problems.
Ensemble methods [5] return a weighted vote of baseline classifiers.
Principal component analysis (PCA) is widely used for data pre-processing, data compression and dimensionality reduction.
Mechanistic system modeling employing nonlinear ordinary or delay differential equations 1 (ODEs or DDEs) is oftentimes hampered by incomplete knowledge of the system structure or the specific parameter values defining the observed dynamics [16].
Synapses are the primary locations in neural systems where information is processed and transmitted.
Most of the facts that we know about the world are not learned through first-hand experience, but are the result of information being passed from one person to another.
Kernel-based methods have been highly popular in statistical learning, starting with Parzen windows, kernel regression, locally weighted regression and radial basis function networks, and leading to newer formulations such as Reproducing Kernel Hilbert Spaces, Support Vector Machines, and Gaussian process regression [1].
The broad problem we consider is how to equip artificial agents with the ability to form useful high-level behaviors, or skills, from available primitives.
Dimensionality reduction is a common and often necessary step in most machine learning applications and high-dimensional data analyses.
Many problems in statistical pattern recognition and analysis require the classification and analysis of high dimensional data vectors.
Discriminative questions such as “What is it?” (categorization) and “Where is it?” (detection) are central to machine vision and have received much attention in recent years.
Principal component analysis (PCA) has been extensively used for data analysis and processing.
Spectral graph-theoretic methods have been used widely in unsupervised and semi-supervised learning recently.
Manifold learning (ML) methods have attracted substantial attention due to their demonstrated potential.
Regularization using the `1 -norm has attracted a lot of interest in the statistics [1], signal processing [2], and machine learning communities.
Images of natural environments contain a rich diversity of spatial structure at both coarse and fine scales.
Graphical models, such as Bayesian networks and factor graphs [1], are widely used to represent and visualise fixed dependency relationships between random variables.
Magnetoencephalography (MEG) and related electroencephalography (EEG) use an array of sensors to take electromagnetic field (or voltage potential) measurements from on or near the scalp surface with excellent temporal resolution.
Sparse signal models have been used successfully in a variety of applications including waveletbased image processing and pattern recognition.
Unsupervised learning of structured variables in data is a difficult problem that has received considerable recent attention.
Economists and computer scientists are often concerned with inferring people’s preferences from their choices, developing econometric methods (e.
Spike sorting (see [1] and [2] for review and methodological background) is the name given to the problem of grouping action potentials by source neuron.
Linear dynamical systems (LDSs) are useful in describing dynamical phenomena as diverse as human motion [9], financial time-series [4], maneuvering targets [6, 10], and the dance of honey bees [8].
In 2001, Jaeger [1] and Maass [2] independently introduced the idea of using a fixed, randomly connected recurrent neural network of simple units as a set of basis filters (operating at the edge-ofstability where the system has fading memory).
The area of high-dimensional statistics deals with estimation in the “large p, small n” setting, where p and n correspond, respectively, to the dimensionality of the data and the sample size.
The importance of dimension reduction for predictive modeling and visualization has a long and central role in statistical graphics and computation In the modern context of high-dimensional data analysis this perspective posits that the functional dependence between a response variable y and a large set of explanatory variables x ∈ Rp is driven by a low dimensional subspace of the p variables.
The problem of bipartite graph inference is to predict the presence or absence of edges between heterogeneous objects known to form the vertices of the bipartite graph, based on the observation about the heterogeneous objects.
Many learning problems can be naturally formulated in terms of multi-category classification or multi-task regression.
Reinforcement Learning [1] addresses the problem of how autonomous agents can improve their behavior using their experience.
A very active supervised learning trend has been flourishing over the last decade: it studies functions known as surrogates — upperbounds of the empirical risk, generally with particular convexity properties —, whose minimization remarkably impacts on empirical / true risks minimization [3, 4, 10].
Structure learning of dynamic Bayesian networks allows conditional dependencies to be identified in time-series data with the assumption that the data are generated by a distribution that does not change with time (i.
The existence of nested beliefs is one of the defining characteristics of a multi-agent world.
Bandit problems arise in many settings, including clinical trials, scheduling, on-line parameter tuning of algorithms or optimization of controllers based on simulations.
Undirected graphical models are a popular tool in machine learning; they represent real-valued energy functions of the form X X 0 E 0 (y) := Ei0 (yi ) + Eij (yi , yj ) , (1) i∈V (i,j)∈E where the terms in the first sum range over the nodes V = {1, 2, .
In Principal Components Analysis (PCA) we seek to reduce the dimensionality of a D-dimensional data vector to a smaller K-dimensional vector, which represents an embedding of the data in a lower dimensional space.
A number of recent techniques address the problem of metric learning, in which a distance function between data objects is learned based on given (or inferred) similarity constraints between examples [4, 7, 11, 16, 5, 15].
Visual category recognition is a vital thread in computer vision research.
Learning algorithms that can perform in a distributed asynchronous manner are of interest for several different reasons.
Odor sensors are diverse in terms of their sensitivity to odor identity and concentrations.
Principal Component Analysis (PCA) [9] is one of the primary statistical techniques for feature extraction and data modeling.
The Problem.
A common assumption in theoretical models of learning such as the standard PAC model [16], as well as in the design of learning algorithms, is that training instances are drawn according to the same distribution as the unseen test examples.
Latent variable models capture underlying structure in data by explaining observations as part of a more complex, partially observed system.
Message passing algorithms, in particular Belief Propagation (BP), have been very successful in efficiently computing interesting properties of succinctly represented large spaces, such as joint probability distributions.
It is known that visual cells (visual features) selectively respond to imagery patterns in perception.
Online regret minimizing algorithms provide some of the most successful algorithms for many machine learning problems, both in terms of the speed of optimization and the quality of generalization.
Analysis of large scale sequential data has become an important task in machine learning and data mining, inspired by applications such as biological sequence analysis, text and audio mining.
The Singular Value Decomposition (SVD) is a fundamental linear algebraic operation whose abundant useful properties have placed it at the computational center of many methods in machine learning and related fields.
Sparse and overcomplete image models were first introduced in [1] for modeling the spatial receptive fields of simple cells in the human visual system.
Brain computer interfaces (BCI) have seen a rapid development towards faster and more userfriendly systems for thought-based control of devices such as video games, wheel chairs, robotic devices etc.
Markov Decision Processes (MDPs) have been extensively studied in the context of planning and decision-making.
Pattern classification approaches to analyzing functional neuroimaging data have become increasingly popular [12] [3] [4].
In machine learning problems the data often live in a vector space, typically a Euclidean space.
Language and image understanding are two major tasks in artificial intelligence.
Consider the setting where we are given large amounts of unlabeled data together with dual supervision in the form of a few labeled examples as well as a few labeled features, and the goal is to estimate an unknown classification function.
Discrete random fields are a powerful tool for formulating several problems in Computer Vision such as stereo reconstruction, segmentation, image stitching and image denoising [22].
Precise models of technical systems can be crucial in technical applications.
In many areas of machine learning such as clustering, dimensionality reduction, or semi-supervised learning, neighborhood graphs are used to model local relationships between data points and to build global structure from local information.
Clustering is considered as one of the most fundamental unsupervised learning problems.
One of the major tasks of primary visual cortex (V1) is the computation of a representation of orientation in the visual field.
Statistical language modelling is concerned with building probabilistic models of word sequences.
Since the seminal contribution of [14], so-called ROC curves (ROC standing for Receiving Operator Characteristic) have been extensively used in a wide variety of applications (anomaly detection in signal analysis, medical diagnosis, search engines, credit-risk screening) as a visual tool for evaluating the performance of a test statistic regarding its capacity of discrimination between two populations, see [8].
Cell assemblies exhibiting episodes of recurrent coherent activity have been observed in several brain regions including the striatum[1] and hippocampus CA3[2], but how such correlated activity emerges in neural microcircuits is not well understood.
Exploration, in reinforcement learning, refers to the strategy an agent uses to discover new information about the environment.
Nonrigid structure from motion is the process of recovering the time varying 3D coordinates of points on a deforming object from their 2D locations in an image sequence.
Independent component analysis (ICA) is the problem of recovering latent random vector from observations of unknown linear functions of that vector.
It is sometimes possible to find a way of mapping objects in a “data” domain into objects in a “target” domain so that operations in the data domain can be modelled by operations in the target domain.
In machine learning, manifold structure has so far been mainly used in manifold learning [1], to enhance learning methods especially in semi-supervised learning.
We are concerned with machine learning over large datasets.
High-throughput functional data is emerging as an indispensible resource for generating a complete picture of genome-wide gene and protein function.
The inverse dynamics problem for a robotic manipulator is to compute the torques τ needed at the joints to drive it along a given trajectory, i.
An ultimate goal of neuroscience is to elucidate how information is encoded and decoded by neural activities.
Ranking is an important task in information sciences.
Cognitive control can be characterized as the ability to guide behavior according to current goals and plans.
When exposed to a novel visuomotor environment, for instance while wearing prism goggles, subjects initially exhibit large directional errors during reaching movements but are able to rapidly adapt their movement patterns and approach baseline performance levels within around 30-50 reach trials.
In the standard online learning protocol for binary classification the learner receives a sequence of instances generated by an unknown source.
Image annotation, or image labeling, in which the task is to label each pixel or region of an image with a class label, is becoming an increasingly popular problem in the machine learning and machine vision communities [7, 14].
Message passing algorithms such as Belief Propagation (BP) [1] exploit factorization to perform inference.
The Sample compression framework [Littlestone and Warmuth, 1986, Floyd and Warmuth, 1995] has resulted in an important class of learning algorithms known as sample compression algorithms.
Is accurate classification possible in the complete absence of hand-labeled data? A Priori, the answer would seem to be no, unless the learner has knowledge of some additional problem structure.
In recent years, online regret minimizing algorithms have become widely used and empirically successful algorithms for many machine learning problems.
Labeled data can be expensive, time-consuming and difficult to obtain in many applications.
The availability of large amounts of unlabeled data such as text on the Internet is a strong motivation for research in semi-supervised learning [4].
Much research on how people acquire knowledge focuses on discrete structures, such as the nature of categories or the existence of causal relationships.
Multivariate binary data arise in a variety of fields, such as biostatistics [1], econometrics [2] or artificial intelligence [3].
The goal of this study is to prove that the most influential form of reinforcement learning (RL) [1], which relies on the temporal difference (TD) learning rule [2], is equivalent to correlation based learning (Hebb, CL) which is convergent over wide parameter ranges when using a local third factor, as a gating signal, together with a differential Hebbian emulation of CL.
Gaussian summations occur in many machine learning algorithms, including kernel density estimation [1], Gaussian process regression [2], fast particle smoothing [3], and kernel based machine learning techniques that need to solve a linear system with a similarity matrix [4].
Nearly every sentence occurring in natural language can, given appropriate contexts, be interpreted in more than one way.
Consider a set of input vectors x1 , .
Semi-supervised Learning (SSL) takes advantage of a large amount of unlabeled data to enhance classification accuracy.
Classification methods which rely upon the graph Laplacian (see [3, 20, 13] and references therein), have proven to be useful for semi-supervised learning.
Graphical models are used in many different fields.
Magnetic resonance imaging (MRI) [7, 2] is a key diagnostic technique in healthcare nowadays, and of central importance for experimental research of the brain.
It is a long standing hypothesis that sensory systems are adapted to the statistics of their inputs.
An important problem in systems neuroscience is to develop flexible, statistically accurate models of neural responses.
Visual recognition remains a challenging task for machines.
Learning algorithms often assume a data matrix A ∈ Rn×D with n observations and D attributes and operate on the data matrix A through pairwise distances.
Consider the following regression model: Yi = X′i β(ti ) + ǫi , i = 1, .
The linear model is a mainstay of statistical inference.
Comparing speakers in speech signals is a common operation in many applications including forensic speaker recognition, speaker clustering, and speaker verification.
Many dynamic optimization problems can be cast as Markov decision problems (MDPs) and solved, in principle, via dynamic programming.
Pairwise distance computations are fundamental to many important computations in machine learning and are some of the most expensive for large datasets.
In reinforcement learning, Bellman’s dynamic programming equation is typically viewed as a method for determining the value function — the maximum achievable utility at each state.
Neurons in the brain communicate through the firing of action potentials.
Online advertising has become the cornerstone of many sustainable business models in today’s Internet, including search engines (e.
Segmenting semantic objects, and more broadly image parsing, is a fundamentally challenging problem.
Learning with examples having multiple labels is an important problem in machine learning and data mining.
Using a low-dimensional embedding to summarize a high-dimensional data set has been widely used for exploring the structure in the data.
Online learning algorithms are fast, simple, make few statistical assumptions, and perform well in a wide variety of settings.
The minimum description length (MDL) principle recommends to use, among competing models, the one that allows to compress the data+model most [Grü07].
Before we begin, we establish notation for this paper.
Extensive games are a powerful model of sequential decision-making with imperfect information, subsuming finite-horizon MDPs, finite-horizon POMDPs, and perfect information games.
There has been significant recent interest in sparse signal expansions in several settings.
Clustering is the problem of organizing a set of objects into groups, or clusters, in a way as to have similar objects grouped together and dissimilar ones assigned to different groups, according to some similarity measure.
The problem of feature selection has a long history due to its significance in a wide range of important problems, from early ones like pattern recognition to recent ones such as text categorization, gene expression analysis and others.
Object detection is one of the great challenges of computer vision, having received continuous attention since the birth of the field.
The problem of finding and exploiting low-dimensional structure in high-dimensional data is taking on increasing importance in image, audio and video processing, web search, and bioinformatics, where datasets now routinely lie in thousand- or even million-dimensional observation spaces.
Kernel-based methods are now well-established tools for supervised learning, allowing to perform various tasks, such as regression or binary classification, with linear and non-linear predictors [1, 2].
For many machine learning algorithms, the choice of a distance metric has a direct impact on their success.
The statistical problem of testing homogeneity of two samples arises in a wide variety of applications, ranging from bioinformatics to psychometrics through database attribute matching for instance.
The hierarchical Dirichlet process (HDP) [1] has emerged as a powerful model for the unsupervised analysis of text.
The Indian Buffet Process (IBP) [5] is a Bayesian nonparametric approach that models objects as arising from an unbounded number of latent features.
Much recent research in reinforcement learning (RL) has focused on hierarchical RL methods [1] and in particular the options framework [2], which adds to the RL framework principled methods for planning and learning using high level-skills (called options).
Temporal alignment of time series has been an active research topic in many scientific disciplines such as bioinformatics, text analysis, computer graphics, and computer vision.
Multiclass classification is the task of assigning labels from a predefined label-set to instances in a given domain.
In this paper the problem of Multiple Kernel Learning (MKL) is studied where the given kernels are assumed to be grouped into distinct components and each component is crucial for the learning task in hand.
In this paper, we consider the problem of decision-theoretic online learning (DTOL), proposed by Freund and Schapire [1].
The relative merits of different population coding schemes have mostly been studied (e.
Modern learning problems in computer vision, natural language processing, computational biology, and other areas are often based on large data sets of tens of thousands to millions of training instances.
An effective distance function plays an important role in many machine learning and data mining techniques.
A huge amount of images with accompanying text captions are available on the Internet.
Humans make rapid, consistent intuitive inferences about the goals of agents from the most impoverished of visual stimuli.
Given only a few examples of a particular category, people have strong expectations as to which new examples also belong to that same category.
This paper deals with the problem of single-channel signal separation – separating out signals from individual sources in a mixed recording.
The complementary nature of discriminative and generative approaches to machine learning [20] has motivated lots of research on the ways in which these can be combined [5, 12, 15, 18, 9, 24, 27].
Recent work has led to the ability to perform space efficient counting over large vocabularies [Talbot, 2009; Van Durme and Lall, 2009].
When applying off-the-shelf machine learning algorithms to data with spatial dimensions (images, geo-spatial data, fMRI, etc) a central question arises: how to incorporate prior information on the spatial characteristics of the data? For example, if we feed a boosting or SVM algorithm with individual image voxels as features, the voxel spatial information is ignored.
Completing a matrix from a few given entries is a fundamental problem with many applications in machine learning, statistics, and compressed sensing.
Learning from massive datasets, such as text, images, and high throughput biological data, has applications in various scientific and engineering disciplines.
Learning a pairwise similarity measure from data is a fundamental task in machine learning.
Decentralized and partially observable stochastic decision and planning problems are very common, comprising anything from strategic games of chance to robotic space exploration.
Learning to rank has become an important research topic in many fields, such as machine learning and information retrieval.
Many problems in machine learning and statistics involve the estimation of parameters from finite data.
Many companies that provide large scale online services, such as banking services, e-mail services, or search engines, use large server farms, often containing tens of thousands of computers in order to support fast computation with low latency.
In many fundamental problems in machine learning, such as feature selection and active learning, we try to select a subset of a finite set so that some utility of the subset is maximized.
The image basis is a key concept for understanding neural representation of visual images.
The process of training classifiers with small amounts of labeled data and relatively large amounts of unlabeled data is known as semi-supervised learning (SSL).
Gigantic quantities of visual imagery are present on the web and in off-line databases.
In this paper, we address the problem of nearest-neighbor (NN) search in large datasets of high dimensionality.
There has recently been much interest in developing statistical models for analyzing and organizing images, based on image features and, when available, auxiliary information, such as words (e.
The development of non-invasive brain computer interface (BCI) using the electroencephalogram (EEG) signal has become a very hot research topic in the BCI community [1].
In a sequence labeling problem, we are given an input sequence x and need to label each component of x with its class to produce a label sequence y.
Anomaly detection involves detecting statistically significant deviations of test data from nominal distribution.
When used to guide decision-making, linear regression analysis typically treats estimation of regression coefficients separately from their use to make decisions.
In today’s software projects, two types of source code are developed: product and test code.
Consider the problem of learning a nonlinear function f (x) on a high dimensional space x ∈ Rd .
Consider a seller who must pick from a universe of N products, N , a subset M of products to offer to his customers.
Multi-body motion segmentation concerns the separation of motions arising from multiple moving objects in a video sequence.
The last years in BCI research have seen drastically reduced training and calibration times due to the use of machine learning and adaptive signal processing techniques (see [9] and references therein) and novel dry electrodes [18].
The task of generating a topological map from video data has gained prominence in recent years.
Recently there has been great interest in social annotations, also called collaborative tagging or folksonomy, created by users freely annotating objects such as web pages [7], photographs [9], blog posts [23], videos [26], music [19], and scientific papers [5].
Estimating a vector x ∈ Rn from measurements of the form y = Φx + w, (1) where Φ ∈ Rm×n represents a known measurement matrix and w ∈ Rm represents measurement errors or noise, is a generic problem that arises in a range of circumstances.
Transduction relies on the fundamental assumption that training and test data should exhibit similar behavior.
Consider a real-world problem scenario where the challenge is to diagnose a patient who presents with several salient symptoms by performing inference with a probabilistic diagnostic model.
Many practical scenarios call for robots or agents which can learn a visual model on the fly given only a spoken or textual definition of an object category.
In real applications, the model of the problem at hand inevitably embodies some form of uncertainty: the parameters of the model are usually (roughly) estimated from data, which themselves can be uncertain due to various kinds of noises.
Factor analysis and principal component analysis (PCA) are widely used linear techniques for finding dominant patterns in multivariate datasets.
Boosting procedures attempt to improve the accuracy of general machine learning algorithms, through repeated executions on reweighted data.
A long-standing goal of unsupervised learning on images is to be able to learn the shape and form of objects from unlabelled scenes.
In this paper we develop an approach to learn stochastic 3D geometric models of object categories from single view images.
In the past, most articles investigating statistical properties of learning algorithms assumed that the observed data was generated in an i.
For better or worse, human memory is not perfect, causing us to forget.
A fundamental challenge facing reinforcement learning (RL) algorithms is to maintain a proper balance between exploration and exploitation.
Sliced inverse regression (SIR) [7] was proposed for sufficient dimension reduction.
Kernel methods [5, 24] such as Support Vector Machines (SVM) have recently attracted much attention due to their good generalization performance and appealing optimization approaches.
Picture an intense match point at the Wimbledon tennis championship, Nadal on the defense from Federer’s powerful shots.
Machine learning algorithms have been successfully applied to learning classifiers in many domains such as computer vision, fraud detection, and brain image analysis.
It has been long recognized that extracting an informative set of application-specific features from the raw data is essential in practical applications of machine learning, and often contributes even more to the success of learning than the choice of a particular classifier.
Clustering is ubiquitous in science and engineering, with numerous and diverse application domains, ranging from bioinformatics and medicine to the social sciences and the web [15].
Sparse factor models have proven to be a very versatile tool for detailed modeling and interpretation of multivariate data, for example in the context of gene expression data analysis [1, 2].
In recent years there has been an increasing interest in learning with output which differs from the case of standard classification and regression.
In the present paper, we are concerned with a class of non-linear inverse problems appearing in the structure and motion problem of multiview geometry.
Distance metric learning is a fundamental problem in machine learning and pattern recognition.
Many problems in cognitive psychology arise from questions of capacity.
Sketches are everywhere.
Bag-of-words document representations are widely used in text, image, and video processing [14, 1].
This paper proposes an unsupervised approach to the detection of regions of interest (ROIs) from a Web-sized dataset (Fig.
Graph matching and MAP inference are essential problems in computer vision and machine learning that are frequently formulated as integer quadratic programs, where obtaining an exact solution is computationally intractable.
As commercial, social, and scientific data sources continue to grow at an unprecedented rate, it is increasingly important that algorithms to process and analyze this data operate in online, or one-pass streaming settings.
Latent variable generative models are widely used in inducing meaningful representations from unlabeled data.
There has been a steady increase in the performance of object category detection as measured by the annual PASCAL VOC challenges [3].
It is widely believed that one of the main principles underlying functional organization of the early visual system is the reduction of the redundancy of relayed input from the retina.
Modern computational models of verbal memory assume that the recall of items is shaped by their semantic representations.
Supervised learning has emerged as a serious contender in the field of image segmentation, ever since the creation of training sets of images with “ground truth” segmentations provided by humans, such as the Berkeley Segmentation Dataset [15].
It is a commonly accepted hypothesis that adaptation of behavior results from changes in synaptic efficacies in the nervous system.
Gaussian processes (GPs) (see e.
Ranking text documents given a text-based query is one of the key tasks in information retrieval.
From information retrieval to recommender systems, from bioinformatics to financial market analysis, the amount of data available to researchers has exploded in recent years.
A commonly used observation model in the Gaussian process (GP) regression is the Normal distribution.
The accessibility of large quantities of off-line discrete-time dynamic data—state-action sequences drawn from real-world domains—represents an untapped opportunity for widespread adoption of reinforcement learning.
Regression is a foundational problem in machine learning and statistics.
Visual area V1 is the first area of cortex devoted to handling visual input in the human visual system (Dayan & Abbott, 2001).
Consider the problem of repeatedly choosing advertisements to display in sponsored search to maximize our revenue.
We consider the problem of estimating the value function of a given stationary policy of a Markov Decision Process (MDP).
The concave-convex procedure (CCCP) [30] is a majorization-minimization algorithm [15] that is popularly used to solve d.
Natural image statistics are a powerful tool in image processing, computer vision and computational photography.
Semi-supervised learning has attracted a lot of research focus in recently years.
Learning probabilistic graphical models from data serves two primary purposes: (i) finding compact representations of probability distributions to make inference efficient and (ii) modeling unknown data generating mechanisms and predicting causal relationships.
Factor graphs are a widely used representation for modeling complex dependencies amongst hidden variables in structured prediction problems.
Algorithms for fast indexing and search have become important for a variety of problems, particularly in the domains of computer vision, text mining, and web databases.
Nonparametric Bayesian models are now widely used in machine learning.
Compressive sensing (CS), also known as compressive sampling, has recently received increasing attention in many areas of science and engineering [3].
During haptic exploration tasks, forces are applied to the skin of the hand, and in particular to the fingertips, which constitute the most sensitive parts of the hand and are prominently involved in object manipulation/recognition tasks.
Recent technical advances in systems neuroscience allow us to monitor the activity of increasingly large neural ensembles simultaneously (e.
There are two main reasons to learn the structure of graphical models: knowledge discovery (to interpret the learned topology) and density estimation (to compute log-likelihoods and make predictions).
Household scenes commonly contain transparent objects such as glasses and bottles made of various materials (like those in Fig.
We address the problem of variable selection for regression, where a natural grouping structure exists within the explanatory variables, and the goal is to select the correct group of variables, rather than the individual variables.
Recent work in machine learning has highlighted the circumstances that appear to favor deep architectures, such as multilayer neural nets, over shallow architectures, such as support vector machines (SVMs) [1].
To understand how the brain represents and processes information we must account for two complementary properties: information is represented in a distributed fashion, and brain regions are strongly interconnected.
Finding the linear least squares fit to data is a well-known problem, with applications in almost every field of science.
In multitask learning, one is interested in learning a set of related models for predicting multiple (possibly) related outputs (i.
Kernel methods provide a principled framework for nonlinear data modeling, where the inference in the input space can be transferred intactly to any feature space by simply treating the associated kernel as inner products, or more generally, as nonlinear mappings on the data (Schölkopf & Smola, 2002).
Ranking is the central problem in many applications including information retrieval (IR).
Pearl’s belief propagation [1] provides an efficient method for exact computation in the inference with probabilistic models associated to trees.
Sequence labeling is a ubiquitous problem arising in many areas, including natural language processing [1], bioinformatics [2, 3, 4] and computer vision [5].
It has been an extensively sought-after goal to learn an appropriate distance metric in image classification and retrieval problems using simple and efficient algorithms [1–5].
Extensive games provide a general model for describing the interactions of multiple agents within an environment.
The Maximum-Weight Bipartite Matching Problem (henceforth ‘matching problem’) is a fundamental problem in combinatorial optimization [22].
Risk minimization is at the heart of many machine learning algorithms.
Living creatures occupy an environment full of uncertainty due to noisy sensory inputs, incomplete observations, and hidden variables.
What should a learning algorithm optimize on the training data in order to give classifiers having the smallest possible true risk? Many different specifications of what should be optimized on the training data have been provided by using different inductive principles.
On-line learning has been studied for decades.
Graph kernels have recently evolved into a branch of kernel machines that reaches deep into graph mining.
We consider a parameterized hidden Markov model (HMM) deﬁned on continuous state and observation spaces.
Bayesian nonparametrics have recently garnered much attention in the machine learning and statistics communities, due to their elegant treatment of infinite dimensional objects like functions and densities, as well as their ability to sidestep the need for model selection.
In this paper we consider the limit behavior of two popular semi-supervised learning (SSL) methods based on the graph Laplacian: the regularization approach [15] and the spectral approach [3].
Understanding how to recognize complex, high-dimensional audio data is one of the greatest challenges of our time.
Kernel methods are broadly established as a useful way of constructing nonlinear algorithms from linear ones, by embedding points into higher dimensional reproducing kernel Hilbert spaces (RKHSs) [9].
We consider multi-class, multi-label and multi-instance classification (M3 C), a task of learning decision rules from corpora in which each pattern consists of multiple instances1 and is associated with multiple classes.
In high-throughput regression problems, the cost of evaluating test samples is just as important as the accuracy of the regression and most non-parametric regression techniques do not produce models that admit efficient implementation, particularly in hardware.
Semi-supervised learning, i.
Source separation problems arise when a number of signals are mixed together, and the objective is to estimate the underlying sources based on the observed mixture.
Many problems in signal processing, machine learning, and communications can be cast as a linear regression problem where an unknown signal x ∈ RN is related to its observations y ∈ RM via y = Φx + n.
Learning with relational data, or sets of propositions of the form (object, relation, object), has been important in a number of areas of AI and statistical data analysis.
In this work we consider learning on a graph.
Conditional random fields [1], or discriminatively trained undirected graphical models, have become the tool of choice for addressing many important tasks across bioinformatics, natural language processing, robotics, and many other fields [2, 3, 4].
The idea of search bootstrapping is to adjust the parameters of a heuristic evaluation function towards the value of a deep search.
In most real-world machine learning problems (e.
In many fields of science and engineering (among them genomics, financial engineering, natural language processing, remote sensing, and social network analysis), one encounters statistical inference problems in which the number of predictors p is comparable to or even larger than the number of observations n.
Functional MRI (fMRI) studies of human neuroanatomical organization commonly analyze fMRI data across a population of subjects.
For many application areas, such as text analysis and image analysis, learning a tree-based hierarchy is an appealing approach to illuminate the internal structure of the data.
Many problems in modern science and engineering involve high-dimensional data, by which we mean that the ambient dimension p in which the data lies is of the same order or larger than the sample size n.
Students often face the task of memorizing facts such as foreign language vocabulary or state capitals.
Convex optimization forms the backbone of many algorithms for statistical learning and estimation.
Inductive learning the process by which a new concept or category is acquired through observation of exemplars - poses a fundamental theoretical problem for cognitive science.
Probabilistic topic models have become popular tools for the unsupervised analysis of large document collections [1].
In recent years machine learning-based approaches to computer vision have helped to greatly accelerate progress in the field.
Suppose we have a large database of images, and we want to learn to predict who or what is in any given one.
In Natural Language Processing (NLP), the task of event coreference has numerous applications, including question answering, multi-document summarization, and information extraction.
Central to semi-supervised learning is the question how unlabeled data can help in either classification or regression.
Distributions over permutations play an important role in applications such as multi-object tracking, visual feature matching, and ranking.
There has recently been considerable interest in structure learning of Bayesian networks.
Magnetic resonance imaging (MRI) [10, 6] is a very flexible imaging modality.
Online learning has been extensively studied in the machine learning community (Rosenblatt, 1958; Freund & Schapire, 1999; Kivinen et al.
Sparseness is being regarded as one of the key features in machine learning [15] and biology [16].
Stochastic optimal control is of interest in many fields of science and engineering, however it remains hard to solve.
Finding a precise statistical characterization of natural images is an endeavor that has concerned research for more than fifty years now and is still an open problem.
Languages have a complex structure, full of general rules with idiosyncratic exceptions.
Topic models such as latent Dirichlet allocation (LDA) [3] have been recognized as useful tools for analyzing large, unstructured collections of documents.
Detecting abnormal points in small and simple datasets can often be performed by visual inspection, using notably dimensionality reduction techniques.
The abstraction of Markov random field (MRF) allows one to utilize graphical representation to capture inter-dependency between large number of random variables in a succinct manner.
The Indian buffet process (IBP) is an infinitely exchangeable distribution over binary matrices with a finite number of rows and an unbounded number of columns [1, 2].
Neural implementations of reinforcement learning have to solve two basic credit assignment problems: (a) the temporal credit assignment problem, i.
In a typical high dimensional setting, the number of variables p is much larger than the number of observations n.
Neurons in the visual cortex of primates and many other mammals are organized according to their tuning properties.
Far from being static relays, synapses are complex dynamical elements.
There is a growing body of experimental evidence consistent with the idea that animals are somehow able to represent, manipulate and, ultimately, make decisions based on, probability distributions.
Active learning addresses the problem that the algorithm is given a pool of unlabeled data drawn i.
Recent work on deep belief nets (DBNs) [10], [13] has shown that it is possible to learn multiple layers of non-linear features that are useful for object classification without requiring labeled data.
Over the past several decades, researchers in many different fields—statistics, economics, physics, genetics and machine learning—have focused on coming up with more accurate and more efficient approximate solutions to intractable probabilistic inference problems.
Spectral techniques are an authentic workhorse in machine learning, statistics, numerical analysis, and signal processing.
Statistical analysis of social networks and other relational data has been an active area of research for over seventy years and is becoming an increasingly important problem as the scope and availability of social network datasets increase [1].
Linear classifiers (i.
The assumption that training and test data are governed by identical distributions underlies many popular learning mechanisms.
Search engines typically use a ranking function to order results.
The problem of learning rotations finds application in many areas of signal processing and machine learning.
Incremental decremental algorithm for online learning of Support Vector Machine (SVM) was previously proposed in [1], and the approach was adapted to other variants of kernel machines [2–4].
Texture learning and synthesis are important tasks in computer vision and graphics.
Recent work has focused on the learning of similarity metrics within the context of nearest-neighbor (NN) classification [7, 8, 12, 15].
Graphical models have proven themselves to be an effective tool for representing the underlying structure of probability distributions and organizing the computations required for exact and approximate inference.
Structure learning aims to discover the topology of a probabilistic network of variables such that this network represents accurately a given dataset while maintaining low complexity.
Online learning has become the paradigm of choice for tackling very large scale estimation problems.
In this work we study the use of resting state activity for the semi-supervised analysis of human fMRI studies.
Probabilistic topic models [2, 9, 6] are often used to analyze and extract semantic topics from large text collections.
Suppose x ∈ Rn is a sparse vector, meaning its number of nonzero components k is smaller than n.
The Partially Observable Markov Decision Process (POMDP) model has proven attractive in domains where agents must reason in the face of uncertainty because it provides a framework for agents to compare the values of actions that gather information and actions that provide immediate reward.
In recent years, machine learning approaches for analyzing fMRI data have become increasingly popular [15, 24, 18, 16].
Understanding how the nervous system makes sense of uncertain visual stimuli is one of the central goals of perception research.
We study the learning ability of classifiers trained on examples generated from different sources, but where some observations are partially missing.
It is well known that natural images occupy a small fraction of the space of all possible images.
With the advent of compressive sensing and other related applications, there has been growing interest in finding sparse signal representations from redundant dictionaries [3, 5].
Many natural language processing applications, from discriminative training [18, 9] to minimumrisk decoding [16, 34], require the computation of expectations over large-scale combinatorial spaces.
Learning algorithms based on kernels have been used with much success in a variety of tasks [17,19].
Relaxations are a popular approach for tackling intractable optimization problems.
Relative to parametric methods, nonparametric regressors require few structural assumptions on the function being learned.
In machine learning, online algorithms operate by repetitively drawing random examples, one at a time, and adjusting the learning variables using simple calculations that are usually based on the single example only.
Policy Gradient Reinforcement Learning (PGRL) attempts to find a policy that maximizes the average (or time-discounted) reward, based on gradient ascent in the policy parameter space [2, 3, 4].
Markov random fields (MRF’s) provide a powerful tool for representing dependency structure between random variables.
Several models of object recognition drawing inspiration from visual cortex have been developed over the past few decades [3, 8, 6, 12, 10, 9, 7], and have enjoyed substantial empirical success.
Learning algorithms based on kernel methods have enjoyed considerable success in a wide range of supervised learning tasks, such as regression and classification [25].
It is widely acknowledged that identifying the regions that originate malicious traffic on the Internet is vital to network security and management, e.
Visualization as an important tool for exploratory data analysis has attracted much research effort in recent years.
Object class detection has been one of the mainstream research areas in computer vision.
Learning to rank has attracted the focus of many machine learning researchers in the last decade because of its growing application in the areas like information retrieval (IR) and recommender systems.
“Average-case” Investing: Much of mathematical finance theory is devoted to the modeling of stock prices and devising investment strategies that maximize wealth gain, minimize risk while doing so, and so on.
The linear correlation coefficient is of central importance in many studies that deal with spike count data of neural populations.
We seek to integrate knowledge from multiple information sources.
It is well-known that synapses change their synaptic efficacy (“weight”) w in dependence of the difference tpost − tpre of the firing times of the post- and presynaptic neuron according to variations of a generic STDP rule (see [1] for a recent review).
Natural images, although apparently diverse, have a surprisingly regular statistical regularity.
Recently, there has been a lot of interest in the problem of designing compact binary codes for reducing storage requirements and accelerating search and retrieval in large collections of highdimensional vector data [11, 13, 15].
Linear Dyna-style planning extends Dyna to linear function approximation (Sutton, Szepesvári, Geramifard & Bowling, 2008), and can be used in large-scale applications.
In many applications, one would like to discover and model dynamical behaviors which are shared among several related time series.
Electroencephalography (EEG) and magnetoencephalography (MEG) provide an instantaneous and non-invasive measure of brain activity.
Since the presence of supervision in biological learning mechanisms is rare, organisms often have to rely on the ability of these mechanisms to extract statistical regularities from their environment.
Since William James first described the phenomenology of attention [11], psychologists have been struggling to specify the cognitive architecture of this process, how it is limited, and how it helps information processing.
Analysis of biological networks has led to numerous advances in understanding the organizational principles and functional properties of various biological systems, such as gene regulatory systems [1] and central nervous systems [2].
Range search is a fundamental proximity task at the core of many learning problems.
Online decision-making is a learning problem in which one needs to choose a decision repeatedly from a given set of decisions, in an effort to minimize costs over the long run, even in the face of complete uncertainty about future outcomes.
A continuous-time Markov chain (CTMC) is a model of a dynamical system which, upon entering some state, remains in that state for a random real-valued amount of time (called the dwell time or occupancy time) and then transitions randomly to a new state.
Discovering how DNA-binding proteins called transcription factors (TFs) regulate gene expression programs in living cells is fundamental to understanding transcriptional regulatory networks controlling development, cancer, and many human diseases.
A central problem in computational neuroscience is to develop functional models that can accurately describe the relationship between external variables and neural spike trains.
People appear to make rational statistical inferences from noisy, uncertain input in a wide variety of perceptual and cognitive domains [1, 9].
Probabilistic models provide a powerful and intuitive framework for formulating several problems in machine learning and its application areas, such as computer vision and computational biology.
Image understanding is one of the Holy Grail problems in computer vision.
Models based on local features have achieved state-of-the art results in many visual object recognition tasks.
The Fisher linear discriminant analysis (LDA) is a classical method that considers dimensionality reduction and classification jointly.
Conditional maximum entropy models [1, 3], conditional maxent models for short, also known as multinomial logistic regression models, are widely used in applications, most prominently for multiclass classification problems with a large number of classes in natural language processing [1, 3] and computer vision [12] over the last decade or more.
That visual input at a given point is greatly influenced by its spatial context is manifest in a host of neural and perceptual effects (see, e.
This paper studies learning problems of the following form.
Invariance to abstract input variables is a highly desirable property of features for many detection and classification tasks, such as object recognition.
L1 regularization has become an important tool over the last decade with a wide variety of machine learning applications.
Energy issues present one of the largest challenges facing our society.
Conditional random fields (CRFs, [1]) have been successful in modeling complex systems, with applications from speech tagging [1] to heart motion abnormality detection [2].
Let Y be a p-dimensional random vector with distribution P .
The problem of sparse estimation is becoming increasingly important in machine learning and statistics.
In this paper we consider the problem of estimating the support of an arbitrary probability distribution and we are more broadly motivated by the problem of learning from complex high dimensional data.
Linear discriminative online algorithms have been shown to perform very well on binary and multiclass labeling problems [10, 6, 14, 3].
Semi-supervised learning algorithms use both labeled and unlabeled examples.
Consider the indoor image shown in Figure 1.
Researchers have been increasingly relying on cross species analysis to understand how biological systems operate.
Dimensionality reduction is an important aspect of many statistical learning tasks.
In recent years interest has been building [10, 21, 16, 8, 12] in the problem of detecting locations in the visual field that are responsible for auditory signals.
In the past decade, computer vision research in object recognition has firmly established the efficacy of representing images as collections of local descriptors of edge orientation.
The invention of the perceptron [12] goes back to the very beginning of AI more than half a century ago.
Clustering algorithms group data items into categories without requiring human supervision or definition of categories.
Clustering has traditionally been a tool of unsupervised learning.
Over the past decade the amount of available data has increased steadily.
Neurons communicate mostly through noisy sequences of action potentials, also known as spike trains.
A clustering of a finite set V of data points is a partition of V into subsets (called clusters) such that data points in the same cluster are similar to each other.
In 24 hours an electrocardiogram (ECG) can record over 100,000 heartbeats for a single patient.
Accurate wind speed and direction forecasts are essential for efficient integration of wind energy into electrical transmission systems.
There is currently considerable interest in structure learning of dynamic Bayesian networks (DBNs), with a variety of applications in signal processing and computational biology; see e.
Network-structured optimization problems arise in a variety of application domains within the information sciences and engineering.
Consider a classification task where a learner is given training items x1 , .
Suppose you are asked to make a series of moral judgments by rating, on a 1–10 scale, various actions, with a rating of 1 indicating ’not particularly bad or wrong’ and a rating of 10 indicating ’extremely evil.
In the last decade, there has been much research at the interface of computer science and game theory (see e.
How should we perform experiments to determine the most accurate scientific theory among competing candidates, or choose among expensive medical procedures to accurately determine a patient’s condition, or select which labels to obtain in order to determine the hypothesis that minimizes generalization error? In all these applications, we have to sequentially select among a set of noisy, expensive observations (outcomes of experiments, medical tests, expert labels) in order to determine which hypothesis (theory, diagnosis, classifier) is most accurate.
Recently, there has been a push towards combining logical and probabilistic approaches in Artificial Intelligence.
Active learning is well-motivated in many supervised learning scenarios where unlabeled instances are abundant and easy to retrieve but labels are difficult, time-consuming, or expensive to obtain.
Continuous latent factor models, such as factor analysis (FA) and probabilistic principal components analysis (PPCA), are very commonly used density models for continuous-valued data.
Probabilistic grammars are an important statistical model family used in natural language processing [7], computer vision [16], computational biology [19] and more recently, in human activity analysis [12].
Sequential decision-making problems under uncertainty and partial observability are typically modeled using Partially Observable Markov Decision Processes (POMDPs) [1], where the objective is to decide how to act so that the sequence of visited states optimizes some performance criterion.
The exact solution for the reinforcement learning (RL) and planning problems with large state space is difficult or impossible to obtain, so one usually has to aim for approximate solutions.
High-dimensional data sets present challenges that are both statistical and computational in nature.
Control of sensorimotor systems, artificial or biological, is inherently both a spatial and temporal process.
The problem of identifying high-dimensional activation patterns embedded in noise is important for applications such as contamination monitoring by a sensor network, determining the set of differentially expressed genes, and anomaly detection in networks.
Many machine learning algorithms can be formulated as a penalized optimization problem: min l(x) + λφ(x), x (1) where l(x) is the empirical loss function (e.
Support vector machines (SVMs) as proposed in [1, 8] are powerful classifiers, especially in the binary case of two possible classes.
The classic dichotomy between generative and discriminative methods for classification in machine learning can be clearly seen in two distinct performance regimes as the number of training examples is varied [12, 18].
In this paper we tackle the problem of obtaining a tight characterization of the sample complexity which a particular learning rule requires, in order to learn a particular source distribution.
Since the beginning of neuromorphic engineering [1, 2] designers have had great success in building VLSI1 neurons mimicking the behavior of biological neurons using analog circuits [3–8].
Let V be a set of |V | = n nodes and B ⊂ K1/2 ⊂ K be the following sets: B = {0, 1}V K1/2 = {0, 21 , 1}V K = [0, 1]V A function f : B → R is called pseudo-boolean.
Determining the pose of a human body from one or more images is a central problem in Computer Vision.
Many machine learning algorithms minimize a regularized risk [1]: m J(θ) = Ω(θ) + Remp (θ), where Remp (θ) = 1 � l(xi , yi , θ).
Black-box identification approaches are widely used to learn dynamic models from a finite set of input/output data [1].
With the development of advanced data collection techniques, large quantities of high-dimensional data are commonly available in many applications.
Alignment of functional neuroanatomy across individuals forms the basis for the study of the functional organization of the brain.
Consider the advertisement display problem, where a search engine company chooses an ad to display which is intended to interest the user.
Understanding the meanings and contents of images remains one of the most challenging problems in machine intelligence and statistical learning.
Probabilistic topic models like the latent Dirichlet allocation (LDA) [5] have recently been applied to a number of computer vision tasks such as objection annotation and scene classification due to their ability to capture latent semantic compositions of natural images [22, 23, 9, 13].
Research on Multiple Kernel Learning (MKL) needs to follow a two pronged approach.
Complex visual hallucinations [1] can offer a fascinating insight into how the brain realizes visual perception.
In many implantable biomedical microsystems [1, 2], an embedded system capable of recognising high-dimensional, time-varying signals have been demanded.
Consider a classification task, in which the objective is to assign labels Y to vectors of one or more attribute values X.
In applications such as active learning [1, 2, 3, 4], disease/fault diagnosis [5, 6, 7], toxic chemical identification [8], computer vision [9, 10] or the adaptive traveling salesman problem [11], one often encounters the problem of identifying an unknown object while minimizing the number of binary questions posed about that object.
Sparsity has become a popular way to deal with small samples of high dimensional data and, in a broad sense, refers to the possibility of writing the solution in terms of a few building blocks.
The Curse of Dimensionality [2] has inspired research in several directions in Computer Science and has led to the development of several novel techniques such as dimensionality reduction, sketching etc.
Two themes dominate recent progress towards situated visual object recognition.
In the field of population coding, a recurring question is the impact on coding efficiency of so-called noise correlations, i.
Thanks to a sustained world-wide effort, modern automatic speech recognition technology has now reached a level of performance that makes it suitable as an enabling technology for novel applications such as automated dictation, speech based car navigation, multimedia information retrieval, etc.
Multi-instance Learning (MIL) was first proposed by Dietterich et.
In this paper we study the general affine rank minimization problem (ARMP), min rank(X) s.
Action potentials are stereotyped all-or-nothing events, meaning that their amplitude is not considered to transmit any information and only the exact time of occurrence matters.
We address the problem of prediction in graphical models that are computationally challenging because of both high-treewidth and large state-spaces.
Utility elicitation is a key component in many decision support applications and recommender systems, since appropriate decisions or recommendations depend critically on the preferences of the user on whose behalf decisions are being made.
In the classical supervised machine learning paradigm the learner is given a labeled training set of examples and her goal is to find a decision function with the small generalization error on the unknown test examples.
In machine learning it is widely known that if several tasks are related, then learning them simultaneously can improve performance [1–4].
Neuronal elements of the brain constitute an intriguing complex network [4].
Producing large-scale training, validation and test sets is vital for many applications.
Multiple Instance Learning (MIL) has been proposed over 10 years ago as a methodology to learn models under weak labeling constraints [1].
The sequence memoizer (SM) is a Bayesian nonparametric model for discrete sequence data producing state-of-the-art results for language modeling and compression [1, 2].
Let x0 ∈ RN be an unknown vector, and assume that a vector y ∈ Rn of noisy linear measurements of x0 is available.
The emergence of organization is at the core of many complex systems, from neural cell assemblies to living insect societies.
The nature of encoding and processing of sensory information in the visual, auditory and olfactory systems has been extensively investigated in the systems neuroscience literature.
Feature selection, the process of selecting a subset of relevant features, is a key component in building robust machine learning models for classification, clustering, and other tasks.
In recent years sparse coding has become a popular paradigm to learn dictionaries of natural images [10, 1, 4].
Continuous Markov random fields are a general and expressive formalism to model complex probability distributions over multiple continuous random variables.
Learning to rank has recently gained much attention in machine learning, due to its wide applications in real problems such as information retrieval (IR).
Given an undirected, weighted graph, the commute distance between two vertices u and v is defined as the expected time it takes a random walk starting in vertex u to travel to vertex v and back to u.
Image representation (features) is arguably the most fundamental task in computer vision.
Learning from weakly annotated data is a long standing goal for the practical application of machine learning techniques to real world data.
One way the human brain manages the massive amount of sensory information it receives is by learning invariants — regularities in its input that do not change across many stimuli sharing some property of interest.
Acoustic modeling is a fundamental problem in automatic continuous speech recognition.
Optimal classifiers minimize the expected value of a loss function, or risk.
Monte-Carlo tree search (MCTS) is a new approach to online planning that has provided exceptional performance in large, fully observable domains.
Social network analysis has traditionally relied on self-reported data collected via interviews and questionnaires [27].
Online learning algorithms are simple, fast, and require less memory compared to batch learning algorithms.
Huge quantities of text data such as news articles and blog posts arrives in a continuous stream.
Least-squares temporal difference (LSTD) learning [3, 2] is a widely used reinforcement learning (RL) algorithm for learning the value function V π of a given policy π.
Local learning machines such as nearest neighbors classifiers, radial basis function (RBF) kernel machines or linear classifiers predict the class of new data points from their neighbors in the input space.
Rank aggregation aims at combining multiple rankings of objects to generate a better ranking.
This paper is concerned with the problem of model selection (or structure learning) in Gaussian graphical modelling.
Eigenvalue problems associated to a symmetric and positive semi-definite matrix are quite abundant in machine learning and statistics.
In multivariate data analysis, graphical models such as Gaussian Markov Random Fields provide a way to discover meaningful interactions among variables.
In reinforcement learning, an agent interacts with the environment, learning through trial-and-error based on scalar reward signals.
Sparse coding has attracted significant attention in recent years because it has been shown to be effective for some classification problems [12, 10, 9, 13, 11, 14, 2, 5].
Hierarchical reinforcement learning [1] offers an appealing family of approaches to scaling up standard reinforcement learning (RL) [2] methods by enabling the use of both low-level primitive actions and higher-level macro-actions (or skills).
Markov decision processes (MDPs) are a well-studied model of sequential decision-making under uncertainty [11].
There has been an explosion of interest in machine learning over the past decade, much of which has been fueled by the phenomenal success of binary Support Vector Machines (SVMs).
Probabilistic mixture modeling [7] has been widely used for density estimation and clustering applications.
Decision making has often been studied in experiments in which a subject repeatedly chooses actions and rewards are given depending on the action.
We consider ordinary least-squares regression using randomly generated feature spaces.
Simultaneous analysis of groups of system components with similar functions, or subsystems, has recently received considerable attention.
Imagine that you are traveling in a moving car and observe a walker through a fence full of punch holes.
Many application problems need to simultaneously predict several quantities using a common set of variables, e.
Learning kernel functions is an ongoing research topic in machine learning that focuses on learning an appropriate kernel function for a given task.
The prediction of the future state of an evolving graph is a challenge of interest in many applications such as predicting hyperlinks of webpages [16], finding protein-protein interactions [7], studying social networks [9], as well as collaborative filtering and recommendations [6].
CUR decompositions are a recently-popular class of randomized algorithms that approximate a data matrix X ∈ Rn×p by using only a small number of actual columns of X [12, 4].
Bayesian methods in machine learning, although elegant and concrete, have often been criticized not only for their computational cost, but also for their strong assumptions on the correctness of the prior distribution.
We address the reinforcement learning (RL) problem of finding a good policy in an unknown, stochastic, and partially observable domain, given both data from independent exploration and expert demonstrations.
In active learning, a learner is given access to unlabeled data and is allowed to adaptively choose which ones to label.
Graphical models provide an effective framework to model complex systems via simpler local interactions and also provide an insight into the structure of the underlying probabilistic model.
Convolutional neural networks (CNNs) [1] have been successfully applied to many recognition tasks.
The need for distributions over sets of structures arises frequently in computer vision, computational biology, and natural language processing.
In the nervous system, sensory and motor information, as well as internal brain states, are represented by action potentials in populations of neurons.
The advent of functional neuroimaging techniques, in particular fMRI, has for the first time provided non-invasive, large-scale observations of brain processes.
Parametric flow problems have been well-studied in operations research [7].
For many animals, it is vital to be able to quickly locate the source of an unexpected sound, for example to escape a predator or locate a prey.
In many scientific and engineering applications, such as image annotation [28] and web-page classification [6], the available data usually come from diverse domains or are extracted from different aspects, which will be referred to as views.
Convex optimization has become a key tool in many machine learning algorithms.
This paper deals with the problem of recognizing transitive actions in single images.
We consider the problem of maximizing an unknown function f (x) when each evaluation of the function has a high cost.
Compilation to arithmetic circuits (ACs) [1] is one of the most effective methods for exact inference in Bayesian networks.
Look at the two persons in Fig.
The broad goal of supervised learning is to effectively learn unknown functional dependencies between a set of input variables and a set of output variables, given a finite collection of training examples.
Consider the subject of density estimation for high-dimensional continuous random variables, like images.
The k-means clustering algorithm [16] was recently recognized as one of the top ten data mining tools of the last fifty years [20].
The motivation of the present work lies in the growing interest of the machine learning community for learning tasks that are richer than now well-studied classification and regression.
As software systems grow in size and complexity, they become more difficult to develop and maintain.
Assume we have a set of n data observations (x1 , y1 ), .
Coherence is perhaps the most fundamental property of probability estimation.
Q-learning is a popular reinforcement learning algorithm that was proposed by Watkins [1] and can be used to optimally solve Markov Decision Processes (MDPs) [2].
It has been widely accepted that biological sensory systems are adapted to match the statistical properties of the signals in the natural environments.
A key issue in computational neuroscience is the interpretation of neural signaling, as expressed by a neuron’s sequence of action potentials.
Layered models of scenes offer significant benefits for optical flow estimation [8, 11, 25].
In standard supervised learning, each training sample is associated with a label, and the classifier is usually trained through the minimization of the empirical risk on the training set.
Brain image analyses have widely relied on univariate voxel-wise analyses, such as voxel-based morphometry (VBM) for structural MRI [1].
Many bottom up theories of neural encoding posit that sensory systems are optimized to represent sensory information, subject to limitations of noise and resources (e.
Multi-task learning (MTL) [6, 8, 19] refers to the joint training of multiple problems, enforcing a common intermediate parameterization or representation.
Policy gradient methods have been some of the most effective learning algorithms for dynamic control tasks in robotics.
Consider a general two-hidden-layer multilayer perceptron (MLP) having a single (terminal) output, H nodes at the second hidden layer (next to the terminal layer), I nodes at the first hidden layer, and J nodes at the input layer; hence, a J-I-H-1 MLP.
Size, color, and orientation have long been considered elementary features [14] that are available to guide attention and visual search [17].
There has been significant recent research on the analysis of incomplete matrices [10, 15, 1, 12, 13, 18].
Robust statistics is a well established field that analyzes the sensitivity of common estimators to outliers and provides alternative estimators that achieve improved robustness [11, 13, 17, 23].
Unlike standard supervised learning problems which involve simple scalar outputs, structured prediction deals with structured outputs such as sequences, grids, or more general graphs.
Conventional views of language acquisition often assume that human language learners initially use a single source of information to acquire one component of language, which they then use to leverage the acquisition of other linguistic components.
Over the last few years, a growing amount of research on visual recognition has focused on learning low-level and mid-level features using unsupervised learning, supervised learning, or a combination of the two.
Heavy-tailed distributions naturally occur in many real life phenomena, for example in computer networks [23, 14, 16].
Efficient similarity search with large databases is central to many applications of interest, such as example-based learning algorithms, content-based image or audio retrieval, and quantization-based data compression.
Recent studies have shown promising performance of kernel methods for object classification, recognition and localization [1].
The efficient computation of the similarity (or overlap) between sets is a central operation in a variety of applications, such as word associations (e.
Continuous time stochastic processes are receiving increasing attention within the statistical machine learning community, as they provide a convenient and physically realistic tool for modelling and inference in a variety of real world problems.
Image understanding is a central problem in computer vision that has been extensively studied in the forms of various types of tasks.
The CTBT aims to prevent the proliferation and the advancement of nuclear weapon technology by banning all nuclear explosions.
Semi-supervised learning methods make assumptions about how unlabeled data can help in the learning process, such as the manifold assumption (data lies on a low-dimensional manifold) and the cluster assumption (classes are separated by low density regions) [4, 16].
In natural behavior as well as in engineering applications, there is often the need to choose, under time pressure, an action among multiple options with imprecisely known consequences.
One of the most striking features of spike trains is their variability – that is, the same visual stimulus does not elicit the same spike pattern on repeated presentations.
We present a generative probabilistic model for learning concept graphs from text.
Many probabilistic models incorporate multivariate Gaussian distributions to explain dependencies between variables.
Understanding how the brain performs statistical inference is one of the main problems of theoretical neuroscience.
Statistical relational learning has attracted ever-growing interest in the last decade, because of widely available relational data, which can be as complex as citation graphs, the World Wide Web, or relational databases.
Latent Dirichlet Allocation [4] assigns topics to documents and generates topic distributions over words given a collection of texts.
Many computer vision problems such as SfM [26], non-rigid SfM [3] and photometric stereo [11] can be formulated as a matrix factorization problem.
In many forensic domains it is necessary to characterize the degree to which a given piece of evidence is unique.
We consider the nonparametric problem of estimating Rényi α-entropy and mutual information (MI) based on a finite sample drawn from an unknown, absolutely continuous distribution over Rd .
There has recently been significant interest in the analysis of data sets whose individual records are too sensitive to expose directly, examples of which include medical information, financial data, and personal data from social networking sites.
Datasets available for prediction tasks are growing over time, resulting in increasing scale in all their measurable dimensions: separate from the issue of the growing number of examples m and features d, they are also growing in the number of classes k.
In spite of the wide use of clustering in many practical applications, currently, there exists no principled method to guide the selection of a clustering algorithm.
Many computer vision problems inherently involve data that is represented by multiple modalities such as different types of image features, or images and surrounding text.
Latent force models [1] are a new approach for modeling data that allows combining dimensionality reduction with systems of differential equations.
How can we reasonably expect to learn given possibly adversarial data? Overcoming this obstacle has been one of the major successes of the Online Learning framework or, more generally, the so-called competitive analysis of algorithms: rather than measure an algorithm only by the cost it incurs, consider this cost relative to an optimal “comparator algorithm” which has knowledge of the data in advance.
Finding the place (or time) where most or all of a set of one-dimensional signals (or profiles) jointly change in some specific way is an important question in several fields.
Hierarchical Bayesian modeling has become a mainstay in machine learning and applied statistics.
In the classical K-armed bandit problem, an agent selects at each time step one of the K arms and receives a reward that depends on the chosen action.
Many applications require classification techniques dealing with train and/or test instances with missing features: e.
The concept of sparsity is widely used in the signal processing, machine learning and statistics communities for model fitting and solving inverse problems.
Motion capture systems have become widespread in many application areas such as robotics [18], physical therapy, sports sciences [10], virtual reality [15], artificial movie generation [13], computer games [1], etc.
The focus of this paper is a novel Bayesian framework for learning with probabilistic deterministic finite automata (PDFA) [9].
This paper presents: (1) a new formulation of image segmentation as the maximum-weight independent set (MWIS) problem; and (2) a new algorithm for solving MWIS.
In stochastic optimization, a decision maker makes a decision and faces a random cost based on that decision.
Diffusion Tensor Imaging (DTI or DT-MR) is an imaging modality that facilitates measurement of the diffusion of water molecules in tissues.
The concept of parsimony is central in many scientific domains.
Action potentials play the central role in neuron-to-neuron communication.
This paper addresses the problem of evaluating a given model in terms of its predictive performance.
In traditional bandit models, the learner is presented with a set of K actions.
We develop a novel generative model which combines the powerful invariance properties achieved through the use of hidden variables in epitome [2] and stel (structural element) models [6, 8].
A graph G = (V, E) denotes a collection of entities, represented by vertices V , along with some relationship between pairs, represented by edges E.
The last few years have seen a proliferation of human efforts to collect labeled image data sets for the purpose of training and evaluating visual recognition systems.
Motivated by [KS02, KK99] among others, Li, Littman and Walsh [LLW08] introduced the KWIK framework for online learning, standing for knows what it knows.
The sparse signal recovery problem has been studied in many areas including machine learning [18, 19, 22], signal processing [8, 14, 17], and mathematics/statistics [2, 5, 7, 10, 11, 12, 13, 20].
In this work, we focus on the pool-based active learning, which selects an unlabeled instance from a given pool for manually labeling.
Reinforcement learning can be used to handle policy search problems in unknown environments.
Many of the world’s languages are sensitive to word order.
Trace-norm regularization is a popular approach for matrix completion and collaborative filtering, motivated both as a convex surrogate to the rank [7, 6] and in terms of a regularized infinite factor model with connections to large-margin norm-regularized learning [17, 1, 15].
Integrating AI with Human Computer Interaction has received significant attention in recent years [8, 11, 13, 3, 2].
Latent variable models provide an elegant formulation for several applications of machine learning.
If Sparse Coding (SC, [1]) or Independent Component Analysis (ICA; [2, 3]) are applied to image patches, basis functions are inferred that closely resemble Gabor wavelet functions.
Graphical models such as Markov Random Fields (MRFs) have been successfully applied to a wide variety of applications such as image understanding [1], error correcting codes [2], protein folding [3] and multi-agent systems coordination [4].
As graphical models are applied to more complex problems, it is increasingly necessary to learn parameters from data.
We are increasingly confronted with very high dimensional data in speech signals, images, geneexpression data, and other sources.
This paper is about the following problem: suppose we are given a large data matrix M , and we know it can be decomposed as M = L0 + C0 , where L0 is a low-rank matrix, and C0 is non-zero in only a fraction of the columns.
Preference elicitation (PE) is an important component of interactive decision support systems that aim to make optimal recommendations to users by actively querying their preferences.
Multivariate real-valued distributions are of paramount importance in a variety of fields ranging from computational biology and neuro-science to economics to climatology.
Markov Chain Monte Carlo (MCMC) methods have gained enormous popularity over a wide variety of research fields [6, 8], owing to their ability to compute expectations with respect to complex, high dimensional probability distributions.
The counting problem is the estimation of the number of objects in a still image or video frame.
Analysis of “relational data”, such as the hyperlink structure on the Internet, friend links on social networks, or bibliographic citations between scientific articles, is useful in many aspects.
A foundational concept in modern machine learning is to construct models for data by balancing the complexity of the model against fidelity to the measurements.
Learning algorithms based on 𝑙1 regularization have a long history in machine learning and statistics.
Many modern software systems compute a result as the solution, or approximate solution, to an optimization problem.
We wish to estimate the value function of a policy in an unknown decision process in a high dimensional and partially-observable environment.
In the past years, kernel methods, like support vector machines (SVM), have achieved great success in many learning problems, such as classification and regression.
The framework we propose is applicable in the following setup: let C denote a combinatorial space, by which we mean a finite but large set, where P testing membership is tractable, but enumeration is not, and suppose that the goal is to compute x∈C f (x), where f is a positive function.
In this work, we are interested in the problem of extracting the CPU instructions that comprise a binary executable file.
The contribution of this paper is the learning theoretical analysis of kernel-based least squares regression in combination with conjugate gradient techniques.
Learning procedures based on positive semidefinite (psd) kernel functions, like Support vector machines (SVMs), have emerged as powerful tools for several learning tasks with wide applicability [13].
A domain adaptation approach for NLP tasks, termed E ASYA DAPT (EA), augments the source domain feature space using features from labeled data in target domain [1].
Latent representations of data are wide-spread tools in supervised and unsupervised learning.
There are many applications where a classifier must be designed under computational constraints.
Consider empirical risk minimization for a hypothesis class H = {h : X → R} w.
Using Markov random fields (MRFs) one can capture global statistical properties in large scale probabilistic networks while only explicitly modeling the interactions of neighboring sites.
A central preoccupation of learning theory is to understand what statistical estimation based on a finite data set reveals about the underlying distribution from which the data were sampled.
Two salient features of cortical networks are the numerous recurrent lateral connections within a cortical area and the high ratio of cortical cells to sensory input channels.
Inverse reinforcement learning aims to find a reward function for a Markov decision process, given only example traces from its optimal policy.
Sparse linear models have become a popular framework for dealing with various unsupervised and supervised tasks in machine learning and signal processing.
A problem common to both epidemiology and to systems biology is to infer causal relationships between phenotypic measurements (biomarkers) and disease outcomes or quantitative traits.
In medicine, as in many other disciplines, decisions are often based upon a comparative analysis.
Neurons are the predominant component of the nervous system and understanding them is a major challenge in modern neuroscience research [1].
In active learning [10, 13, 16], the learner draws unlabeled data from the unknown distribution defined on the learning task and actively queries some labels from an oracle.
Apprenticeship learning is a variant of reinforcement learning, first introduced by Abbeel & Ng [1] (see also [2, 3, 4, 5, 6]), designed to address the difficulty of correctly specifying the reward function in many reinforcement learning problems.
Consider the binary classification problem, where each input is classified into +1 or −1.
Many difficult problems have been shown to admit elegant and tractably computable representations via optimization over the set of positive semidefinite (PSD) matrices.
In real-world applications of machine learning, often the sampling of the training and test instances may differ, which results in a mismatch between the two distributions.
Undirected graphical models have emerged as a useful tool because they allow for a stochastic description of complex associations in high-dimensional data.
Learning curves are a convenient way of characterising the performance that can be achieved with machine learning algorithms: they give the generalisation error  as a function of the number of training examples n, averaged over all datasets of size n under appropriate assumptions about the data-generating process.
Assume a standard setting with data D = {(xi , yi )}ni=1 , where (xi , yi ) are sampled iid from the joint distribution p(x, y) on Rd × {±1}.
For more than a decade, kernel methods such as support vector machines (SVMs) have belonged to the most successful learning methods.
Communication and inference are intimately linked.
Many learning problems involve models represented in matrix form.
Sequential decision making in stochastic dynamic environments, also called the “planning problem,” is often modeled using a Markov Decision Process (MDP, cf [1, 2, 3]).
Elucidating neural encoding is one of the most important issues in neuroscience.
Discovering causal relations from observational data is an important, ubiquitous problem in science.
Optical flow refers to the deformation of the domain of an image that results from ego- or scene motion.
Modern robots are designed to perform complicated planning and control tasks, such as manipulating objects, navigating in outdoor environments, and driving in urban settings.
This paper brings together two topics: online convex optimization and sampling from logconcave distributions over convex bodies.
Recently, multi-label classification (MLC) has been drawing increasing attention from the machine learning community (e.
Human knowledge and expertise is often tied to particular contexts.
Gaussian process classifiers (GPCs) [12] provide a Bayesian approach to nonparametric classification with the key advantage of producing predictive class probabilities.
We consider online learning in finite Markov decision processes (MDPs) with a fixed, known dynamics.
Data clustering is a fundamental problem in many fields, such as machine learning, data mining and computer vision [1].
Acting optimally under uncertainty requires comparing the expected utility of each possible action, but in most situations of practical interest this expectation is impossible to calculate exactly: the hidden states that must be integrated over may be high-dimensional and the probability density may not take on any simple form.
Networks of various types, formed by a large number of neurons through synapses, are the substrate of brain functions.
Consider the problem of recognizing an image that contains a single hand-written digit that has been approximately normalized but may have been written in one of a number of different styles.
We encounter complex dynamic scenes in everyday life.
Like insects with unmovable compound eyes, most current computer vision systems use images of uniform resolution.
One of the fundamental problems in computational biology is to understand associations between genomic variations and phenotypic effects.
Object detection remains one of the core objectives of computer vision, either as an objective per se, for instance for automatic focusing on faces in digital cameras, or as means to get high-level understanding of natural scenes for robotics and image retrieval.
A general form of estimating a quantity w ∈ Rn from an empirical measurement set X by minimizing a regularized or penalized functional is ŵ = argmin{L(Iw (X )) + λJ (w)}, (1) w where Iw (X ) ∈ Rm expresses the relationship between w and data X , L(.
Note by authors after publication: The results in Figure 3 could not be reproduced in subsequent experiments and should be considered invalid.
How neuronal networks can store a memory trace for recent sequences of stimuli is a central question in theoretical neuroscience.
In the online learning framework, the learner is faced with a sequence of data appearing at discrete time intervals.
In many machine learning domains, several sub-tasks operate on the same raw data to provide correlated outputs.
In recent years, individuals and corporate entities have gathered large quantities of personal data.
Markov networks (also known as Markov random fields, etc.
The problem of ﬁnding a low-rank approximation of a target matrix through matrix factorization (MF) attracted considerable attention recently since it can be used for various purposes such as reduced rank regression [19], canonical correlation analysis [8], partial least-squares [27, 21], multi-class classiﬁcation [1], and multi-task learning [7, 29].
Boosting [17] refers to a general technique of combining rules of thumb, or weak classifiers, to form highly accurate combined classifiers.
We consider pairwise Markov Random Field (MRF) over n binary variables x = x1 , .
Recently, it has been shown that learning multiple tasks together helps learning [8, 19, 9] when the tasks are related, and one is able to use an appropriate notion of task relatedness.
Structural aspects of models are often critical to obtaining flexible, expressive model families.
Learning multiple tasks has been studied for more than a decade [6, 24, 11].
This paper studies a family of decision-making problems in which discrete events occur on a continuous time scale.
During the past decade the effects of exact spike timing on the change of synaptic connectivity have been studied extensively.
The study of brain functional connectivity, as revealed through distant correlations in the signals measured by functional Magnetic Resonance Imaging (fMRI), represents an easily accessible, albeit indirect marker of brain functional architecture; in the recent years, it has given rise to fundamental insights on brain organization by representing it as a modular graph with large functionallyspecialized networks [1, 2, 3].
Policy gradient methods [18] in Reinforcement Learning have gained popularity, due to the guaranteed improvement in control performance over iterations (which is often lacking in approximate policy or value iteration) as well as the discovery of more efficient gradient estimation methods.
The challenge of inferring whether X causes Y (“X → Y ”) or vice versa (“Y → X”) from joint observations of the pair (X, Y ) has recently attracted increasing interest [1, 2, 3, 4, 5, 6, 7, 8].
Computing systems today are ubiquitous, and range from the very small (e.
This paper presents a new computational framework, called random forest random field (RF)2 , which provides a principled way to jointly reason about multiple, statistically dependent random variables and their attributes.
The last two decades have been marked by significant advances in modeling multivariate probability density functions (PDFs) on graphs.
Estimating the 3D pose of an articulated body or of a deformable surface from monocular images is one of the fundamental problems in computer vision.
Camera shake is a common problem of handheld, longer exposed photographs occurring especially in low light situations, e.
As the cornerstone of Bayesian nonparametric modeling, Dirichlet processes (DP) [22] have been applied to a wide variety of inference and estimation problems [3, 10, 20] with Dirichlet process mixtures (DPMs) [15, 17] being one of the most successful.
The immense volume of textual information available on the web provides an important opportunity and challenge for AI: Can we develop methods that can learn domain knowledge by reading natural texts such as news articles and web pages.
1 Video Annotation and Tracking with Active Learning Carl Vondrick UC Irvine Deva Ramanan UC Irvine vondrick@mit.
Paraphrase detection determines whether two phrases of arbitrary length and form capture the same meaning.
Most reinforcement learning [1] algorithms are value-function based—learning is performed by estimating the expected return (discounted sum of rewards) obtained by following the current policy from each state, and then updating the policy based on the resulting so-called value function.
We are primarily interested in learning in domains where there is only a small amount of labeled data but advice can be provided by a domain expert.
When diagnosed with cancer, most patients ask about their prognosis: “how long will I live”, and “what is the success rate of each treatment option”.
Mirror Descent is a first-order optimization procedure which generalizes the classic Gradient Descent procedure to non-Euclidean geometries by relying on a “distance generating function” specific to the geometry (the squared �2 norm in the case of standard Gradient Descent) [14, 4].
This paper introduces a new type of regularity in the reinforcement learning (RL) and planning problems with finite-action spaces that suggests that the convergence rate of the performance loss to zero is faster than what previous analyses had indicated.
Submodularity has been and continues to be an important property in many fields.
Network datasets comprising edge measurements Aij ∈ {0, 1} of a binary, symmetric, and antireflexive relation on a set of n nodes, 1 ≤ i < j ≤ n, are fast becoming of paramount interest in the statistical analysis and data mining literatures [1].
We consider the problem of finding a good approximation of the maximum of a function f : X → R using a finite budget of evaluations of the function.
Recent years have witnessed the emergence of several reinforcement-learning techniques that make it possible to learn a decision policy from a batch of sample transitions.
Nearest neighbor (NN) classifiers would be one of the classical and perhaps the simplest non-linear classification algorithms.
Increasingly in modern settings, in domains across science and engineering, one is faced with the challenge of working with high-dimensional models where the number of parameters is large, particularly when compared to the number of observations.
Boosting is a simple and efficient machine learning algorithm which provides state-of-the-art performance on many tasks.
Biological systems are constantly interacting with a dynamic, noisy environment, which they can only assess through noisy sensors.
Feature learning is an important problem in statistics and machine learning, characterized by the goal of (typically) inferring a low-dimensional set of features for representation of high-dimensional data.
Consider the linear regression model y = Xβ ∗ + ε, (1) where y is a vector of observations, X ∈ Rn×p a design matrix, ε a vector of noise and β ∗ a vector of coefficients to be estimated.
We nowadays routinely face high-dimensional datasets in diverse application areas such as biology, astronomy, and finance.
With the advent of the Internet, many machine learning applications are faced with very large and inherently high-dimensional datasets, resulting in challenges in scaling up training algorithms and storing the data.
The design of efficient algorithms for large scale similarity search (such as nearest neighbor search) has been a central problem in computer science.
Selective prediction is the study of predictive models that can automatically qualify their own predictions and output “don’t know” when they are not sufficiently confident.
There are many suggestions that humans and other animals employ multiple approaches to learned decision making [1].
Kernel methods [24, 21] allow to obtain nonlinear learning machines from simpler, linear ones; nowadays they can almost completely be applied out-of-the-box [3].
Methods for solving sequential decision problems with delayed reward, where the problems are formulated as Markov decision processes (MDPs), have been compared to the learning mechanisms of animal brains [3, 4, 9, 10, 13, 20, 22].
The objective of inverse reinforcement learning (IRL) is to determine the decision making agent’s underlying reward function from its behaviour data and the model of environment [1].
Reinforcement learning (RL) is often used to solve single tasks for which it is tractable to learn a good policy with minimal initial knowledge.
Traditional image categorization methods usually consider an image as one indiscrete entity, which, however, neglects an important fact that the semantic meanings (labels) of an image mostly arise from its constituent regions, but not the entire image.
The problem of sampling K representative data points from a large dataset arises frequently in various applications.
Kernel methods have been popular in machine learning and pattern analysis for their superior performance on a wide spectrum of learning tasks.
Learning multiple related tasks is increasingly important in modern applications, ranging from the prediction of tests scores in social sciences and the classification of protein functions in systems biology to the categorisation of scenes in computer vision and more recently to web search and ranking.
Vector Auto-regressive models (VAR) are an important class of models for analyzing multivariate time series data.
The concept of parsimony is central in many scientific domains.
Over the last decade the use of Gaussian Processes (GPs) as non-parametric regression models has grown significantly.
“Learning to learn”, or the ability to learn abstract representations that support transfer to novel but related tasks, lies at the core of many problems in computer vision, natural language processing, cognitive science, and machine learning.
Modeling temporal dependence is an important consideration in many learning problems.
(a) (b) (c) (d) (e) (f) Figure 1: Which of these images are the most memorable? See footnote 1 for the answer key.
Various algorithms have been proposed to solve exploration / exploitation or bandit problems.
Classification is a fundamental task of machine learning, and is by now well understood in its basic variants.
Approximation algorithms and heuristic approximations are commonly used to speed up the running time of algorithms in machine learning and data analysis.
The concept of parsimony is central in many scientific domains.
This paper addresses the problem of ranking a set of objects based on a limited number of pairwise comparisons (rankings between pairs of the objects).
In the learning literature, there are several previous results on learning probabilistic tree models, including various Expectation Maximization-based inference algorithms.
Clustering, a fundamental and ubiquitous problem in machine learning, is the task of organizing data points into homogenous groups using a given measure of similarity.
In this paper we consider the problem of selecting correlated variables in a high dimensional space.
Nonparametric Bayesian latent variable models have recently gained remarkable popularity in statistics and machine learning, partly owning to their desirable “nonparametric” nature which allows practitioners to “sidestep” the difficult model selection problem, e.
Bayesian accounts of cortical processing posit that the brain implements a probabilistic model to learn and reason about the causes underlying sensory inputs.
With the ever-growing amount of digital image data in multimedia databases, there is a great need for algorithms that can provide effective semantic indexing.
Learning to classify an object into a relevant target class surfaces in many domains such as document categorization, object recognition in computer vision, and web advertisement.
The explosion of digital sensing technology has unleashed a veritable data deluge that has pushed current signal processing algorithms to their limits.
Multi-structure model fitting is concerned with estimating the multiple instances (or structures) of a geometric model embedded in the input data.
Much recent research has focused on training deep, multi-layered networks of feature extractors applied to challenging visual tasks like object recognition.
Real world problems often produce high dimensional features with sophisticated statistical dependency structures.
Consider a clinical problem with M subpopulations, in which one should decide between Km options for treating subjects from each subpopulation m.
Continuous-time stochastic models play a prominent role in many scientific fields, from biology to physics to economics.
Consider the problem of estimating signal using noisy observation under the model: f (t) = cg(a t − φ) + e(t) , where the random quantities c ∈ R is the scale, a ∈ R is the rate, φ ∈ R is the phase shift, and e(t) ∈ R is the additive noise.
This study considers whether recurrent networks of spiking neurons can be seen as a generative model not only of stationary patterns but also of temporal sequences.
Visual recognition is a fundamental computer vision problem that demands sophisticated image representations—due to both the large number of object categories a system should ultimately recognize, as well as the noisy cluttered conditions in which training examples are often captured.
Fitted value iteration (FVI), both in the model-based [4] and model-free [5, 15, 16, 17] settings, has become a method of choice for various applied batch reinforcement learning problems.
Tensors (multi-way arrays) generalize matrices and naturally represent data having more than two modalities.
Many machine learning applications involve planning under uncertainty.
The problem of finding the best balanced cut of a graph is an important problem in computer science [9, 24, 13].
Unsupervised feature learning has recently emerged as a viable alternative to manually designing feature representations.
We consider the problem of sensing an unknown function f : X → R (where X ⊂ Rd ), where f belongs to span of a large set of (known) features {ϕk }1≤k≤K of L2 (X ): f (x) = K � αk ϕk (x), k=1 def where α ∈ RK is the unknown parameter, and is assumed to be S-sparse, i.
Sparse inference has found numerous applications in statistics and machine learning [1, 2, 3].
We consider the domain adaptation scenarios where we have very few or no labeled data from target domain but a large amount of labeled data from multiple related source domains with different data distributions.
The exponential family of distributions is ubiquitous in statistical machine learning.
We begin with the generative model Y = ΦX0 + E, (1) m×t where Φ ∈ R is a dictionary of basis vectors or features, X0 ∈ R is a matrix of unknown coefficients we would like to estimate, Y ∈ Rn×t is an observed signal matrix, and E is a noise matrix with iid elements distributed as N (0, λ).
In recent years the importance of taking advantage of the structure of convex optimization problems has become a topic of intense research in the machine learning community.
The application of non-parametric Bayesian techniques to time series has been an active field in the recent years, and has led to many successful continuous time models.
Recent advances in manifold learning and nonlinear dimensionality reduction have led to powerful, new methods for the analysis and visualization of high dimensional data [14, 1, 20, 24, 16].
This paper considers the problem of stochastic convex optimization under bandit feedback which is a generalization of the classical multi-armed bandit problem, formulated by Robbins in 1952.
Calculating marginal probabilities for a graphical model generally requires summing over exponentially many states, and is NP-hard in general [1].
Deformable Part Models (DPMs) deliver state-of-the-art object detection results [4] on challenging benchmarks when trained discriminatively, and have become a standard in object recognition research.
Brain computer interfaces (BCIs) allow people to control devices directly using brain signals [19].
In standard formulations of prediction problems, it is assumed that the covariates are fully-observed and sampled independently from some underlying distribution.
Nonlinear probabilistic modeling of high dimensional time series data is a key challenge for the machine learning community.
Image segmentation, a partitioning of an image into disjoint regions such that each region is homogeneous, is an important preprocessing step for many of the state-of-the-art algorithms for high-level image/scene understanding for three reasons.
We study the problem of learning to rank from pairwise preferences, and solve an open problem that has led to development of many heuristics but no provable results.
Problems with high dimensionality have become common over the recent years.
Q-learning [20] is a well-known model-free reinforcement learning (RL) algorithm that finds an estimate of the optimal action-value function.
The sparse Principal Component Analysis (Sparse PCA) problem is a variant of the classical PCA problem, which accomplishes a trade-off between the explained variance along a normalized vector, and the number of non-zero components of that vector.
Multiple Instance Learning (MIL) is a variation of the classical learning methods for problems with incomplete knowledge on the instances (or examples) [4].
In each trial of a standard visual short-term memory (VSTM) experiment (e.
Recently, compressive sensing [5] and sparse representation [19] have been hot research topics and also have found abundant applications in signal processing and machine learning.
Multiarmed bandits with side information are an elegant mathematical model for many real-life interactive systems, such as personalized online advertising, personalized medical treatment, and so on.
“Reinforcement learning” and “negative-feedback models of homeostatic regulation” are two control theory-based computational frameworks that have had major contributions in learning and motivation literatures in behavioral psychology, respectively.
Graphical models are a central tool in modern machine learning applications, as they provide a natural methodology for succinctly representing high-dimensional distributions.
The proliferation of social networks on the web has spurred many significant advances in modeling networks [1, 2, 4, 12, 13, 15, 16, 26].
The objective of transfer in reinforcement learning (RL) [10] is to speed-up RL algorithms by reusing knowledge (e.
The belief propagation algorithm [1] was originally proposed as an efficient method for the exact computation in the inference with graphical models associated to trees; the algorithm has been extended to general graphs with cycles and called Loopy Belief Propagation (LBP) algorithm.
Comparing probability distributions is a fundamental task in statistical data processing.
As the scope of machine learning applications has increased, the complexity of the classification tasks that are commonly tackled has grown dramatically.
Over the last several decades, information theory [1, 2] has played a major role in our effort to understand the neural code in the brain [3, 4].
In this paper we consider the following rank minimization problems: min{f (X) : rank(X) ≤ r, X ∈ X ∩ Ω}, (1) min{f (X) + ν rank(X) : X ∈ X ∩ Ω} (2) X X for some r, ν ≥ 0, where X is a closed convex set, Ω is a closed unitarily invariant set in <m×n , and f : <m×n → < is a continuously differentiable function (for the definition of unitarily invariant set, see Section 2.
We consider optimization problems where the objective function is costly to evaluate and may be accessed only by evaluating it at requested points.
A particularly effective image representation has developed in recent years, formed by computing the statistics of oriented gradients quantized into various spatial and orientation selective bins.
Image categorization is the task of classifying an image as containing an objects from a predefined list of categories.
Local Coordinate Coding (LCC) [18] is a coding scheme that encodes the data locally so that any non-linear (α, β, p)-Lipschitz smooth function (see Definition 1 in Section 2 for details) over the data manifold can be approximated using linear functions.
Many real-world problems involve multiple related classificatrion/regression tasks.
Is it possible to achieve the same test performance as the best classifier in hindsight? The answer to this question is “probably not.
Multi-class image segmentation and labeling is one of the most challenging and actively studied problems in computer vision.
Online learning algorithms, which have received much attention in recent years, enjoy an attractive combination of computational efficiency, lack of distributional assumptions, and strong theoretical guarantees.
Animals and humans often use vision to find things: mushrooms in the woods, keys on a desk, a predator hiding in tall grass.
The minimization of an objective function which is only available through unbiased estimates of the function values or its gradients is a key methodological problem in many disciplines.
We design improved algorithms for Euclidean k-means in the streaming model.
Computing the appearance distance between two windows is a fundamental operation in a wide variety of computer vision techniques.
Probabilistic programming simplifies the development of probabilistic models by allowing modelers to specify a stochastic process using syntax that resembles modern programming languages.
The goal of matrix factorization (MF) is to approximate an observed matrix by a low-rank one.
The Chinese restaurant process (CRP) is a distribution on partitions of integers [2].
We focus on stochastic convex optimization problems of the form Z F (x; ξ)dP (ξ), minimize f (x) for f (x) := EP [F (x; ξ)] = x∈X (1) Ξ where X ⊆ Rd is a closed convex set, P is a probability distribution over Ξ, and F (· ; ξ) is convex for all ξ ∈ Ξ, so that f is convex.
Determining the major products of chemical reactions given the input reactants and conditions is a fundamental problem in organic chemistry.
We consider the problem of selecting the right state-representation in an average-reward reinforcement learning problem.
We are now able to record from hundreds—and very likely soon from thousands—of neurons in vivo.
Sparse modeling (Olshausen and Field, 1996; Aharon et al.
Outsourcing information processing to large groups of anonymous workers has been made easier by the internet.
Given a training set of normal events, the anomaly detection problem aims to identify unknown, anomalous events that deviate from the normal set.
Scene understanding is an important task in neural information processing systems.
We consider the problem of training statistical mixture models, in particular mixtures of Gaussians and some natural generalizations, on massive data sets.
The operation of neural circuits fundamentally depends on the capacity of neurons to perform complex, nonlinear mappings from their inputs to their outputs.
There is increasing evidence that visual cortex contains discrete patches involved in processing faces but not other objects [2, 3, 4, 5, 6, 7].
Samples of multivariate data are often connected with labels or parameters.
The recent progress in sensor technology has made possible a plethora of novel applications, which typically require increasingly large amount of multidimensional data, such as large-scale images, 3D video sequences, and neuroimaging data.
The idea that images can be hierarchically parsed into objects and their parts has a long history in computer vision, see for example [15].
According to the recently quite influential statistical approach to perception, our brain represents not only the most likely interpretation of a stimulus, but also its corresponding uncertainty.
On the basis of i.
Many machine learning algorithms implement empirical risk minimization or a regularized variant of it.
Much of machine learning research can be viewed as an exploration of ways to compensate for scarce prior knowledge about how to solve a specific task by extracting (usually implicit) knowledge from vast amounts of data.
Learning or inferring a hidden structure from discrete samples is a fundamental problem in data analysis, ubiquitous in a broad range of application fields.
Understanding the factors that influence the performance of a statistical procedure is a key step for finding a way to improve it.
The problem of causal induction is central to science, and is something at which people are remarkably skilled, especially given its apparent difficulty.
Kernel methods have long provided powerful tools for generalizing linear statistical approaches to nonlinear settings, through an embedding of the sample to a high dimensional feature space, namely a reproducing kernel Hilbert space (RKHS) [16].
Learning to produce temporal sequences is a general problem that the brain needs to solve.
Fitting parametric probabilistic models to data is a basic task in statistics and machine learning.
Variational Message Passing [20] is a message passing implementation of the mean-field approximation [1, 2], also known as variational Bayes (VB).
Kernel methods are widely used to address a variety of learning problems including classification, regression, structured prediction, data fusion, clustering and dimensionality reduction [22, 23].
Multi-electrode array recording and similar methods provide measurements of activity from dozens of neurons simultaneously, and thus allow unprecedented insights into the statistical structure of neural population activity.
Inexpensive RGB-D sensors that augment an RGB image with depth data have recently become widely available.
We consider the reinforcement learning problem in which one attempts to find a good policy for controlling a stochastic nonlinear dynamical system.
Stochastic approximation (online) approaches, such as stochastic gradient descent and stochastic dual averaging, have become the optimization method of choice for many learning problems, including linear SVMs.
Topic modeling holds much promise for improving the ways users search, discover, and organize online content by automatically extracting semantic themes from collections of text documents.
Dimensionality reduction methods are important for data analysis and processing, with their use motivated mainly from two considerations: (1) the impracticality of working with high-dimensional spaces along with the deterioration of performance due to the curse of dimensionality; and (2) the realization that many classes of signals reside on manifolds of much lower dimension than that of their ambient space.
The analysis of action potentials (“spikes”) from neural-recording devices is a problem of longstanding interest (see [21, 1, 16, 22, 8, 4, 6] and the references therein).
In this work we consider the problem of efficient object-class recognition in large image collections.
One of the central problems in statistics is the linear regression in which the goal is to accurately estimate a regression vector β ? ∈ Rp from the noisy observations y = Xβ ? + w, n×p (1) n where X ∈ R is the measurement or design matrix, and w ∈ R is the stochastic observation vector noise.
Consider the situation where we want to search and navigate a database, but the underlying relationships between the objects are unknown and are accessible only through a comparison oracle.
In the eighteen years since variational inference was first proposed for neural networks [10] it has not seen widespread use.
Owing to the great advancements in measurement technology, a huge amount of data is generated in the ﬁeld of science, engineering, and medicine, and accordingly, there is an increasing demand for estimating the hidden states underlying the observed signals.
User feedback has become an invaluable source of training data for optimizing information retrieval systems in a rapidly expanding range of domains, most notably content recommendation (e.
The Euclidean distance between two vectors x and y in d-dimensional space is a typical distance measure that reflects their proximity in the space.
Boosting is a popular approach to classifier design in machine learning.
Undirected graphical models, also known as Markov random fields, are used in a variety of domains, including statistical physics, natural language processing and image analysis among others.
In this paper we consider large-margin halfspace learning in the PAC model: there is a target halfspace f (x) = sign(w · x), where w is an unknown unit vector, and an unknown probability distribution D over the unit ball Bn = {x ∈ Rn : kxk2 ≤ 1} which has support on {x ∈ Bn : |w·x| ≥ γ}.
The notion of “representativeness” appeared in cognitive psychology as a proposal for a heuristic that people might use in the place of performing a probabilistic computation [1, 2].
Visual 3D scene understanding is an important component in applications such as autonomous driving and robot navigation.
There is an increasing tendency to consider machine learning as a problem in optimization: define a loss function, add constraints and/or regularizers and formulate it as a preferably convex program.
Machine learning has become a central tool in areas such as speech recognition, natural language translation, machine question answering, and visual object detection.
Semantic segmentation (i.
Coding efficiency is a well-known objective for the evaluation and design of signal processing systems, and provides a theoretical framework for understanding biological sensory systems.
Classification problems with many classes arise in many important domains and pose significant computational challenges.
The problem of learning-by-example has the promise to create strong models from a restricted number of cases; certainly humans show the ability to generalize from limited experience.
One of the fundamental questions in computational neuroscience is how synapses are modified by neural activity [1, 2].
Most existing analyses of active learning are based on an i.
One of the main goals of scene understanding is the semantic segmentation of images: label a diverse set of object properties, at multiple scales, while at the same time identifying the spatial extent over which such properties hold.
The goal in matrix factorization is to recover a low-rank matrix from irrelevant noise and corruption.
In this paper we address the task of nonnegative dictionary learning described by V ≈ W H, (1) where V , W , H are nonnegative matrices of dimensions F × N , F × K and K × N , respectively.
Matrix-variate normal (MVN) models have important applications in various fields.
In the papers [1, 10, 11], an array of tools has been developed to study the minimax value of diverse sequential problems under the worst-case assumption on Nature.
Determining interactions between entities based on observations is a major challenge when analyzing biological and social network data [1, 12, 15].
We consider the extension of the classical online problem of predicting outcomes from a finite alphabet to the matrix domain.
In many machine learning problems, one is often confronted with very high dimensional data.
As time progresses, the choices humans make often change.
Value functions are an essential concept for determining optimal policies in both optimal control [1] and reinforcement learning [2, 3].
The oft used linear regression paradigm models a dependent variable Y as a linear function of a vector-valued independent variable X.
Log-linear models, also known as maximum entropy models or multiclass logistic regression, have found a wide range of applications in machine learning.
Human actions are ubiquitous and represent essential information for understanding the content of many still images such as consumer photographs, news images, sparsely sampled surveillance videos, and street-side imagery.
Increasingly, optimization problems in machine learning are very high-dimensional, where the number of variables is very large.
Consider building a sofa detector using a database of annotated images containing sofas and many other classes, as shown in Figure 1.
Most statistical regression models in use today are of the form: g(y) = f (x1 )+f (x2 )+· · ·+f (xD ).
Many sequential decision-making problems are commonly modelled as an extensive form game.
While being rooted in information retrieval [1], the so-called F-measure is nowadays routinely used as a performance metric for different types of prediction problems, including binary classification, multi-label classification (MLC), and certain applications of structured output prediction, like text chunking and named entity recognition.
Crowdsourcing is becoming an increasingly important methodology for collecting labeled data, as demonstrated among others by Amazon Mechanical Turk, reCAPTCHA, Netflix, and the ESP game.
Penalized likelihood estimation has evolved into a major area of research, with `1 [22] and other regularization penalties now used routinely in a rich variety of domains.
Machine learning algorithms have found applications in diverse domains such as computer vision, bio-informatics and speech recognition.
The graph Laplacian is a popular tool for unsupervised and semi-supervised learning problems on graphs.
A central problem in systems neuroscience is to understand the probabilistic relationship between sensory stimuli and neural responses.
We derive new rates of convergence in terms of dimension for the popular approach of Nearest Neighbor (k-NN) regression.
This paper considers the model combination problem, where the goal is to combine multiple models in order to achieve improved accuracy.
Renewal processes are stochastic point processes on the real line where intervals between successive points (times) are drawn i.
In undirected graphical models (UGMs), a graph is defined as G = (V, E), where V = {1, · · · , K} is the set of nodes and E ⊂ V × V is the set of edges between the nodes.
Decision-theoretic online learning (DTOL) is a framework to capture learning problems that proceed in rounds.
Manifold Embedding In many areas of machine learning, pattern recognition, information retrieval and computer vision, we are confronted with high-dimensional data that lie in or close to a manifold of intrinsically lowdimension.
With machine learning comes the question of how to effectively teach.
Statistical models of natural signals have provided a rich framework to describe how sensory neurons process and adapt to ecologically-valid stimuli [28, 12].
Episodic memory such as that in the hippocampus acts like a palimpsest – each new entity to be stored is overlaid on top of its predecessors, and, in turn, is submerged by its successors.
In temporal prediction tasks, Temporal Difference (TD) learning provides a method for learning long-term expected rewards (the “value function”) using only trajectories from the system.
Partially observable Markov decision process (POMDP) provides a principled general framework for planning with imperfect state information.
We address the problem of recovering a low-rank m-by-n matrix X of which a few entries are observed, possibly with noise.
One of the most common approaches to collaborative filtering and matrix completion is trace-norm regularization [1, 2, 3, 4, 5].
Biclustering is the problem of identifying a (typically) sparse set of relevant columns and rows in a large, noisy data matrix.
In a range of applications, a dynamic decision making problem exhibits two distinctly different kinds of phases: experimentation and commitment.
Research in multi-label classification has seen a substantial growth in recent years (e.
In Machine Learning, model quality is most often limited by the lack of sufficient training data.
In cluster analysis we are concerned with identifying subsets of n objects that share some similarity and therefore potentially belong to the same sub-population.
It has long been argued that many key questions in neuroscience can best be posed in informationtheoretic terms; the efficient coding hypothesis discussed in [2, 3, 1], represents perhaps the bestknown example.
The design of effective approximate inference methods for continuous variables often requires considering the curvature of the target distribution.
Consider approximating a p-dimensional data point x by a linear combination x ≈ Bw of m (possibly linearly dependent) codewords in a dictionary B = [b1 , b2 , .
Planning research typically assumes that the planning system is provided complete and correct models of the actions.
The on-line domain is a rich environment for observing social contagion — the tendency of new information, ideas, and behaviors to spread from person to person through a social network [1, 4, 6, 10, 12, 14, 17, 19].
Combinatorial optimization techniques have been actively applied to many machine learning applications, where submodularity often plays an important role to develop algorithms [10, 16, 27, 14, 15, 19, 1].
Low-rank matrix recovery is the following problem: let M be some unknown matrix of dimension d and rank r  d, and let A1 , A2 , .
Anomaly detection is a crucial problem in processing large-scale data sets when our goal is to find rare or unusual events.
Producing a relevant and accurate caption for an arbitrary image is an extremely challenging problem, perhaps nearly as difficult as the underlying general image understanding task.
The motivation of this paper is to understand the intrinsic structure and properties of suitable loss functions for the problem of multiclass prediction, which includes multiclass probability estimation.
The problem of finding maximal cliques in a weighted graph is faced in many applications from computer vision to social networks.
Inverse reinforcement learning (IRL) methods learn a reward function in a Markov decision process (MDP) from expert demonstrations, allowing the expert’s policy to be generalized to unobserved situations [7].
The contemporary problem of exploring huge collections of discrete data, from biological sequences to text documents, has prompted the development of increasingly sophisticated statistical models.
Consider the problem of learning to optimize a complex system subject to varying environmental conditions.
In cluster analysis, the objective is to segment a dataset into subgroups, such that data points in the same subgroup are more similar to each other (in a sense that will be specified) than to those in other subgroups.
Visual recognition is a major focus of research in computer vision, machine learning, and robotics.
The task of recovering intrinsic images is to separate a given input image into its material-dependent properties, known as reflectance or albedo, and its light-dependent properties, such as shading, shadows, specular highlights, and inter-reflectance.
In many regression/classification problems, the features exhibit certain hierarchical or structural relationships, the usage of which can yield an interpretable model with improved regression/classification performance [25].
Image categorization / object recognition has been one of the most important research problems in the computer vision community.
Most scene understanding tasks (e.
According to [18], more than 33 million people worldwide are infected with the human immunodeficiency virus (HIV), for which there exists no cure.
The current trend towards ‘big data’ has created a strong demand for techniques to efficiently extract structure from ever-accumulating unstructured datasets.
Privacy-preserving data analysis has received increasing attention in recent years.
The analysis of the structure and evolution of network data is an increasingly important task in a variety of disciplines, including biology and engineering.
Causal discovery refers to a special class of statistical and machine learning methods that infer causal relationships between variables from data and prior knowledge [1, 2, 3].
There exist many sources of unstructured data that have been partially or completely categorized by human editors.
Probabilistic graphical models provide a compact and principled representation for capturing complex statistical dependencies among a set of random variables.
Intelligent decision making in real-world scenarios requires an agent to take into account its limitations in sensing and actuation.
The identification of individual spikes in extracellularly recorded voltage traces is a critical step in the analysis of neural data for much of systems neuroscience.
Background.
Domain adaptation addresses the problem of generalizing from a source distribution for which we have ample labeled training data to a target distribution for which we have little or no labels [3, 14, 28].
One of the central problems in systems neuroscience is to understand how neural spike responses convey information about environmental stimuli, which is often called the neural coding problem.
While difficulty adjustment is common practise in many traditional games (consider, for instance, the handicap in golf or the handicap stones in go), the case for dynamic difficulty adjustment in electronic games has been made only recently [7].
Probabilistic models of text and topics, known as topic models, are powerful tools for exploring large data sets and for making inferences about the content of documents.
As problem domains become more complex, human guidance becomes increasingly necessary to improve agent performance.
Predictive analysis of networked data is a fast-growing research area whose application domains include document networks, online social networks, and biological networks.
The problem of modeling temporal dependencies in temporal streams of discrete events arises in a wide variety of applications.
Recently, sparse coding [3, 18] has attracted much attention in computer vision research.
We are interested in probablistic models for sequences arising from the study of genetic variations in a population of organisms (particularly humans).
Over the last twenty years, researchers have used a number of unsupervised learning algorithms to model a range of neural phenomena in early sensory processing.
Amplitude and frequency demodulation (AFD) is the process by which a signal (yt ) is decomposed into the product of a slowly varying envelope or amplitude component (at ) and a quickly varying sinusoidal carrier (cos(φt )), that is yt = at cos(φt ).
A fruitful modelling approach for extracting meaningful information from highly structured multivariate datasets is based on matrix factorisations (MFs).
Not all Vapnik-Chervonenkis classes are created equal.
Individuals are often asked to convey their opinions and sentiments in the form of quantitative judgments.
We study the optimization of an unknown function f by requesting n experiments, each specifying an input x and producing a noisy observation of f (x).
A Restricted Boltzmann Machine (RBM) [24, 10] is a learning system consisting of two layers of binary stochastic units, a hidden layer and a visible layer, with a complete bipartite interaction graph.
Semidefinite programming (SDP) has become a tool of great importance in optimization in the past years.
Sparsity has been shown to work well for learning feature representations that are robust for object recognition [1, 2, 3, 4, 5, 6, 7].
Algorithms for filtering and prediction have a venerable history studded by quantum leaps by Wiener, Kolmogorov, Mortensen, Zakai, Duncan among others.
The formalism of probabilistic graphical models can be employed to represent dependencies among a large set of random variables in the form of a graph [1].
Linear stochastic bandit problem is a sequential decision-making problem where in each time step we have to choose an action, and as a response we receive a stochastic reward, expected value of which is an unknown linear function of the action.
Gaussian Markov Random Fields; Covariance Estimation.
Is it possible to leverage the solution of one classification problem to solve another? This is a question that has received increasing attention in recent years from the machine learning community, and has been studied in a variety of settings, including multi-task learning, covariate shift, and transfer learning.
We consider a stochastic convex optimization problem of the form minw∈W L(w), where L(w) = Ez [�(w, z)], based on an empirical sample of instances z1 , .
When are the objects in two images the same?1 Although people recognize and categorize objects successfully and effortlessly, object recognition in machine learning is an incredibly difficult problem and people’s success is a puzzle to cognitive scientists.
Monte-Carlo Tree Search (MCTS) has become a popular approach for decision making in large domains.
One distinguishing property of life is its temporal dynamics, and it is hence only natural that time lapse experiments play a crucial role in current research on signaling pathways, drug discovery and developmental biology [17].
The goal of reinforcement learning (RL) is to find an optimal decision-making policy that maximizes the return (i.
In a multi-armed bandit (MAB) problem, a player is presented with a sequence of trials.
Hidden Markov Models (HMM) provide one of the simplest examples of structured data observed through a noisy channel.
The last several years has revealed a new trend in Machine Learning: prediction and learning problems rolled into prize-driven competitions.
Belief propagation (BP) has become the standard procedure to decode channel codes, since in 1996 MacKay [7] proposed BP to decode codes based on low-density parity-check (LDPC) matrices with linear complexity.
Multiple Kernel Learning (MKL) proposed by [20] is one of the most promising methods that adaptively select the kernel function in supervised kernel learning.
A clustering is defined as a partitioning of a set of elements into subsets called clusters.
The recent development of conditional random fields (CRFs) [1], max-margin Markov networks (M3Ns) [2], and structured support vector machines (SSVMs) [3] has triggered a wave of interest in the prediction of complex outputs.
In fields such as ecology, marketing, and the social sciences, data about identifiable individuals is rarely available, either because of privacy issues or because of the difficulty of tracking individuals over time.
Consider a polling institute that has to estimate as accurately as possible the average income of a country, given a ﬁnite budget for polls.
It is well known that speech conveys various yet mixed information where there are linguistic information, a major component, and non-verbal information such as speaker-specific and emotional components [1].
Models such as Deep Belief Networks (DBNs) [2], stacked denoising autoencoders [3], convolutional networks [4], as well as classifiers based on sophisticated feature extraction techniques have from ten to perhaps fifty hyper-parameters, depending on how the experimenter chooses to parametrize the model, and how many hyper-parameters the experimenter chooses to fix at a reasonable default.
Suppose an observation y ∈ Rm is available where y = Ax + u + Dξ.
Fundamental to describing the behavior of neurons in response to sensory stimuli or to inputs from other neurons is the need for succinct models that can be estimated and validated with limited data.
Multi-class Gaussian process classifiers (MGPCs) are a Bayesian approach to non-parametric multiclass classification with the advantage of producing probabilistic outputs that measure uncertainty in the predictions [1].
Tracking human 3D articulated motions from video sequences is well known to be a challenging machine vision problem.
Metric learning (ML), which aims at learning dissimilarities by determining the importance of different input features and their correlations, has become a very active research field over the last years [23, 5, 3, 14, 22, 7, 12].
Graphical models are useful for representing relationships between large numbers of random variables in probabilistic models spanning a wide range of applications, including information extraction and data integration.
In many tasks, bounded resources and physical constraints force decisions to be made based on limited information [1, 2].
Many real-world datasets have representations in the form of multiple views [1, 2].
In many areas of application, problems are naturally expressed as a Gibbs measure, where the distribution over the domain X is given by, for x ∈ X : q(x) = X q̃(x) exp{−βE(x)} = , with Z(β) = q̃(x).
We propose a novel and general method to approximate the partition function of intricate probability distributions defined over combinatorial spaces.
One of the most basic learning settings studied in the online learning framework is learning from experts.
Least-squares analysis was introduced by Gauss in 1795 and has since has bloomed into a staple of the data analyst.
Boosting is the task of converting inaccurate weak learners into a single accurate predictor.
Two-sample hypothesis tests are concerned with the question of whether two samples of data are generated from the same distribution.
Consider a gambler who is trying to predict the next bit in a sequence of bits.
Learning an unknown halfspace from labeled examples that satisfy a margin constraint (meaning that no example may lie too close to the separating hyperplane) is one of the oldest and most intensively studied problems in machine learning, with research going back at least five decades to early seminal work on the Perceptron algorithm [5, 26, 27].
With its small memory footprint, robustness against noise, and rapid learning rates, Stochastic Gradient Descent (SGD) has proved to be well suited to data-intensive machine learning tasks [3, 5, 24].
Due to its fast query speed and low storage cost, hashing [1, 5] has been successfully used for approximate nearest neighbor (ANN) search [28].
The work of Benjamin Libet [1, 2] and others [3, 4] has challenged our intuitive notions of the relation between decision making and conscious voluntary action.
The accurate prediction of molecular energetics in chemical compound space (CCS) is a crucial ingredient for compound design efforts in chemical and pharmaceutical industries.
We begin with the likelihood model y = Φx + ǫ, (1) where Φ ∈ Rn×m is a dictionary of unit ℓ2 -norm basis vectors, x ∈ Rm is a vector of unknown coefficients we would like to estimate, y ∈ Rn is the observed signal, and ǫ is noise distributed as N (ǫ; 0, λI) (later we consider more general likelihood models).
In real-world applications of visual object recognition, performance is time-sensitive.
Dimension reduction is involved in most of modern data analysis, in which high dimensional data must be handled.
The uncertainty of plan execution can be modeled by using probabilistic effects in actions, and therefore the probability of reaching different states from a given state and action.
In this paper, we consider the problem of debugging pipelines consisting of a set of data processing operators.
The multi-label classification problem is an extension of the traditional multiclass classification problem.
Nearest neighbor search, a.
Estimating semantic 3D information from monocular images is an important task in applications such as autonomous driving and personal robotics.
The probabilistic generative model is an important tool for statistical learning because it enables rich data to be explained in terms of simpler latent structure.
The Gaussian process (GP) is a popular nonparametric Bayesian method for nonlinear regression.
Datasets from a wide range of modern research areas are increasingly high dimensional, which presents a number of theoretical and practical challenges.
Let {x1 , x2 , .
Gaussian processes (GPs) [1] have been popular in the NIPS community for a number of years now, as one of the key non-parametric Bayesian inference approaches.
Many uses of graphical models either directly employ chains or tree structures—as in part-of-speech tagging—or employ them to enable inference in more complex models—as in junction trees and tree block coordinate descent [1].
Markov logic [1] is a language for statistical relational learning, which employs weighted first-order logic formulas to compactly represent a Markov random field (MRF) or a conditional random field (CRF).
This paper proposes a new algorithm for the following task: given a sparse undirected unweighted graph, partition the nodes into disjoint clusters so that the density of edges within clusters is higher than the edges across clusters.
In many applications, labeled data are expensive and time consuming to obtain while unlabeled data are abundant.
Networks have been powerful abstractions for modeling a variety of natural and artificial systems that consist of a large collection of interacting entities.
Regularizing with the `1 norm, when we expect a sparse solution to a regression problem, is often justified by kwk1 being the “convex envelope” of kwk0 (the number of non-zero coordinates of a vector w ∈ Rd ).
Cortical thickness measures the distance between the outer and inner cortical surfaces (see Fig.
In order to leverage the large amount of available unlabeled text, a lot of research has been devoted to developing good probabilistic models of documents.
Characterizing the statistical features of spike time sequences in the brain is important for understanding how the brain represents information about stimuli or actions in the sequences of spikes.
Traditional computer vision algorithms, particularly those that exploit various probabilistic and learning-based approaches, are often formulated in centralized settings.
Object recognition is one of the hardest problems in computer vision and important for making robots useful in home environments.
The performance of (semi-)supervised learning methods typically depends on the amount of labeled data.
Multi-task learning (MTL) exploits the relationships among multiple related tasks to improve the generalization performance.
There is a growing need for statistical models which can capture rich dependencies in structured data.
The accuracy of the items placed near the top is crucial for many information retrieval systems such as search engines or recommendation systems, since most users of these systems browse or consider only the first k items.
d Learning functions f : x → y based on training data (yi , xi )m i=1 : R × R is a fundamental problem with many scientific and engineering applications.
Statistical Relational Learning (SRL) [7] aims at modeling data consisting of relations between entities.
We consider optimization problems of the following form, p∗ = min x∈C, 1T x=1, x≥0 f (x) + λcard(x) where f is a convex function, C is a convex set, card(x) denotes the number of nonzero elements of x and λ ≥ 0 is a given tradeoff parameter for adjusting desired sparsity.
When estimating a quantity consisting of two elements, a two-stage approach of first estimating the two elements separately and then approximating the target quantity based on the estimates of the two elements often performs poorly, because the first stage is carried out without regard to the second stage and thus a small estimation error incurred in the first stage can cause a big error in the second stage.
Problem description.
Preference learning is concerned with making inference from data consisting of pairs of items and corresponding binary labels indicating user preferences.
There has been signiﬁcant interest and progress in recent years in understanding consistency of learning methods for various ﬁnite-output learning problems, such as binary classiﬁcation, multiclass 0-1 classiﬁcation, and various forms of ranking and multi-label prediction problems [1–15].
Brain Computer Interfaces interpret brain signals to allow direct man-machine communication [17].
Experimental findings from neuro- and cognitive sciences have led to the hypothesis that humans create and maintain an internal model of their environment in neuronal circuitry of the brain during learning and development [1, 2, 3, 4], and employ this model for Bayesian inference in everyday cognition [5, 6].
Reinforcement learning is focused on learning optimal policies from trajectories of data.
Inverse reinforcement learning (IRL) [14] consists in finding a reward function such that a demonstrated expert behavior is optimal.
Problems of learning with rank-based error metrics [16] and the adoption of learning for the purpose of rank aggregation in social choice [7, 8, 23, 25, 29, 30] are gaining in prominence in recent years.
Probabilistic graphical models are widely used in a variety of applications, from computer vision to natural language processing to computational biology.
Large-scale matrices emerging from stocks, genomes, web documents, web images and videos everyday bring new challenges in modern data analysis.
There has been increasing interest in count modeling using the Poisson process, geometric process [1, 2, 3, 4] and recently the negative binomial (NB) process [5, 6].
Network analysis methods such as MMSB [1], ERGMs [20], spectral clustering [17] and latent feature models [12] require the adjacency matrix A of the network as input, reflecting the natural assumption that networks are best represented as a set of edges taking on the values 0 (absent) or 1 (present).
Many machine learning algorithms use graphs as input, such as clustering [16, 14], manifold based dimensional reduction [2, 15], and graph-based semi-supervised learning [23, 22].
Receptor neurons in early sensory systems are more numerous than the projection neurons that transmit sensory information to higher brain areas, implying that sensory signals must be compressed to pass through a limited bandwidth channel known as “Barlow’s bottleneck” [1].
A key challenge in many time series applications is capturing long-range dependencies for which Markov-based models are insufficient.
Optimizing large-scale complex systems often requires the tuning of many parameters.
Directly specifying desired behaviors for automated agents is a difficult and time consuming process.
Computing the Shannon entropy in massive data have important applications in neural computation [17], graph estimation [5], query logs analysis in Web search [14], network anomaly detection [21], etc.
Undirected graphical models, also known as Markov random fields, are an important class of statistical models that have been extensively used in a wide variety of domains, including statistical physics, natural language processing, image analysis, and medicine.
SAT was originally shown to be a canonical NP-complete problem in Cook’s seminal work [5].
The expected return is often the objective function of choice in planning problems where outcomes not only depend on the actor’s decisions but also on random events.
Finding a subset of data points, called representatives or exemplars, which can efficiently describe the data collection, is an important problem in scientific data analysis with applications in machine learning, computer vision, information retrieval, etc.
The high volume and velocity of social media, such as blogs and Twitter, have propelled them to the forefront as sources of breaking news.
The analysis of legislative roll-call data provides an interesting setting for recent developments in the joint analysis of matrices and text [23, 8].
There is an increasing interest in using crowdsourcing to collect labels for machine learning [19, 6, 21, 17, 20, 10, 13, 12].
Clustering analysis as a discrete optimization problem is usually NP-hard.
State-space models (SSMs) are widely used to model time series and dynamical systems.
Graphical models are a common method to describe the dependencies of a joint probability distribution over a set of discrete random variables.
In this paper, we focus on scalable parallelization of Monte Carlo simulation, a problem motivated by the increasingly large inference problems occurring in a variety of fields in science and engineering.
Variable selection plays a fundamental role in statistical modeling for high-dimensional data sets, especially when the underlying model has a sparse representation.
The ability of neurons to adapt their responses to greatly varying sensory signal statistics is central to efficient neural coding [1, 2, 3, 4, 5, 6, 7].
One of the goals of multi-set data analysis is forming qualitative comparisons between datasets.
In a Bayesian setting, the Gaussian process (GP) is commonly used to define a prior probability distribution over functions.
Problems of dynamic optimization in the face of uncertainty are frequently posed as Markov decision processes (MDPs).
The question of similarity between two sets of examples is common to many ﬁelds, including statistics, data mining, machine learning and computer vision.
Fast clustering algorithms are a staple of exploratory data analysis.
A number of problems in Computer Vision, Natural Language Processing and Computational Biology involve predictions over complex but structured interdependent outputs, also known as structured-output prediction.
In this work we consider multiclass prediction: The problem of classifying objects into one of several possible target classes.
Data clustering is a data analysis methodology which aims to automatically reveal the underlying structure of data.
The problem of learning multiple classes from data with imprecise label information has attracted a recent attention in the literature.
Nonnegative matrix factorization (NMF) is a popular approach for selecting features in data [16–18, 23].
Partially-observable Markov decision processes (POMDPs) are a powerful modeling formalism for real-world sequential decision-making problems [3].
Discriminative learning algorithms are typically trained from large collections of vectorial training examples.
Given an infinite-horizon stationary γ-discounted Markov Decision Process [24, 4], we consider approximate versions of the standard Dynamic Programming algorithms, Policy and Value Iteration, that build sequences of value functions vk and policies πk as follows vk+1 ← T vk + k+1 Approximate Value Iteration (AVI):  Approximate Policy Iteration (API): vk πk+1 ← vπ k +  k ← any element of G(vk ) (1) (2) where v0 and π0 are arbitrary, T is the Bellman optimality operator, vπk is the value of policy πk and G(vk ) is the set of policies that are greedy with respect to vk .
Human activity understanding has been a research topic of substantial interest in computer vision [1].
Additive Models are a class of nonparametric regression methods which have been the subject of intensive theoretical research and found widespread applications in practice (see [1]).
Neuroimaging is a powerful tool for characterizing neurodegenerative process in the progression of Alzheimer’s disease (AD).
With the increasing amount of data that is available for training, it becomes an urgent task to devise efficient algorithms for optimization/learning problems with unprecedented sizes.
Normative theories of human choice behavior have long been based on how economic theory has postulated they should be made.
How many processors should we use and how often should they communicate for large-scale distributed optimization? We address these questions by studying the performance and limitations of a class of distributed algorithms that solve the general optimization problem m minimize F (x) = x∈X 1 X lj (x) m j=1 (1) where each function lj (x) is convex over a convex set X ⊆ Rd .
We address situations in which an informed choice between candidate predictive models—for instance, a baseline method and a challenger—has to be made.
Discrete undirected graphical models have seen wide use in natural language processing [11, 24] and computer vision [19].
Temporal-difference (TD) learning is a widely used method in reinforcement learning (RL).
Covariance selection, first described in [2], has come to refer to the problem of estimating a normal distribution that has a sparse inverse covariance matrix P, whose non-zero entries correspond to edges in an associated Gaussian Markov Random Field, [3].
Observed image signals are often corrupted by acquisition channel or artificial editing.
Information in the real world comes through multiple input channels.
Linear models are widely used for a variety of tasks including classification and regression.
Kernel methods [16], such as support vector machines, are among the most effective learning methods.
Ranking is a central problem in many applications, such as document retrieval, meta search, and collaborative filtering.
The considered problem has a simple formulation: Given are multiple similarities between the same set of n data points, each similarity can be represented as a weighted graph.
The nearest neighbor (NN) classifier is one of the simplest and most classical non-linear classification algorithms.
The motivating hypothesis behind multi-task learning (MTL) algorithms is that leveraging data from related tasks can yield superior performance over learning from each task independently.
Recent progress in voltage clamp techniques has enabled the recording of local membrane voltage in dendritic branches, and this greatly changed our view of the potential for single neuron computation.
Every year, more than 34,000 suicides occur and over 370,000 individuals are treated for selfinflicted injuries in emergency rooms in the U.
Learning a model of a high-dimensional environment can pose a significant challenge, but the ability to make predictions about future events is key to good decision making.
Multi-Agent Plan Recognition (MAPR) seeks an explanation of observed team-action traces.
Latent variable models have shown great success in various fields, including computational linguistics and machine learning.
We consider the problem of nonparametrically estimating the Shannon mutual information between two random variables.
Many risk minimization problems in machine learning can be formulated into a regularized stochastic optimization problem of the following form: minx∈X {φ(x) := f (x) + h(x)}.
An important statistical problem in the study of natural systems is to estimate the entropy of an unknown discrete distribution on the basis of an observed sample.
Machine learning algorithms are rarely parameter-free: parameters controlling the rate of learning or the capacity of the underlying model must often be specified.
With the tremendous growth of data, providing a multi-granularity conceptual view using hierarchical classification (HC) has become increasingly important.
How to compare examples is a fundamental question in machine learning.
Diffusion processes are a flexible and useful tool in stochastic modelling.
Bayesian nonparametric time series models, including various “infinite” Markov switching processes [1, 2, 3], provide a promising modeling framework for complex sequential data.
The rapidly growing number of products available online makes it increasingly difficult for users to choose the ones worth their attention.
In the multivariate regression problem the objective is to estimate the conditional mean E(Y ∣ X) = m(X) = (m1 (X), .
We are given a sequence x := X1 , X2 , .
The last few years have seen a tremendous interest in the study, understanding and statistical modeling of complex networks [14, 6].
The multi–armed bandit [13] elegantly formalizes the problem of on–line learning with partial feedback, which encompasses a large number of real–world applications, such as clinical trials, online advertisements, adaptive routing, and cognitive radio.
A plethora of the problems arising in machine learning involve computing an approximate minimizer of the sum of a loss function over a large number of training examples, where there is a large amount of redundancy between examples.
Consider sentiment analysis task for a set of reviews for different products.
The fitting of complex models to big data often requires computationally intractable integrals to be approximated.
Inverse reinforcement learning (IRL) aims to find the agent’s underlying reward function given the behaviour data and the model of environment [1].
Convex optimization has proved to be extremely useful to all quantitative disciplines of science.
Despite the anatomical and functional similarities between the primary auditory cortex (A1) and the primary visual cortex (V1), the computational modelling of A1 has proven to be less fruitful than V1, primarily because the responses of A1 cells are more disorganized.
The Bregman divergence first appeared in the context of relaxation techniques in convex programming ([4]), and has found numerous applications as a general framework in clustering ([2]), proximal minimization ([5]) and online learning ([27]).
Derivative-free optimization schemes have a long history in optimization (see, for example, the book by Spall [21]), and they have the clearly desirable property of never requiring explicit gradient calculations.
Recent approaches to collaborative filtering (CF) have concentrated on estimating an algebraic or statistical model, and using the model for predicting the missing rating of user u on item i.
Extracting a 3D representation from a single-view image depicting a 3D object has been a longstanding goal of computer vision [20].
Clustering is a fundamental problem in data analysis, and has extensive applications in statistics, data mining, computer vision and even in social sciences.
Magnetic Resonance Imaging (MRI) is widely used for observing the tissue changes of the patients within a non-invasive manner.
Behavioral evidence shows that animal behaviors are often influenced not only by the content of sensory information but also by its uncertainty.
Markov Decision Processes (MDP) provide a rich and elegant mathematical framework for solving sequential decision-making problems.
Over the past several years, online convex optimization has emerged as a fundamental tool for solving problems in machine learning (see, e.
Forecasting systems behavior with multiple responses has been a challenging issue in many contexts of applications such as collaborative filtering, financial markets, or bioinformatics, where responses can be, respectively, movie ratings, stock prices, or activity of genes within a cell.
Local image descriptors have long been explored in the context of machine learning and computer vision.
Many problems of relevance in machine learning, signal processing, and high dimensional statistics can be posed in composite form: minimize f (x) := g(x) + h(x), n x∈R (1) where g : Rn → R is a convex, continuously differentiable loss function, and h : Rn → R is a convex, continuous, but not necessarily differentiable penalty function.
Partitioning data points into sensible groups is a fundamental problem in machine learning.
The increasing availability of genetic data (for example, from the Thousand Genomes project [1]) and the importance of genetics in scientific and medical applications requires the development of scalable and accurate models for genetic sequences which are informed by genetic processes.
Data in the form of partial rankings, i.
Variational bounds provide a convenient approach to approximate inference in a range of intractable models [Ghahramani and Beal, 2001].
Scalp recorded electroencephalography (EEG) can be used for non-muscular control and communication systems, commonly called brain-computer interfaces (BCI).
The two-alternative forced-choice (2AFC) task is a standard experimental paradigm used in psychology and neuroscience to investigate various aspects of sensory, motor, and cognitive processing [5].
A Deep Boltzmann Machine (DBM) is a type of binary pairwise Markov Random Field with multiple layers of hidden random variables.
In many real-world classification problems, the output labels are organized in a hierarchy.
Consider the classical problem of Gaussian linear regression1 : Y = Xβ ∗ + σ ∗ ξ, ξ ∼ Nn (0, In ), (1) where Y ∈ Rn and X ∈ Rn×p are observed, in the neoclassical setting of very large dimensional unknown vector β ∗ .
Whilst Bayesian methods have played a significant role in machine learning and related areas (see [1] for an introduction), improving the class of distributions for which inference is either tractable or can be well approximated remains an ongoing challenge.
The use of Markov chain Monte Carlo methods can be extremely challenging in many modern day applications.
The two sample problem addresses the question of whether two independent samples are drawn from the same distribution.
Mesh segmentation methods decompose a three-dimensional (3D) mesh, or a collection of aligned meshes, into their constituent parts.
The term “reservoir computing” encompasses a range of similar machine learning techniques, independently introduced by H.
Probabilistic methods based on Gaussian densities have celebrated successes throughout machine learning.
Rank aggregation is an important task in a wide range of learning and social contexts arising in recommendation systems, information retrieval, and sports and competitions.
Linear programming is a fundamental mathematical model with numerous applications in both combinatorial and continuous optimization.
The development of successful methods for training deep architectures have influenced the development of representation learning algorithms either on top of SIFT descriptors [1, 2] or raw pixel input [3, 4, 5] for feature extraction of full-sized images.
Many problems in machine learning are based on a form of (regularized) empirical risk minimization.
Variational Bayesian (VB) approximation [1] was proposed as a computationally efficient alternative to rigorous Bayesian estimation.
The Lovász ϑ function [19] plays a fundamental role in modern combinatorial optimization and in various approximation algorithms on graphs, indeed Goemans was led to say It seems all roads lead to ϑ [10].
The Bag of Words (BoW) [7] is the de facto standard image feature for the image categorization.
In this paper we study the problem of learning from random samples a probability distribution supported on a manifold, when the learning error is measured using transportation metrics.
Anomaly detection is an important problem that has been studied in a variety of areas and used in diverse applications including intrusion detection, fraud detection, and image processing [1, 2].
Let A ∈ Rm×n be a rank-r matrix, where r ≪ m, n.
This paper aims at promoting an infrequent way to tackle multiclass prediction problems: we advocate for the use of the confusion matrix —the matrix which reports the probability of predicting class q for an instance of class p for all potential label pair (p, q)— as the objective ‘function’ to be optimized.
Time-series data are available in many different fields, including medicine, finance, information retrieval and weather prediction.
In reinforcement learning (RL), the agent interacts with a (partially) unknown environment, classically assumed to be a Markov decision process (MDP), with the goal of maximizing its expected long-term total reward.
Crowdsourcing has become an efficient and inexpensive way to label large datasets in many application domains, including computer vision and natural language processing.
Object recognition research has made impressive gains in recent years, with particular success in using discriminative learning algorithms to train classifiers tuned to each category of interest (e.
There has been growing interest in the machine learning community to model dynamical systems in continuous time.
Associated with any undirected graphical model [1] is the so-called density of states, a term borrowed from statistical physics indicating a distribution that, for any likelihood value, gives the number of configurations with that probability.
A central problem in network analysis is to identify communities, groups of related nodes with dense internal connections and few external connections [1, 2, 3].
Informative subset selection problems arise in many applications where a small number of items must be chosen to represent or cover a much larger set; for instance, text summarization [1, 2], document and image search [3, 4, 5], sensor placement [6], viral marketing [7], and many others.
Real world problems usually demand continuous state or action spaces, and one of the challenges for reinforcement learning is to deal with such continuous domains.
The Restricted Boltzmann Machine (RBM) [1, 2] is an important class of probabilistic graphical models.
Graphical models have proven to be a useful tool for performing approximate inference in a wide variety of application areas including computer vision, combinatorial optimization, statistical physics, and wireless networking.
A neuron represents sensory information via its spike train.
In the matrix reconstruction problem, we are given a matrix Y ∈ Rn×m whose entries are only partly observed, and would like to reconstruct the unobserved entries as accurately as possible.
A variety of problems in machine learning, from ranking to multi-object tracking, involve inference over permutations.
R Non-linear entropy functionals of a multivariate density f of the form g(f (x), x)f (x)dx arise in applications including machine learning, signal processing, mathematical statistics, and statistical communication theory.
Conditional Random Fields (CRF) are a widely popular class of discriminative models for the distribution of a set of hidden states conditional on a set of observable variables.
In this paper we consider the problem of numerical integration of a differentiable function f : [0, 1]d → R given a ﬁnite budget n of evaluations to the function that can be allocated sequentially.
In the field of theoretical signal processing, compressive sensing (CS) has arguably been one of the major developments of the past decade.
We consider the problem of learning high dimensional graphical models.
Models of disease progression are among the core tools of modern medicine for early disease diagnosis, treatment determination and for explaining symptoms to patients.
Perhaps the most striking property of biological visual systems is their ability to efficiently cope with the high bandwidth data streams received from the eyes.
Automatic speech recognition (ASR), the process of automatically translating spoken words into text, has been an important research topic for several decades owing to its wide array of potential applications in the area of human-computer interaction (HCI).
In regression problems over Rd , the unknown function f might vary more in some coordinates than in others, even though all coordinates might be relevant.
Figure 1: We describe two-stage models for detecting and analyzing the 3D shape of objects in unconstrained images.
Our focus in this paper is on unsupervised learning problems such as matrix factorization or latent subspace identification.
Feature selection is a key component in many machine learning settings.
Gaussian processes (GP) are a popular non-parametric prior for function estimation.
Topic models use latent variables to explain the observed (co-)occurrences of words in documents.
Ultimately, the main aim of a clinical trial is straightforward: it is to examine and quantify the effectiveness of a treatment of interest.
Consider a standard least squares regression problem.
The structures of organizational communication networks are critical to collaborative problem solving [1].
Many machine learning algorithms presuppose the existence of a pairwise similarity measure on the input space.
This paper focuses on modeling data matrices by simultaneously capturing the dependent structures among both rows and columns, which is especially useful for filling missing values.
Sensitive statistical data on individuals are ubiquitous, and publishable analysis of such private data is an important objective.
Following recent advances in learning algorithms and robust feature representations, tasks in video understanding have shifted from classifying simple motions and actions [3, 4] to detecting complex events and activities in Internet videos [1,5,6].
Representing salient image regions in a way that is invariant to unwanted image transformations is a crucial Computer Vision task.
Part-based and hierarchical representations have been widely studied in computer vision, and lead to some elegant frameworks for complex object detection and recognition.
Structured relational data arises in a variety of contexts, including graph-valued data [e.
Causal discovery aims to discover the underlying generating mechanism of the observed data, and consequently, the causal relations allow us to predict the effects of interventions on the system [15, 19].
Imitation learning has been successfully applied to a variety of applications [1, 2].
A fundamental problem in probability and statistics is to determine with overwhelming probability the rate of convergence of the empirical covariance (or inverse covariance) of an i.
The task of learning a policy for a sequential decision problem with continuous state space is a long-standing challenge that has attracted the attention of the reinforcement learning community for years.
Scene understanding approaches have largely focused on understanding the physical structure of a scene: “what is where?” [1].
Many real applications can be reduced to a ranking problem.
A fundamental challenge faced by the brain is to combine noisy sensory information with prior knowledge in order to perceive and act in the natural world.
During the past decades, a large number of algorithms have been proposed to deal with learning problems in the case of single-valued functions (e.
Here we study learning and inference in the overcomplete linear model given by Y x = As, p(s) = fi (si ), (1) i where A ∈ RM ×N , N ≥ M , and each marginal source distribution fi may depend on additional parameters.
Consider a book recommendation system.
Large scale multilabel classification problems arise in several practical applications and has recently generated a lot of interest with several efficient algorithms being proposed for different settings [1, 2].
Neurophysiology experiments are costly and time-consuming.
In the area of X-ray imaging, phase retrieval (PR) refers to the problem of recovering a complex multivariate signal from the squared magnitude of its Fourier transform.
The hidden Markov model (HMM) [1] is a probabilistic model that assumes a signal is generated by a double embedded stochastic process.
Latent structure analysis of sequence data is an important technique for many applications such as speech recognition, bioinformatics, and natural language processing.
Topic models, such as Latent Dirichlet Allocation (LDA) [3], have shown great promise in discovering latent semantic representations of large collections of text documents.
Probabilistic models play a crucial role in many scientific disciplines and real world applications.
Minwise hashing [4, 3] is a standard technique in the context of search, for efficiently computing set similarities.
In this paper, we focus on the learning of a general-purpose non-linear classifier applied to perceptual signals such as vision and speech.
Decision-making entities, whether they are businesses, governments, or individuals, usually interact in game-theoretic environments, in which the final outcome is intimately tied to the actions taken by others in the environment.
Resampling methods (e.
Neuromorphic systems try to replicate cognitive processing functions in integrated circuits.
When humans address a new learning problem, they often use knowledge acquired while learning different but related tasks in the past.
Deep learning and unsupervised feature learning have shown great promise in many practical applications.
Stochastic optimization algorithms have many desirable features for large-scale machine learning, and have been studied intensively in the last few years (e.
Advances in sensory neuroscience rely on the development of testable functional models for the encoding of sensory stimuli in neural responses.
Legislative behavior centers around the votes made by lawmakers.
In this paper we propose a simple but new model to learn informative linear projections of multivariate data.
We present an algorithm (with rigorous performance guarantees) for a basic statistical problem.
A key objective in the theory of Markov Decision Processes (MDPs) is to maximize the expected sum of discounted rewards when the dynamics of the MDP are (perhaps partially) unknown.
A surrogate loss is a loss function used as a substitute for the true quality measure during training in order to ease the optimization of the empirical risk.
As social animals, people constantly organise themselves into social groups.
The estimation of probability density functions over sets of random variables is a central problem in learning.
How is the brain structured? The recent field of connectomics [2] is developing high-throughput techniques for mapping connections in nervous systems, one of the most important and ambitious goals of neuroanatomy.
The framework of graphical models allows for parsimonious representation of high-dimensional data by encoding statistical relationships among the given set of variables through a graph, known as the Markov graph.
There are natural tensions between learning and privacy that arise whenever a learner must aggregate data across multiple individuals.
Images of three dimensional objects exhibit a great deal of variation due to viewpoint.
Data samples described in high-dimensional feature spaces are encountered in many important areas.
An enduring challenge for machine learning is in the development of algorithms that scale to truly large data sets.
Our study is broadly motivated by questions in high-dimensional learning.
Retrieving relevant content from massive databases containing high-dimensional data is becoming common in many applications involving images, videos, documents, etc.
The Kalman filter and its extensions [1], such as the extended and unscented Kalman filters [7], are principled statistical models that have been widely used for some of the most challenging and mission-critical applications in automatic control, robotics, machine learning, and economics.
Functional Magnetic Resonance Imaging (fMRI) is a technique used in psychological experiments to measure the blood oxygenation level throughout the brain, which is a proxy for neural activity; this measurement is called brain activation.
Sparse Coding (SC) is one of the most popular algorithms for feature learning and has become a standard approach in Machine Learning, Computational Neuroscience, Computer Vision, and other related ﬁelds.
The nominal goal of predictive inference is to achieve high accuracy.
Dynamical systems (DS) have proved to be a promising framework for encoding and generating complex motions.
We consider the undirected graph estimation problem for a d-dimensional random vector X = (X1 , .
Symmetric positive definite (spd) matrices1 are remarkably pervasive in a multitude of areas, especially machine learning and optimization.
MAP inference on graphical models is a central problem in machine learning, pattern recognition, and computer vision.
We consider sequential prediction of outcomes y1 , y2 , .
Consider the `1 -regularized loss minimization problem n min w 1X `(yi , (Xw)i ) + kwk1 , n i=1 (1) where X 2 IRn⇥p is the design matrix, w 2 IRp is a weight vector to be estimated, and the loss function ` is such that `(y, ·) is a convex differentiable function for each y.
Topic models (also known as mixed-membership models) are a useful method for analyzing large text collections [1, 2].
Structure learning for graphical models is a problem that arises in many contexts.
Dimensionality reduction is one of the most important forms of unsupervised learning, with roots dating to the origins of data analysis.
Locality-sensitive hashing (LSH) method aims to hash similar data samples to the same hash code with high probability [7, 9].
Many algorithms are now available to learn hierarchical features from unlabeled image data.
Reinforcement learning (RL) is a well known framework that formalizes decision making in unknown, uncertain environments.
It is well known that outliers have a detrimental effect on standard regression estimators.
We develop a novel approach for supervised learning based on adaptively partitioning the feature space into different regions and learning local region classifiers.
Current approaches to object recognition make essential use of machine learning methods.
In machine learning, the notion of “abstention” commonly refers to the possibility of refusing a prediction in cases of uncertainty.
Random walks have been widely used for graph-based learning, leading to a variety of models including PageRank [14] for web page ranking, hitting and commute times [8] for similarity measure between vertices, harmonic functions [20] for semi-supervised learning, diffusion maps [7] for dimensionality reduction, and normalized cuts [12] for clustering.
Automatic semantic analysis of multimedia content has been an active area of research due to potential implications for indexing and retrieval [1–7].
The Principal Component Analysis (PCA) is introduced as follows.
In the last decade, several direct policy search (DPS) methods have been developed in the field of reinforcement learning (RL) [1, 2, 3, 4, 5, 6, 7, 8, 9] and have been successfully applied to practical decision making applications [5, 7, 9].
Our visual systems are amazingly competent at recognizing patterns in images.
Some of the most influential machine learning tools are based on the hypothesis class of halfspaces with margin.
Information theory, machine learning, and statistics, are closely related disciplines.
In this paper, we focus on the setting of intrinsically motivated reinforcement learning (see Oudeyer and Kaplan [2007], Baranes and Oudeyer [2009], Schmidhuber [2010], Graziano et al.
Mixture distributions have been widely used for statistical modeling of complex data.
In this paper, we consider the well-studied non-stochastic expert problem in a distributed setting.
Multivariate response prediction, also known as multiple-output regression [3] when the responses are real-valued vectors, is an important problem in machine learning and statistics.
Modeling data using low-dimensional representations is a fundamental approach in data analysis, motivated by the inherent redundancy in many datasets and to increase the interpretability of data via dimensionality reduction.
We consider the problem of learning discriminative classification models for visual recognition.
Human long-term memory can store a remarkable amount of visual information and remember thousands of different pictures even after seeing each of them only once [25, 1].
Hierarchical clustering models aim to fit hierarchies to data, and enjoy the property that clusterings of varying size can be obtained by “pruning” the tree at particular levels.
Time delay is pervasive in neural information processing.
Linear classifiers, including SVM and boosting, play an important role in machine learning.
The Kolmogorov-Smirnov (KS) test is efficient, simple, and often considered the choice method for comparing distributions.
Recent value-based reinforcement learning applications have shown the benefit of exhaustively generating features, both in discrete and continuous state domains.
Many algorithms are now available to learn hierarchical features from unlabeled image data.
Arising from domains as diverse as bioinformatics and web mining, large-scale data exhibiting network structure are becoming increasingly available.
The information bottleneck method (IB) [1] considers the concept of relevant information in the data compression problem, and takes a new perspective to signal compression which was classically treated using rate distortion theory.
In statistical learning theory, one of the major concerns is to obtain the generalization bound of a learning process, which measures the probability that a function, chosen from a function class by an algorithm, has a sufficiently small error (cf.
This paper focuses on nonconvex composite objective problems having the form minimize Φ(x) := f (x) + h(x) x ∈ X, (1) where f : Rn → R is continuously differentiable, h : Rm → R ∪ {∞} is lower semi-continuous (lsc) and convex (possibly nonsmooth), and X is a compact convex set.
Binary classification is one of the most well-understood problems of machine learning and statistics: a wealth of efficient classification algorithms has been developed and applied to a wide range of applications.
The use of graphical models [1, 2] is ubiquitous in machine learning.
Consider the estimation of a random vector x ∈ Rn from a measurement vector y ∈ Rm .
The problem of learning from examples is in most circumstances ill-posed.
This work aims to generate useful probabilistic models of high dimensional trajectories in continuous spaces.
Blaschko and Lampert have recently shown that object localization can be approached as structured regression problem [2].
In reinforcement learning (RL), an agent autonomously learns how to make optimal sequential decisions by interacting with the world.
Bayesian nonparametric (BNP) models [1] have emerged as an important tool for building probability models with flexible latent structure and complexity.
Collaborative prediction is a task of predicting users’ potential preferences on currently unrated items (e.
One of the central problems in machine learning is prediction/inference, where given an input datum X, we would like to predict or infer the value of a target variable of interest, y, assuming X and y have some intrinsic relationship.
Latent variables models such as principal components analysis (Pearson, 1901; Hotelling, 1933; Tipping and Bishop, 1999; Roweis, 1998) and factor analysis (Young, 1941) are popular for summarising high dimensional data, and can be seen as modelling the covariance of the observed dimensions.
Given x1 , .
Bayesian nonparametric methods provide an increasingly important framework for unsupervised learning from structured data.
The essence of machine learning is to exploit what we observe in order to form accurate predictors of what we cannot.
Dimensionality reduction is a fundamental tool for understanding complex data sets that arise in contemporary machine learning and data mining applications.
Latent linear dynamical system (LDS) models, also known as Kalman-filter models or linearGaussian state-space models, provide an important framework for modelling shared temporal structure in multivariate time series.
By giving reward at the right times, animals like monkeys can be trained to perform complex tasks that require the mapping of sensory stimuli onto responses, the storage of information in working memory and the integration of uncertain sensory evidence.
Discrete mixture models characterize the density of y ∈ Y ⊂ <m as f (y) = k X ph φ(y; γh ) (1) h=1 where p = (p1 , .
Regularization [1] is a popular and well-studied methodology to address ill-posed estimation problems [2] and learning from examples [3].
In supervised classification, the goal is to learn a classifier from a collection of training instances, where each instance has a unique class label.
The recent interest in understanding human perception and behavior from the perspective of neuroscience and cognitive psychology has spurred a revival of interest in mathematical decision theory.
The goal of this paper is to develop an extended framework for supervised learning with similarity functions.
Reinforcement learning (RL) agents need to solve the exploitation-exploration tradeoff.
It is widely recognized that the process of fitting observed data to a statistical model needs to incorporate latent or hidden factors, which are not directly observed.
Historically, the fields of statistical inference and stochastic optimization have often developed their own specific methods and approaches.
Probabilistic graphical models (PGMs) have become useful tools for classical machine learning tasks, such as multi-label classification [1] or semi-supervised learning [2], as well for many realworld applications, for example image processing [3], natural language processing [4], bioinformatics [5], and computational neuroscience [6].
Protein structure prediction from amino acidic sequence is one of the grand challenges in Bioinformatics and Computational Biology.
Object class detection is a central problem in computer vision.
In this paper we address the problem of adaptive control of a high dimensional linear quadratic (LQ) system.
Natural sounds, such as speech and animal vocalizations, consist of complex acoustic events occurring at multiple scales.
Modeling large, complex, real-world domains requires the ability to handle both rich relational structure and large amount of uncertainty.
Human memory has a vast capacity, storing all the semantic knowledge, facts, and experiences that people accrue over a lifetime.
A large fraction of the machine learning community is concerned itself with the formulation of a learning problem as a single, well-defined optimization problem.
Probabilistic graphical models [Pearl, 1988] are widely use to model and reason about phenomena in a variety of domains such as medical diagnosis, communication, machine vision and bioinformatics.
Many applications involve simultaneous prediction of multiple variables.
Object recognition is a major focus of research in computer vision and machine learning.
One of the most challenging aspects of image recognition is the large amount of intra-class variability, due to factors such as lighting, background, pose, and perspective transformation.
The cocktail party problem, or the speech separation problem, is one of the central problems in speech processing.
Finding principles underlying learning in neural networks is an important problem for both artificial and biological networks.
There are many factors that contribute to a document’s word choice: topic, syntax, sentiment, author perspective, and others.
Online social networks allow users to follow streams of posts generated by hundreds of their friends and acquaintances.
Generative parsing models, which define joint distributions over sentences and their parse trees, are one of the core techniques in computational linguistics.
In addition to functional localization and integration, mapping the neural correlates of “mental states” or “brain states” (i.
A rapidly emerging theme in the analysis of networked data is the study of signed networks.
The problem of best arm(s) identification [6, 3, 1] in the stochastic multi-armed bandit setting has recently received much attention.
Robustness against uncertainties and sensitivity to risk are major issues that have been addressed in recent development of the Markov decision process (MDP).
As bigger and more complex datasets are available, multiclass learning is becoming increasingly important in machine learning.
This paper studies the online learning framework, where the goal of the player is to incur small regret while observing a sequence of data on which we place no distributional assumptions.
In statistical learning (also called batch learning) [1] one obtains a random sample (X1 , Y1 ), .
Biological vision systems have evolved sophisticated tracking mechanisms, capable of tracking complex objects, undergoing complex motion, in challenging environments.
Kernel learning methods (such as Support Vector Machines) are conceptually simple, strongly rooted in statistical learning theory, and can often be formulated as a convex optimization problem.
The modeling of temporal dependencies is an important and challenging task with applications in fields that use forecasting or retrospective analysis, such as finance, biomedicine, and anomaly detection.
Online convex optimization is a sequential prediction paradigm in which, at each time step, the learner chooses an element from a fixed convex set S and then is given access to a convex loss function defined on the same set.
Many tasks in text and speech processing, computational biology, or learning models of the environment in reinforcement learning, require estimating a function mapping variable-length sequences to real numbers.
We consider the problem of finding a set of locally-biased vectors that inherit many of the “nice” properties that the leading nontrivial global eigenvectors of a graph Laplacian have—for example, that capture “slowly varying” modes in the data, that are fairly-efficiently computable, that can be used for common machine learning and data analysis tasks such as kernel-based and semi-supervised learning, etc.
When models contain multiple output variables, an important potential source of structure is the number of variables that take on a particular value.
Contour detection is a fundamental problem in vision.
One often has to use function approximation to represent the near optimal value function of the reinforcement learning (RL) and planning problems with large state spaces.
Bipartite matching problems (BMPs), which involve mapping one set of items to another, are ubiquitous, with applications ranging from computational biology to information retrieval to computer vision.
An extensive-form game is a common formalism used to model sequential decision making problems containing multiple agents, imperfect information, and chance events.
One of the key challenges of computer vision is the robust representation of complex objects and so over the years, increasingly rich features have been proposed.
The number of parameters in a textbook probabilistic graphical model (PGM) is an exponential function of the number of parents of the nodes in the graph.
Finding genetic correlates of disease is a long-standing important problem with potential contributions to diagnostics and treatment of disease.
Graphical model inference is now prevalent in many fields, running the gamut from computer vision and civil engineering to political science and epidemiology.
The estimation of the inverse of the covariance matrix (also referred to as precision matrix or concentration matrix) is a very important problem with applications in a number of fields, from biology to social sciences, and is a fundamental step in the estimation of underlying data networks.
Collaborative Filtering (CF) is a method of making predictions about an individual’s preferences based on the preference information from many users.
Let Θ ∈ IRd1 ×d2 be a matrix of interest and Ω∗ = {1, .
Nonparametric mixture models allow us to bypass the issue of model selection, by modeling data using a random number of mixture components that can grow if we observe more data.
Variational inference algorithms pose probabilistic inference as an optimization over distributions.
We are interested in visual learning for recognition of objects and scenes embedded in physical space.
Crowdsourcing provides an easy and relatively inexpensive way to utilize human capabilities to solve difficult computational learning problems (e.
A central problem in systems neuroscience is to understand the probabilistic representation of information by neurons and neural populations.
Understanding dependencies within multivariate data is a central problem in the analysis of financial time series, underpinning common tasks such as portfolio construction and calculation of value-atrisk.
Generative latent variable models offer an intuitive way to explain data in terms of hidden structure, and are a cornerstone of exploratory data analysis.
While supervised training is an integral part of building visual, textual, or multi-modal category models, more recently, knowledge transfer between categories has been recognized as an important ingredient to scale to a large number of categories as well as to enable fine-grained categorization.
Continuous time stochastic processes provide a flexible and popular framework for data modelling in a broad spectrum of scientific and engineering disciplines.
We consider the classical reinforcement learning problem of an agent interacting with its environment while trying to maximize total reward accumulated over time [1, 2].
To find the best solution to a complex real-life search problem, e.
It is widely thought that our very ability to remember the past over long time scales depends crucially on our ability to modify synapses in our brain in an experience dependent manner.
In machine learning, we often encounter the following optimization problem.
Information theoretic quantities are popular tools in neuroscience, where they are used to study neural codes whose representation or function is unknown.
Ridge Regression, which penalizes the `2 norm of the weight vector and shrinks it towards zero, is the most widely used penalized regression method.
Standard Restricted Boltzmann Machines (RBMs) are a type of Markov Random Field (MRF) characterized by a bipartite dependency structure between a group of binary visible units x ∈ {0, 1}n and binary hidden units h ∈ {0, 1}m .
Problems related to graph isomorphisms have been an important and enjoyable challenge for the scientific community for a long time.
Decentralized partially observable Markov decision processes (DecPOMDPs) are a popular model for cooperative multi-agent decision problems; however, they are NEXP-complete to solve [15].
Given an m n matrix A, it is often desirable to find a sparser matrix B that is a good proxy for A.
Matrix completion has attracted a lot of attention over the past few years.
The visual world is populated with a vast number of objects, the most appropriate labeling of which is often ambiguous, task specific, or admits multiple equally correct answers.
Multi-relational data refers to directed graphs whose nodes correspond to entities and edges of the form (head, label, tail) (denoted (h, `, t)), each of which indicates that there exists a relationship of name label between the entities head and tail.
Stochastic gradient (SG) optimization [1, 2] is widely used for training machine learning models with very large-scale datasets.
The classification problem in machine learning and data mining is to predict an unobserved discrete output value y based on an observed input vector x.
Kernel-based algorithms are widely used in machine learning and have been shown to often provide very effective solutions.
Natural actor-critics form a class of policy search algorithms for ﬁnding locally optimal policies for Markov decision processes (MDPs) by approximating and ascending the natural gradient [1] of an objective function.
Motivated by applications in viral marketing [1], researchers have been studying the influence maximization problem: find a set of nodes whose initial adoptions of certain idea or product can trigger, in a time window, the largest expected number of follow-ups.
The principle of parsimony is used in many areas of science and engineering to promote “simple” models over more complex ones.
Ordinary Least Squares (OLS) is one of the oldest and most widely studied statistical estimation methods with its origins tracing back over two centuries.
In the real world, two images of the same object may only be related by a very complicated and highly nonlinear transformation.
The problem of acquiring an N -dimensional signal x through M linear measurements, y = F x, arises in many contexts.
Convex optimization has become a tool central to many areas of engineering and applied sciences, such as signal processing [20] and machine learning [24].
Gaussian process (GP) inference methods have been successfully applied to models for dynamical systems, see e.
Model-based machine learning and probabilistic programming offer the promise of a world where a probabilistic model can be specified independently of the inference routine that will operate on the model.
Multi-task learning (MTL) has been studied for learning multiple related tasks simultaneously.
Probability distributions over spike words form the fundamental building blocks of the neural code.
High-treewidth graphical models typically yield distributions where exact inference is intractable.
Massive datasets are becoming an ubiquitous by-product of modern scientific and industrial applications.
Search advertising, also known as sponsored search, has been formulated as a multi-armed bandit (MAB) problem [11], in which the search engine needs to choose one ad from a pool of candidate to maximize some objective (e.
Processing of signals on graphs is emerging as a fundamental problem in an increasing number of applications [22].
The computation performed by a neural circuit is a product of the properties of single neurons in the circuit and their connectivity.
In terms of sheer size, visual data is, by most accounts, the biggest “Big Data” out there.
Dropout is an algorithm for training neural networks that was described at NIPS 2012 [7].
Motivation and background In analyzing multivariate time series data, collected in financial applications, monitoring of influenza outbreaks and other fields, it is often of key importance to accurately characterize dynamic changes over time in not only the mean of the different elements (e.
For evolutionary reasons, biological tissue at all spatial scales is composed of repeating patterns.
During the recent years, there has been a growing interest on the problem of learning a tensor from a set of linear measurements, such as a subset of its entries, see [9, 17, 22, 23, 25, 26, 27] and references therein.
A collection of documents, each consisting of a disorganized bag of words is often modeled compactly using mixture or admixture models, such as Latent Semantic Analysis (LSA) [4] and Latent Dirichlet Allocation (LDA) [1].
Similarity search algorithms are essential to multimedia retrieval, computational biology, and statistical machine learning.
Consider a politician trying to elude a group of reporters.
Many applications of machine learning, ranging from computer vision to computational biology, require the analysis of large volumes of high-dimensional continuous-valued measurements.
As we move towards more complete image understanding, having more precise and detailed object recognition becomes crucial.
Bayesian networks have been popular tools for representing the probability distribution over a large number of variables.
Decision trees have a long history in machine learning and were one of the first models proposed for inductive learning [14].
Modern applications awaiting next generation machine intelligence systems have posed unprecedented scalability challenges.
Calcium imaging methods have revolutionized data acquisition in experimental neuroscience; we can now record from large neural populations to study the structure and function of neural circuits (see e.
In many situations, individuals wish to share their personal data for machine learning applications and other exploration purposes.
A set function f : 2V → R is said to be submodular [4] if for all subsets S, T ⊆ V , it holds that f (S) + f (T ) ≥ f (S ∪ T ) + f (S ∩ T ).
We live in the big data era – a world where an overwhelming amount of data is generated and collected every day, such that it is becoming increasingly impossible to process data in its raw form, even though computers are getting exponentially faster over time.
In the context of network analysis, a latent space refers to a space of unobserved latent representations of individual entities (i.
Machine vision methods have achieved considerable success in recent years, as evidenced by performance on major challenge problems [4, 7], where strong performance has been obtained for assigning one of a large number of labels to each of a large number of images.
Parsimony, preferring a simple explanation to a more complex one, is probably one of the most intuitive principles widely adopted in the modeling of nature.
Graph-based learning algorithms have received considerable attention in machine learning community.
Bayesian nonparametric mixture models [7] provide an important framework to describe complex data.
Policy evaluation, i.
Invariances are among the most useful prior information used in machine learning [1].
Multilabel classification (MLC) is a classification task where each input may be associated to several class labels, and the goal is to predict the label set given the input.
Spatial filtering is a crucial step in the reliable decoding of user intention in Brain-Computer Interfacing (BCI) [1, 2].
It is well-known that Dirichlet process mixtures (DPMs) of normals are consistent for the density — that is, given data from a sufficiently regular density p0 the posterior converges to the point mass at p0 (see [1] for details and references).
Low-rankedness of matrices has frequently been exploited when one reconstructs a matrix from its noisy observations.
The geometry of Hermitian positive definite (hpd) matrices is remarkably rich and forms a foundational pillar of modern convex optimisation [21] and of the rapidly evolving area of convex algebraic geometry [4].
EDML is a recently proposed algorithm for learning MAP parameters of a Bayesian network from incomplete data [5, 16].
Privacy is an important problem in data analysis.
Robot navigation relies on at least three sub-tasks: localization, mapping, and motion planning.
Cross language text classification is an important natural language processing task that exploits a large amount of labeled documents in an auxiliary source language to train a classification model for classifying documents in a target language where labeled data is scarce.
Recent years have seen an explosion in the availability of time series data related to virtually every human endeavor — data that demands to be analyzed and turned into valuable insights.
Graph-structured data appears in many application domains of machine learning, reaching from Social Network Analysis to Computational Biology.
One of the central problems studied in online learning is prediction with expert advice.
Maximization of submodular functions [14] has wide applications in machine learning and artificial intelligence, such as social network analysis [9], sensor placement [10], and recommender systems [7, 2].
In recent years, with the advances in sensorial and information technology, massive amounts of high-dimensional data are available to us.
In the modern digital period, we are facing a rapid growth of available datasets in science and technology.
Optimization of nonconvex functions is known to be computationally intractable in general [11, 12].
The Dirichlet process mixture model (DPMM) is a powerful tool for clustering data that enables the inference of an unbounded number of mixture components, and has been widely studied in the machine learning and statistics communities [1–4].
In recent years of machine learning applications, the size of data has been observed with an unprecedented growth.
The mixture model has been studied extensively from several directions.
In many practical applications of active learning, the cost to acquire a large batch of labels at once is significantly less than the cost of the same number of sequential rounds of individual label requests.
Scaling probabilistic inference algorithms to large datasets and parallel computing architectures is a challenge of great importance and considerable current research interest, and great strides have been made in designing parallelizeable algorithms.
Structured prediction models for action recognition and localization are emerging as prominent alternatives to more traditional holistic bag-of-words (BoW) representations.
Machine learning algorithms rely critically on the features used to represent data; the feature set provides the primary interface through which an algorithm can reason about the data at hand.
Learning from Demonstration (LfD) is a practical framework for learning complex behaviour policies from demonstration trajectories produced by an expert.
Most real-world applications are structured, i.
We consider agents that live for a long time in a sequential decision-making environment.
Learning a boosted classifier from a set of samples S = {X, Y }N ∈ RD × {−1, 1} is usually addressed in the context of two main frameworks.
A neuron’s linear receptive field (RF) is a filter that maps high-dimensional sensory stimuli to a one-dimensional variable underlying the neuron’s spike rate.
In many sequential decision problems, the transition dynamics can change with time.
One-class SVM (OCSVM) [14] is a kernel-based learning algorithm that is often considered to be the method of choice for set estimation in high-dimensional data due to its generalization power, efficiency, and nonparametric nature.
The optimization of non-linear functions whose evaluation may be noisy and expensive is a challenge that has important applications in sciences and engineering.
This work was stimulated by a concrete problem, namely the decomposition of state-of-the-art 2D + time calcium imaging sequences as shown in Fig.
Revealing hidden structures of a graph is the heart of many data analysis problems.
Ontologies and knowledge bases such as WordNet [1], Yago [2] or the Google Knowledge Graph are extremely useful resources for query expansion [3], coreference resolution [4], question answering (Siri), information retrieval or providing structured knowledge to users.
Principal components analysis (PCA) is a popular technique for unsupervised dimension reduction that has a wide range of application—science, engineering, and any place where multivariate data is abundant.
We first introduce the problem of causal inference on iid data, that is in the case with no time structure.
An option is a financial contract that allows the purchase or sale of a given asset, such as a stock, bond, or commodity, for a predetermined price on a predetermined date.
Recent work on scaling deep networks has led to the construction of the largest artificial neural networks to date.
Inference in complex models drives much of the research in machine learning applications, from computer vision, natural language processing, to computational biology.
In this paper we are interested in recovering a complex1 vector x∗ ∈ Cn from magnitudes of its linear measurements.
Dirichlet process mixture models (DPMMs) are widely used in the machine learning community (e.
Principal component regression (PCR) has been widely used in statistics for years (Kendall, 1968).
Various graph-based models, regardless of application, aim to learn a target function on graphs that well respects the graph topology.
We consider a stochastic optimal control problem in discrete time with continuous state and action spaces.
Low contrast stimuli are perceived to move slower than high contrast ones [17].
A major goal of neuroscience is the mapping of neural microcircuits at the scale of hundreds to thousands of neurons [1].
For decades, deep networks with broad hidden layers and full connectivity could not be trained to produce useful results, because of overfitting, slow convergence and other issues.
Functional brain imaging, in particular fMRI, is the workhorse of brain mapping, the systematic study of which areas of the brain are recruited during various experiments.
The problem.
p Consider a p-dimensional probability distribution with true covariance matrix Σ0 ∈ S++ and true p −1 p×n precision (or inverse covariance) matrix Ω0 = Σ0 ∈ S++ .
The last decade, machine learning has seen the rise of neural networks composed of multiple layers, which are often termed deep neural networks (DNN).
Suppose we have completed a placebo-controlled clinical trial of a promising new drug for a neurodegenerative disorder such as Alzheimer’s disease (AD) on a small sized cohort.
Visual tracking, also called object tracking, refers to automatic estimation of the trajectory of an object as it moves around in a video.
Many essential neural computations are implemented by large populations of neurons working in concert, and recent studies have sought both to monitor increasingly large groups of neurons [1, 2] and to characterise their collective behaviour [3, 4].
Inference in large-scale probabilistic models remains a challenge, particularly for modern “big data” problems.
State-space models (SSMs) constitute a popular and general class of models in the context of time series and dynamical systems.
Determinantal Point Process (DPP) [1] is a well-known framework for representing a probability distribution that models diversity.
Discriminatively trained deep convolutional neural networks (CNN) [18] have recently achieved impressive state of the art results over a number of areas, including, in particular, the visual recognition of categories in the ImageNet Large-Scale Visual Recognition Challenge [4].
Tetris is a popular video game created by Alexey Pajitnov in 1985.
Submodularity is a rich combinatorial concept that expresses widely occurring phenomena such as diminishing marginal costs and preferences for grouping.
Markov decision processes (MDPs) provide a general framework for planning and learning under uncertainty.
Gaussian mixture models provide a simple framework for several machine learning problems including clustering, density estimation and classification.
The firing rate of a neuron is arguably the most important characterisation of both neural network dynamics and neural computation, and has been ever since the seminal recordings of Adrian and Zotterman [1] in which the firing rate of a neuron was observed to increase with muscle tension.
The sequential posterior updates play a central role in many Bayesian inference procedures.
People can acquire a new concept from only the barest of experience – just one or a handful of examples in a high-dimensional space of raw perceptual input.
Samples from a determinantal point process (DPP) [15] are sets of points that tend to be spread out.
Many generative models are defined in terms of an unnormalized probability distribution, and computing the probability of a state requires computing the (usually intractable) partition function.
Recent advances in nanoscale devices and biomolecular synthesis have opened up new and exciting possibilities for constructing microscopic systems that can sense and autonomously manipulate the world.
K-armed bandit problems provide an elementary model for exploration-exploitation tradeoffs found at the heart of many online learning problems.
We consider the following generic optimization problem.
Clustering is a major task in machine learning and has been extensively studied over decades of research [11].
Learning dynamic models from observed data has been a central issue in many fields of study, scientific or engineering tasks.
Adaptive decision making algorithms have been used increasingly in the past years, and have attracted researchers from many application areas, like artificial intelligence [16], financial engineering [10], medicine [14] and robotics [15].
Sampling techniques are one of the most widely used approaches to approximate probabilistic reasoning for high-dimensional probability distributions where exact inference is intractable.
Online learning provides a scalable and flexible approach for solving a wide range of prediction problems, including classification, regression, ranking, and portfolio management.
In this paper, we investigate the problem of robust Principal Component Analysis (PCA) in an online fashion.
Although it is often useful for machine learning methods to consider how nature has arrived at a particular solution, it is perhaps more instructive to first understand the functional role of such biological constraints.
Factorized asymptotic Bayesian (FAB) inference is a recently-developed Bayesian approximation inference method for model selection of latent variable models [5, 6].
Privacy-preserving machine learning algorithms are increasingly essential for settings where sensitive and personal data are mined.
Unsupervised feature learning algorithms have recently attracted much attention, with the promise of letting the data guide the discovery of good representations.
Dropout training was introduced by Hinton et al.
Given two samples {xi }ni=1 where xi ∼ P i.
The explosive growth of web videos makes automatic video classification important for online video search and indexing.
Markov Decision Processes (MDPs) have been widely used to model and solve sequential decision making problems under uncertainty, in fields including artificial intelligence, control, finance and management (Puterman, 2009, Barber, 2011).
Submodularity is a pervasive and important property in the areas of combinatorial optimization, economics, operations research, and game theory.
Multilayer perceptrons (MLPs) are general purpose function approximators.
We focus on optimization problems written over the set of permutations.
Learning from prior tasks and transferring that experience to improve future performance is a key aspect of intelligence, and is critical for building lifelong learning agents.
In linear regression, we wish to estimate an unknown but fixed vector of parameters θ0 ∈ Rp from n pairs (Y1 , X1 ), (Y2 , X2 ), .
As the volume of data collected in the social and natural sciences increases, the computational cost of learning from large datasets has become an important consideration.
In many scientific areas, an important methodology that has withstood the test of time is the approximation of “complicated” functions by those that are easier to handle.
For single-task learning, besides global learning methods there are local learning methods [7], e.
Most classic clustering algorithms are designed for the centralized setting, but in recent years data has become distributed over different locations, such as distributed databases [21, 5], images and videos over networks [20], surveillance [11] and sensor networks [4, 12].
Graphical models (GMs) represent the backbone of the generic statistical toolbox for encoding dependence structures in multivariate distributions.
Principal Component Analysis (PCA) [19] is arguably the most widely used method for dimensionality reduction in data analysis.
The structured learning problem is to find a function F (x, y) to map from inputs x to outputs as y ∗ = arg maxy F (x, y).
Digital images are often corrupted with noise during acquisition and transmission, degrading performance in later tasks such as: image recognition and medical diagnosis.
What can one infer about an unknown distribution based on a random sample? If the distribution in question is relatively “simple” in comparison to the sample size—for example if our sample consists of 1000 independent draws from a distribution supported on 100 domain elements—then the empirical distribution given by the sample will likely be an accurate representation of the true distribution.
Humans recognize visually-presented objects rapidly and accurately even under image distortions and variations that make this a computationally challenging problem [27].
Eye movements provide a rich source of knowledge into the human visual information processing and result from the complex interplay between the visual stimulus, prior knowledge of the visual world, and the task.
In this paper, we study the problem of online learning in a class of finite non-stationary episodic Markov decision processes.
We study symbolic dynamic programming (SDP) for Markov Decision Processes (MDPs) with exponentially large factored state and action spaces.
Traditional nonparametric regression such as kernel or k-NN can be expensive to estimate given modern large training data sizes.
We study the problem of discovering the presence of latent variables in data and learning models involving them.
Graphical Models (GMs) provide a useful representation for reasoning in a range of scientific fields [1, 2, 3, 4].
Many machine learning algorithms follow the framework of empirical risk minimization, which often can be cast into the following generic optimization problem n 1∑ (1) gi (w), min G(w) := w∈W n i=1 where n is the number of training examples, gi (w) encodes the loss function related to the ith training example (xi , yi ), and W is a bounded convex domain that is introduced to regularize the solution w ∈ W (i.
The brain is able to integrate noisy and partial information from both sensory inputs and internal states to construct a consistent interpretation of the actual state of the environment.
Work on unsupervised feature selection has received considerable attention.
Learning how to measure similarity (or dissimilarity) is a fundamental problem in machine learning.
It is widely recognized that modern statistical problems are increasingly high-dimensional, i.
There has been much interest in recent years in understanding consistency properties of learning algorithms – particularly algorithms that minimize a surrogate loss – for a variety of ﬁnite-output learning problems, including binary classiﬁcation, multiclass classiﬁcation, multi-label classiﬁcation, subset ranking, and others [1–17].
Principal component analysis (PCA) is a widely-used classical technique for dimensionality reduction.
The estimation of covariance matrices is the basis of many machine learning algorithms and estimation procedures in statistics.
q(x) In this paper we address the problem of estimating the ratio of two functions, p(x) where p is given by a sample and q(x) is either a known function or another probability density function given by a sample.
For many optimization problems, the objective and constraint functions are not adequately modeled by linear or convex functions (e.
Sparsity is an important concept in high-dimensional statistics [1] and signal processing [2] that has led to important application successes by reducing model complexity and improving interpretability of the results.
Motivated by the difficulty in exact specification of reward and transition models, researchers have proposed the uncertain Markov Decision Process (MDP) model and robustness objectives in solving these models.
Gaussian processes (GPs) have found many applications in machine learning and statistics ranging from supervised learning tasks to unsupervised learning and reinforcement learning.
Statistical relational models are capable of representing both probabilistic dependencies and relational structure [1, 2].
Choosing a suitable distance to compare probabilities is a key problem in statistical machine learning.
In this paper we are interested in the Bayesian multi-armed bandit problem which can be described as follows.
Most organisms employ a mutlitude of sensory systems to create an internal representation of their environment.
Recently, no-regret algorithms have received increasing attention in a variety of communities, including theoretical computer science, optimization, and game theory [3, 1].
A deep Boltzmann machine (DBM) [18] is a structured probabilistic model consisting of many layers of random variables, most of which are latent.
High-dimensional statistical models have been the subject of considerable focus over the past decade, both theoretically as well as in practice.
In Gaussian random design model for the linear regression, we seek to reconstruct an unknown coefficient vector θ0 ∈ Rp from a vector of noisy linear measurements y ∈ Rn : y = Xθ0 + w, (1.
In recent years, distributed estimation, learning and prediction has attracted a considerable attention in wide variety of disciplines with applications ranging from sensor networks to social and economic networks [1–6].
Regularization has become an indispensable part of modern machine learning algorithms.
Principal Component Analysis (PCA) is a ubiquitous tool used in many data analysis, machine learning and information retrieval applications.
A tenet of mathematical modeling is to faithfully match the structural properties of the data; yet, on occasion, the available tools are inadequate to perform the task.
An important problem that arises in many applications is that of recovering a high-dimensional sparse (or approximately sparse) vector given a small number of linear measurements.
This paper addresses the problem of learning probability distributions over pairs of input-output sequences, also known as transduction problem.
Many clustering models rely on the minimization of an energy over possible partitions of the data set.
Many linear regression problems are characterized by a large number d of features or explaining attributes and by a reduced number n of training instances.
PAC-Bayesian analysis is a general and powerful tool for data-dependent analysis in machine learning.
The problem of characterizing the statistical properties of a spiking neuron is quite general, but two interesting questions one might ask are: (1) what kind of time dependencies are present? and (2) how much information is the neuron transmitting? With regard to the second question, information theory provides quantifications of the amount of information transmitted by a signal without reference to assumptions about how the information is represented or used.
Markov chain Monte Carlo (MCMC) methods [1] have been dominant tools for posterior analysis in Bayesian inference.
Partially observable Markov decision processes (POMDPs) provide a principled general framework for planning in partially observable stochastic environments.
Matrix Completion is the task to reconstruct low-rank matrices from a subset of its entries and occurs naturally in many practically relevant problems, such as missing feature imputation, multitask learning [2], transductive learning [4], or collaborative filtering and link prediction [1, 8, 9].
Social network analysis is vital to understanding and predicting interactions between network entities [6, 19, 21].
Large-scale machine learning problems are becoming ubiquitous in many areas of science and engineering.
Auto-associative memories have a venerable history in computational neuroscience.
Online display advertising inventory — e.
The recent heightened interest in understanding the brain calls for the development of technologies that will advance our understanding of neuroscience.
Complex dynamical systems can often be observed by monitoring time series of one or more variables.
In many applications, we need to aggregate the preferences of agents over a set of alternatives to produce a joint ranking.
Generative models of text have gained large popularity in analyzing a large collection of documents [3, 4, 17].
Learning prototype from a set of given or observed objects is a core problem in machine learning, and has numerous applications in computer vision, pattern recognition, data mining, bioinformatics, etc.
The Markov chain is a standard model for analyzing the dynamics of stochastic systems, including economic systems [29], traffic systems [11], social systems [12], and ecosystems [6].
Variable selection is a core inferential problem in a multitude of statistical analyses.
In undirected graphical models or Markov random fields, each node represents a random variable while the set of edges specifies the conditional independencies of the underlying distribution.
Natural language processing and information retrieval systems can often benefit from incorporating accurate word similarity information.
We consider the reinforcement learning problem in which one attempts to find an approximately optimal policy for controlling a stochastic nonlinear dynamical system.
A long–term goal of machine learning is to create systems that can be interactively trained or guided by non-expert end-users.
Large, streaming data sets are increasingly the norm in science and technology.
Learning classifiers that generalize well is a hard problem when only few training examples are available.
Natural scene statistics have been used to explain a variety of neural structures.
Multitask learning exploits the relationships between several learning tasks in order to improve performance, which is especially useful if a common subset of features are useful for all tasks at hand.
There is increasing interest in exploring connections between information and estimation theory.
Let us consider a fairly general linear inverse problem, where one wants to estimate a parameter vector z ∈ RD , from a noisy observation y ∈ Rn , such that y = Az + b, where A ∈ Rn×D is sometimes referred to as the observation or design matrix, and b ∈ Rn represents an additive Gaussian noise with a distribution PB ∼ N (0, Σ).
Probabilistic approaches to machine learning involve modeling the probability distributions over large collections of variables.
A probabilistic framework for incorporating features posits latent or hidden variables that can provide a good explanation to the observed data.
In Bayesian models, though conjugate priors normally result in easier inference problems, nonconjugate priors could be more expressive in capturing desired model properties.
The goal of nonnegative matrix factorization (NMF) is to approximate a nonnegative matrix V with the product of two nonnegative matrices, as V ≈ W1 W2 .
The recent rise of crowdsourcing has provided a fast and inexpensive way to collect human knowledge and intelligence, as illustrated by human intelligence marketplaces such as Amazon Mechanical Turk, games with purpose like ESP, reCAPTCHA, and crowd-based forecasting for politics and sports.
A growing body of work on efficient reinforcement learning provides algorithms with guarantees on sample and computational efficiency [13, 6, 2, 22, 4, 9].
The Indian buffet process [IBP, 11] is one of several distributions over matrices with exchangeable rows and infinitely many columns, only a finite (but random) number of which contain any non-zero entries.
People often organise themselves into groups or communities.
Movement Primitives (MPs) are commonly used for representing and learning basic movements in robotics, e.
Bayesian nonparametric methods provide a flexible framework for unsupervised modeling of structured data like text documents, time series, and images.
Multi-task Gaussian process (GP) models are widely used to couple related tasks or functions for joint regression.
Recent literature has advocated the use of randomization as a key algorithmic device with which to dramatically accelerate statistical learning with lp regression or low-rank matrix approximation techniques [12, 6, 8, 10].
Auto-encoders learn an encoder function from input to representation and a decoder function back from representation to input space, such that the reconstruction (composition of encoder and decoder) is good for training examples.
The “missing data” problem arises when values for one or more variables are missing from recorded observations.
The problem faced by the sensory system is to infer the underlying causes of a set of input spike trains.
Designing supervised learning algorithms that can learn from data sets with noisy labels is a problem of great practical importance.
Data with various structures and scales comes from almost every aspect of daily life.
Principal component analysis (PCA) [12] has been widely used to analyze high-dimensional data.
A domain refers to an underlying data distribution.
There has been much work investigating how information about stimulus variables is represented by a population of neurons in the brain [1].
Conditional random fields (CRFs) are a popular class of models that combine the advantages of discriminative modeling and undirected graphical models.
In our daily life, we sense the world through multiple sensory systems.
Deep architectures have strong representational power due to their hierarchical structures.
Sintel MPI KITTI Figure 1: Samples of frames and flows from new flow databases.
Numerous machine learning algorithms require selecting representative subsets of manageable size out of large data sets.
Deep learning has recently been enjoying a resurgence [1, 2] due to the discovery that stage-wise pre-training can significantly improve the results of classical training methods [3–5].
Learning and inference in complex models drives much of the research in machine learning applications ranging from computer vision, natural language processing, to computational biology [1, 18, 21].
The ability to classify instances of an unseen visual class, called zero-shot learning, is useful in several situations.
Dimensionality reduction, as an important form of unsupervised learning, has been widely explored for analyzing complex data such as images, video sequences, text documents, etc.
This paper addresses the problem of focused active inference: selecting a subset of observable random variables that is maximally informative with respect to a specified subset of latent random variables.
In this paper, we study a variant of online learning, called online probing, which is motivated by practical problems where there is a cost to observing the features that may help one’s predictions.
Non-cooperative game theory is a formal mathematical framework for describing behavior of interacting self-interested agents.
In pool-based active learning [1], we select training data from a finite set (called a pool) of unlabeled examples and aim to obtain good performance on the set by asking for as few labels as possible.
Statistical models of neural spike recordings have greatly facilitated the study of both intra-neuron spiking behavior and the interaction between populations of neurons.
The phrase “one-shot learning” has been used to describe our ability – as humans – to correctly recognize and understand objects (e.
Image blur is an undesirable degradation that often accompanies the image formation process and may arise, for example, because of camera shake during acquisition.
Prediction with expert advice —see, e.
Let {y1 , y2 , .
Over the past years, multi-armed bandit (MAB) algorithms have been employed in an increasing amount of large-scale applications.
Learning problems with binary labels, where one is given training examples consisting of objects with binary labels (such as emails labeled spam/non-spam or documents labeled relevant/irrelevant), are widespread in machine learning.
The subspace learning problem is that of finding the smallest linear space supporting data drawn from an unknown distribution.
The proper setting of high-level hyperparameters in machine learning algorithms – regularization weights, learning rates, etc.
Effective models in complex computer vision and natural language problems try to strike a favorable balance between accuracy and speed of prediction.
Markov decision processes (MDPs) [Puterman, 1994] have been widely used to model and solve sequential decision problems in stochastic environments.
In recent years, the music industry has shifted more and more towards digital distribution through online music stores and streaming services such as iTunes, Spotify, Grooveshark and Google Play.
Mapping a problem involving discrete variables into continuous variables often results in a more tractable formulation.
The usual optimization criteria for an infinite horizon Markov decision process (MDP) are the expected sum of discounted rewards and the average reward.
An outlier, which is “an observation which deviates so much from other observations as to arouse suspicions that it was generated by a different mechanism” (by Hawkins [10]), appears in many reallife situations.
Computing the stationary distribution of a Markov chain (MC) with a very large state space (finite, or countably infinite) has become central to statistical inference.
Suppose a set of k centers {pi }ki=1 is selected by approximate minimization of k-means cost; how does the fit over the sample compare with the fit over the distribution? Concretely: given m points sampled from a source distribution ρ, what can be said about the quantities   Z m 1 X    2 2 min kxj − pi k2 − min kx − pi k2 dρ(x)  i i m  j=1   ! ! Z m k k 1 X  X X   ln αi pθi (xj ) − ln αi pθi (x) dρ(x)  m  j=1 i=1 i=1 (k-means), (1.
An important instance of the framework of prediction with expert advice —see, e.
We study the precision matrix estimation problem: let X = (X1 , .
Matrix completion concerns the problem of recovering a low-rank matrix from a limited number of observed entries.
In this paper, we study the problem of estimating the cluster tree of a density when the density is supported on or near a manifold.
Many tasks of recommender systems can be formulated as recovering an unknown tensor (multiway array) from a few observations of its entries [17, 26, 25, 21].
Majorization-minimization [15] is a simple optimization principle for minimizing an objective function.
In the matrix completion problem we observe a subset of the entries of a target matrix Y , and our aim is to retrieve the rest of the matrix.
Consider the simple task of learning a threshold classifier in 1D (Figure 1).
Recently, the machine learning and signal processing communities have focused considerable attention toward understanding the benefits of adaptive sensing.
Principal component analysis (PCA) is a popular form of dimensionality reduction that projects a data set on the top eigenvector(s) of its covariance matrix.
Contemporary statistical procedures are making inroads into a diverse range of applications in the natural sciences and engineering.
Active learning algorithms seek to mitigate the cost of learning by using unlabeled data and sequentially selecting examples to query for their label to minimize total number of queries.
Mobile manipulator robots have arms with high degrees of freedom (DoF), enabling them to perform household chores (e.
Neural signals from electrodes implanted in cortex [1], electrocorticography (ECoG) [2], and electroencephalography (EEG) [3] all have been used to decode motor intentions and control motor prostheses.
A host of machine-learning problems can be solved effectively as approximations of such NP-hard combinatorial problems as set cover, set packing, and multiway-cuts [8, 11, 16, 22].
The movement between the specification of “local” marginals and models for complete joint distributions is ingrained in the language and methods of modern probabilistic inference.
Decomposition of tensors [10, 14] (or multi-way arrays) into low-rank components arises naturally in many real world data analysis problems.
The ability of a website to present personalized content recommendations is playing an increasingly crucial role in achieving user satisfaction.
The standard approach to learning models from data assumes that the data were generated by a certain model, and the goal of learning is to recover this generative model.
Most classic machine learning methods depend on the assumption that humans can annotate all the data available for training.
What makes a teacher effective? A critical factor is their instructional policy, which specifies the manner and content of instruction.
In recent years there has been increasing interest in probabilistic models where the latent variables or parameters of interest are discrete probability distributions over K items, i.
Real-world data are often presented as a graph where the nodes in the graph bear labels that vary smoothly along edges.
The desire to apply machine learning to increasingly larger datasets has pushed the machine learning community to address the challenges of distributed algorithm design: partitioning and coordinating computation across the processing resources.
Statistical analysis of social networks and other relational data is becoming an increasingly important problem as the scope and availability of network data increases.
How humans achieve long-term goals in an uncertain environment, via repeated trials and noisy observations, is an important problem in cognitive science.
Many machine learning problems can be interpreted as matching two objects, e.
When a trader enters a market, say a stock or commodity market, with the desire to buy or sell a certain quantity of an asset, how is this trader guaranteed to find a counterparty to agree to transact at a reasonable price? This is not a problem in a liquid market, with a deep pool of traders ready to buy or sell at any time, but in a thin market the lack of counterparties can be troublesome.
Object detection and segmentation approaches often assume that the training and test samples are drawn from the same distribution.
Although both stochastic optimization [17, 4, 18, 10, 26, 20, 22] and multiple objective optimization [9] are well studied subjects in Operational Research and Machine Learning [11, 12, 24], much less is developed for stochastic multiple objective optimization, which is the focus of this work.
A wide range of statistical models have been proposed for the discovery of hidden communities within observed networks.
The focus of this paper is energy minimization for Markov random fields.
Hippocampus, olfactory cortex, and other brain regions are thought to operate as associative memories [1,2], having the ability to learn patterns from presented inputs, store a large number of patterns, and retrieve them reliably in the face of noisy or corrupted queries [3–5].
Although sensory stimuli are high-dimensional, sensory neurons are typically sensitive to only a small number of stimulus features.
Encoding high-dimensional objects using short binary hashes can be useful for fast approximate similarity computations and nearest neighbor searches.
Statistical learning theory, especially the theory of “probably approximately correct” (PAC) learning, has mostly developed under the assumption that data are independent and identically distributed (IID) samples from a fixed, though perhaps adversarially-chosen, distribution.
Measuring statistical dependence between random variables is a fundamental problem in statistics.
Principal component analysis is a fundamental tool for dimensionality reduction, clustering, classification, and many more learning tasks.
Recovering scene properties (shape, illumination, reflectance) that led to the generation of an image has been one of the fundamental problems in computer vision.
Perceptrons are paradigmatic building blocks of neural networks [1].
Direct policy search methods have the potential to scale gracefully to complex, high-dimensional control tasks [12].
Policy gradient methods have established as the most effective reinforcement–learning techniques in robotic applications.
Motivation.
Our lives are embedded in networks–social, biological, communication, etc.
Undirected graphical models, or Markov random fields (MRFs), are a popular class of statistical models for representing distributions over a large number of variables.
Probabilistic generative models are used to mathematically formulate the generation process of observed data.
Low-rank matrix factorization techniques like the singular value decomposition (SVD) constitute an important tool in data analysis yielding a compact representation of data points as linear combinations of a comparatively small number of ’basis elements’ commonly referred to as factors, components or latent variables.
For humans and other primates, action recognition is an important ability that facilitates social interaction, as well as recognition of threats and intentions.
Rapid growth in the size and scale of datasets has fueled increasing interest in statistical estimation in distributed settings [see, e.
Large-scale classification of textual and visual data into a large number of target classes has been the focus of several studies, from researchers and developers in industry and academia alike.
Clustering aims to summarize observed data by grouping its elements according to their similarities.
An important aspect of deciphering the neural code is to determine those stimulus features populations of sensory neurons are most sensitive to.
Undirected graphical models, a.
Visual clutter, defined colloquially as a “confused collection” or a “crowded disorderly state”, is a dimension of image understanding that has implications for applications ranging from visualization and interface design to marketing and image aesthetics.
Distributed representations of words in a vector space help learning algorithms to achieve better performance in natural language processing tasks by grouping similar words.
Graphical models are used in a wide variety of domains, both to provide compact representations of probability distributions for rapid, efficient inference, and also to represent complex causal structures.
Online marketplaces such as Walmart, Netflix, and Amazon store information about their customers and the products they purchase in binary matrices.
Bayesian inference is computationally expensive.
Detecting anomalous activity refers to determining if we are observing merely noise (business as usual) or if there is some signal in the noise (anomalous activity).
Computer vision has historically been formulated as the problem of producing symbolic descriptions of scenes from input images [10].
Probabilistic logical modes (PLMs) combine elements of first-order logic with graphical models to succinctly model complex, uncertain, structured domains [5].
In the past several years, significant strides have been made in scaling up plan synthesis techniques.
Graph-based learning is by now well established in machine learning and is the standard way to deal with data that encode pairwise relationships.
Progress in neural recording technology has made it possible to record spikes from ever larger populations of neurons [1].
Two puzzles present themselves to language users: What do words mean in general, and what do they mean in context? Consider the utterances “it’s raining,” “I ate some of the cookies,” or “can you close the window?” In each, a listener must go beyond the literal meaning of the words to fill in contextual details (“it’s raining here and now”), infer that a stronger alternative is not true (“I ate some but not all of the cookies”), or more generally infer the speaker’s communicative goal (“I want you to close the window right now because I’m cold”), a process known as pragmatic reasoning.
High-dimensional representations have become very popular in modern applications of machine learning, computer vision, and information retrieval.
Stochastic grammars are traditionally used to represent natural language syntax and semantics, but they have also been extended to model other types of data like images [1, 2, 3] and events [4, 5, 6, 7].
We consider a discrete-time dynamic system whose state transition depends on a control.
This paper looks at the information leaked by online learning algorithms, and seeks to design accurate learning algorithms with rigorous privacy guarantees – that is, algorithms that provably leak very little about individual inputs.
The original motivation for providing privacy in statistical problems, first discussed by Warner [23], was that “for reasons of modesty, fear of being thought bigoted, or merely a reluctance to confide secrets to strangers,” respondents to surveys might prefer to be able to answer certain questions non-truthfully, or at least without the interviewer knowing their true response.
One core operation in computer vision involves evaluating a bank of templates at a set of sample locations in an image.
The problem of nonparametric testing of interaction between variables has been widely treated in the machine learning and statistics literature.
The comparison problem asks which of a number of objects has a higher value on an unobserved criterion.
Random utility models (RUM), which presume agent utility to be composed of a deterministic component and a stochastic unobserved error component, are frequently used to model choices by individuals over alternatives.
Both artificial and natural sensing systems face the challenge of making sense out of a continuous stream of noisy sensory inputs.
Gaussian Process (GP) learning and inference are computationally prohibitive with large datasets, having time complexities O(n3 ) and O(n2 ), where n is the number of training points.
Minimax analysis has recently been shown to be a powerful tool for the construction of online learning algorithms [Rakhlin et al.
Finding the correct bijection between two sets of objects X = {x1 , x2 , .
The problem of finding a low–rank matrix that (approximately) satisfies a given set of conditions has recently generated a lot of interest in many communities.
Despite a long history of prior work, human body pose estimation, or specifically the localization of human joints in monocular RGB images, remains a very challenging task in computer vision.
Several methods have been proposed to solve the image denoising problem including anisotropic diffusion [15], frequency-based methods [26], Bayesian and Markov Random Fields methods [20], locally adaptive kernel-based methods [17] and sparse representation [10].
In this paper, we consider the minimization of block-seperable convex functions subject to linear constraints, with a canonical form: J J X X min f (x) = fj (xj ) , s.
Neuroscience has made significant progress in learning how activity in specific neurons or brain areas correlates with behavior.
Matching two potentially heterogenous language objects is central to many natural language applications [28, 2].
Recommendation systems have become ubiquitous in our lives, helping us filter the vast expanse of information we encounter into small selections tailored to our personal tastes.
Modern statistical estimation is routinely faced with real world problems where the number of parameters p handily outnumbers the number of observations n.
Kernel machines have become widely used in many machine learning problems, including classification, regression, and clustering.
A permutation-valued function, also called a ranking function, outputs a ranking over a set of objects given features corresponding to the objects, and learning such ranking functions given data is becoming an increasingly key machine learning task.
Deep connectionist architectures involve many layers of nonlinear information processing [1].
1.
Stimulus information is encoded in neuronal responses.
In this paper, we deal with binary prediction in metric spaces.
Recognition of human actions in videos is a challenging task which has received a significant amount of attention in the research community [11, 14, 17, 26].
Covariance matrices are a key ingredient in many algorithms in signal processing, machine learning and statistics.
We consider problems where the goal is to detect outstanding events or extreme values in domains such as outlier detection [1], security [18], or medicine [17].
Data in scientific and commercial disciplines are increasingly characterized by high dimensions and relatively few samples.
It has long been recognised that the firing properties of cortical neurons are not constant over time, but that neural systems can exhibit multiple distinct firing regimes.
Many object recognition schemes, inspired from biological vision, are based on feed-forward hierarchical architectures, e.
Canonical Correlation Analysis (CCA) is a widely used spectrum method for finding correlation structures in multi-view datasets introduced by [15].
A triumph of machine learning is the ability to predict many human aspects: is certain mail spam or not, is a news-item of interest or not, does a movie meet one’s taste or not, and so on.
To cope with the rich variety of transformations in natural images, recognition systems require a representative sample of possible variations.
Learning in animals involves the active gathering of sensor data, presumably selecting those sensor inputs that are most useful for learning a model of the world.
Many learning problems can be formulated in terms of inference on predictive stochastic models.
Conventional methods for real-time abstract planning over options in reinforcement learning require a single pre-specified reward function, and these methods are not efficient in settings with multiple reward functions that can be specified at any time.
Nonlinear inversion problems, where we wish to infer the latent inputs to a system given observations of its output and the system’s forward-model, have a long history in the natural sciences, dynamical modeling and estimation.
On-line learning has received much attention in recent years.
Given a design matrix X 2 Rn⇥d and a response matrix Y 2 Rn⇥m , we consider a multivariate linear model Y = XB0 + Z, where B0 2 Rd⇥m is an unknown regression coefficient matrix and Z 2 Rn⇥m is a noise matrix [1].
Neural network-based architectures have recently had great success in significantly advancing the state of the art on challenging image classification and object detection datasets [8, 12, 19].
Besides accuracy and sample efficiency, computational cost is a crucial design criterion for machine learning algorithms in real-time settings, such as control problems.
In the spiked covariance model proposed by [JL04], we are given data x1 , x2 , .
Many application domains of machine learning use massive data sets in dense medium-dimensional or sparse high-dimensional spaces.
One of the most significant recent developments in machine learning has been the resurgence of “deep learning”, usually in the form of artificial neural networks.
A boosting algorithm can be seen as a meta-algorithm that maintains a distribution over the sample space.
Consider a linear inverse problem of the form: y = Ax + e, N ×D (1) N where A ∈ R is the measurement matrix, y ∈ R is the measurement vector, x ∈ RD is the desired solution and e ∈ RN is a vector of corruptive noise.
The general perception is that kernel methods are not scalable.
With the advent of online crowdsourcing services such as Amazon Mechanical Turk, crowdsourcing has become an appealing way to collect labels for large-scale data.
We consider the reinforcement learning (RL) problem of optimizing rewards in an unknown Markov decision process (MDP) [1].
The generic viewpoint assumption (GVA) [5, 9, 21, 22] postulates that what we see in the world is not seen from a special viewpoint, or lighting condition.
Consider the following fundamental statistical task: Given independent draws from an unknown probability distribution, what is the minimum sample size needed to obtain an accurate estimate of the distribution? This is the question of density estimation, a classical problem in statistics with a rich history and an extensive literature (see e.
In the era of big data, a natural idea is to select a small subset of m samples Ce = {xe1 , .
A number of real world applications model data as being sampled from a union of independent subspaces.
Judging a person as a friend or foe, a mushroom as edible or poisonous, or a sound as an \l\ or \r\ are examples of categorization tasks.
Graphical models in computer vision Optimization of undirected graphical models such as Markov Random Fields, MRF, or Conditional Random Fields, CRF, is of fundamental importance in computer vision.
Machine learning has recently experienced a proliferation of problem settings that, to some extent, enrich the classical dichotomy between supervised and unsupervised learning.
A detailed understanding of brain function is a still-elusive grand challenge.
Principal Component Analysis (PCA) aims at recovering the top k leading eigenvectors u1 , .
Musical rhythm occurs in all human societies and is related to many phenomena, such as the perception of a regular emphasis (i.
Determining the topology of macro-scale functional networks in the brain and micro-scale neural networks has important applications to disease diagnosis and is an important step in understanding brain function in general [11, 19].
Kernel methods and methods based on integral operators have become one of the central areas of machine learning and learning theory.
One typical vision problem usually comprises several subproblems, which tend to be tackled jointly to achieve superior capability.
Given a data matrix X, Principal Component Analysis (PCA) can be regarded as a ‘denoising’ technique that replaces X by its closest rank-one approximation.
Graphical models have had tremendous impact in a variety of application domains.
Imagine independently consulting a small set of medical experts for the purpose of reaching a binary decision (e.
Many important problems including sensor placement [3], image co-segmentation [4], MAP inference for determinantal point processes [5], influence maximization in social networks [6], and document summarization [7] may be expressed as the maximization of a submodular function.
The kernel mean or the mean element, which corresponds to the mean of the kernel function in a reproducing kernel Hilbert space (RKHS) computed w.
Markov Logic Networks (MLNs) [5] are powerful template models that define Markov networks by instantiating first-order formulas with objects from its domain.
Exact inference in Markov Random Fields (MRFs) is generally intractable, motivating approximate algorithms.
Reinforcement learning (RL) and approximate dynamic programming (ADP) [24, 2] are effective approaches to solve the problem of decision-making under uncertainty.
Learning sparse polynomials over the Boolean domain is one of the fundamental problems from computational learning theory and has been studied extensively over the last twenty-five years [1– 6].
Matrix factorization methods currently enjoy a large popularity in machine learning and signal processing.
The stochastic multi-armed bandit problem (MAB) [16] offers a simple formalization for the study of sequential design of experiments.
The focus of this paper is on the problem of Maximum Inner Product Search (MIPS).
In [1], Nesterov introduced a primal-dual technique, called the excessive gap, for constructing and analyzing first-order methods for nonsmooth and unconstrained convex optimization problems.
The emergence of social graphs of the World Wide Web has had a considerable effect on propagation of ideas or information.
Graphical models provide a mechanism for expressing the relationships among a collection of variables.
With the advent of massively open online courses (MOOCs) and online learning platforms such as Khan Academy and Reasoning Mind, large volumes of data are collected from students as they solve exercises, acquire cognitive skills, and achieve a conceptual understanding.
Online social platforms routinely track and record a large volume of event data, which may correspond to the usage of a service (e.
The accuracy of Natural Language Processing (NLP) tools for a given language depend heavily on the availability of annotated resources in that language.
Marginal inference and estimating the partition function for undirected graphical models, also called Markov random fields (MRFs), are fundamental problems in machine learning.
Spectral decomposition of large-scale graphs is one of the most informative and fundamental matrix approximations.
Classical statistical theory studies the rate at which the error in an estimation problem decreases as the sample size increases.
We would like to live in a world where we can define a probabilistic model, press a button, and get accurate inference results within a matter of seconds or minutes.
The last decade has witnessed a tremendous growth in the amount of data involved in machine learning tasks.
1.
Time series of financial returns often exhibit heteroscedasticity, that is the standard deviation or volatility of the returns is time-dependent.
While value/utility is a useful abstraction for macroeconomic applications, it has little psychological validity [1].
We are interested in general strategies for sequential prediction and decision making (a.
Background and motivation.
Bipartite ranking aims to learn a real-valued ranking function that places positive instances above negative instances.
State aggregation is one of the simplest approximate methods for reinforcement learning with very large state spaces; it is a special case of linear value function approximation with binary features.
Faces of the same identity could look much different when presented in different poses, illuminations, expressions, ages, and occlusions.
In many machine learning applications, different datasets may reside on different but highly correlated manifolds.
Finding an hyperplane that minimizes the number of misclassifications is N P-hard.
Many domains today—vision, speech, biology, and others—are flush with data.
The multi-armed bandit problem is a reinforcement learning problem with K actions.
We consider Partial Monitoring, a repeated game where in every time step a learner chooses an action while, simultaneously, an opponent chooses an outcome.
Object recognition is a central problem in vision.
Since 2006 there has been a boost in machine learning due to improvements in the field of unsupervised learning of representations.
This paper concerns two algorithms which until now have remained somewhat disjoint in the literature: the randomized Kaczmarz algorithm for solving linear systems and the stochastic gradient descent (SGD) method for optimizing a convex objective using unbiased gradient estimates.
Many real-world Reinforcement Learning (RL) problems combine the challenges of closed-loop action (or policy) selection with the already significant challenges of high-dimensional perception (shared with many Supervised Learning problems).
Recent work in statistics has focused on high-dimensional inference problems where the number of parameters p equals or exceeds the number of samples n.
We study the problem of ranking a set of n items given pairwise comparisons between these items.
Active learning has been the subject of significant theoretical and experimental study in machine learning, due to its potential to greatly reduce the amount of labeling effort needed to learn a given target function.
Tensor data analysis have witnessed increasing applications in machine learning, data mining and computer vision.
Suppose we are given a linear subspace S of a high-dimensional space Rp , which contains a sparse vector x0 6= 0.
Graphical models are a powerful framework for succinct representation of complex highdimensional distributions.
In recent years, the alternating direction method of multipliers (ADMM) [4] has been successfully used in a broad spectrum of applications, ranging from image processing [11, 14] to applied statistics and machine learning [26, 25, 12].
Social choice studies the design and evaluation of voting rules (or rank aggregation rules).
Traditional building blocks for deep learning have some unsatisfactory properties.
Recently, Multilayer1 Neural Networks (MNNs) with deep architecture have achieved state-of-theart performance in various supervised learning tasks [11, 14, 8].
The problem of aligning temporal sequences is ubiquitous in applications ranging from bioinformatics [5, 1, 23] to audio processing [4, 6].
Consider the following optimization problem min x h(x) , f (x1 , .
The point process generalized linear model (GLM) has provided a useful and highly tractable tool for characterizing neural encoding in a variety of sensory, cognitive, and motor brain areas [1–5].
Learning Bayesian network parameters is the problem of estimating the parameters of a known structure given a dataset.
In numerous learning problems the decision maker is provided with vast amounts of different types of information which it can utilize to learn how to select actions that lead to high rewards.
People like to look at examples.
Graphical models are a very useful tool to capture the dependencies between the variables of interest.
As data sets and problems are ever increasing in size, accelerating first-order methods is both of practical and theoretical interest.
Logistic regression (LR) is a standard probabilistic statistical classification model that has been extensively used across disciplines such as computer vision, marketing, social sciences, to name a few.
Statistical tests based on distribution embeddings into reproducing kernel Hilbert spaces have been applied in many contexts, including two sample testing [18, 15, 32], tests of independence [17, 33, 4], tests of conditional independence [14, 33], and tests for higher order (Lancaster) interactions [24].
We denote by x1 , .
Our ability to effortlessly extrapolate patterns is a hallmark of intelligent systems: even with large missing regions in our field of view, we can see patterns and textures, and we can visualise in our mind how they generalise across space.
In many statistical estimation problems, observations can be modeled as noisy quadratic functions of an unknown vector v0 = (v0,1 , v0,2 , .
With the immense growth of available data, developing distributed algorithms for machine learning is increasingly important, and yet remains a challenging topic both theoretically and in practice.
Clustering is an important problem in unsupervised learning that deals with grouping observations (data points) appropriately based on their similarities or distances [20].
From the considerable amount of recent research on high-dimensional statistical estimation, it has now become well understood that it is vital to impose structural constraints upon the statistical model parameters for their statistically consistent estimation.
Bayesian networks are graphical models widely used to represent joint probability distributions on complex multivariate domains.
The motivation for this paper started with a question: Why are the number of samples needed for Reinforcement Learning (RL) in practice so much smaller than those given by theory? Can we improve this? In Markov Decision Processes (MDPs, Puterman (1994)), when the performance is measured by (1) the sample complexity (Kearns and Singh, 2002; Kakade, 2003; Strehl and Littman, 2008; Szita and Szepesvári, 2010) or (2) the regret (Bartlett and Tewari, 2009; Jaksch, 2010; Ortner, 2012), algorithms have been developed that achieve provably near-optimal performance.
Both classification and detection are key visual recognition challenges, though historically very different architectures have been deployed for each.
Devising ensembles of base predictors is a standard approach in machine learning which often helps improve performance in practice.
Visual recognition research has achieved major successes in recent years using large datasets and discriminative learning algorithms.
The efficient harnessing of renewable energy has become paramount in an era characterized by decreasing natural resources and increasing pollution.
Matrix completion has attracted a lot of contributions over the past decade.
A range of machine learning problems such as link prediction in graphs containing community structure [16], phase retrieval [5], subspace clustering [18] or dictionary learning [12] amount to solve sparse matrix factorization problems, i.
During the past few years, hashing has become a popular tool for tackling a variety of large-scale computer vision and machine learning problems including object detection [6], object recognition [35], image retrieval [22], linear classiﬁer training [19], active learning [24], kernel matrix approximation [34], multi-task learning [36], etc.
Let p∗ (x) be a probability density on Rd corresponding to a random vector X = (X1 , .
Several problems in science and engineering require estimating a real-valued, non-linear (and often non-convex) function f defined on a compact subset of Rd in high dimensions.
Synaptic plasticity is believed to be the fundamental building block of learning and memory in the brain.
Big Data challenge modern data analysis in terms of large dimension, insufficient sample and the inhomogeneity.
Binary classification performance is often measured using metrics designed to address the shortcomings of classification accuracy.
Pairwise clustering methods partition the data into a set of self-similar clusters based on the pairwise similarity between the data points.
Computing the dominant singular vectors of a matrix is one of the most important algorithmic tasks underlying many applications including low-rank approximation, PCA, spectral clustering, dimensionality reduction, matrix completion and topic modeling.
Conditional random fields [24] are used to model structure in numerous problem domains, including natural language processing (NLP), computational biology, and computer vision.
Among the many probabilistic models over permutations, models based on penalizing inversions with respect to a reference permutation have proved particularly elegant, intuitive, and useful.
There has been an increasing interest in generative models for unsupervised learning, with many applications in Image processing [1, 2], natural language processing [3, 4], vision [5] and audio [6].
A substantial body of work has examined the optimality of neural population codes [1–19].
One of the most detailed and widely accepted models of the neuron is the Hodgkin Huxley (HH) model [1].
The hidden Markov model (HMM) [1] is one of the most widely and successfully applied statistical models for the description of discrete time series data.
Different types of multiple data modalities can be used to describe the same event.
Inferring the distributions of latent variables is a key tool in statistical modeling.
In the big data era, many applications require solving optimization problems with billions of variables on a huge amount of training data.
Consider the model yi = µ(xi ) + i , i ∼ N (0, σ 2 I), (1) p where µ(x) is an arbitrary function, and xi ∈ R .
Nowadays our data are often high-dimensional, massive and full of gross errors (e.
Action recognition in real-world videos has many potential applications in multimedia retrieval, video surveillance and human computer interaction.
Undirected graphical models are a familiar framework in diverse application domains such as computer vision, statistical physics, coding theory, social science, and epidemiology.
Relational and graph-structured data has become ubiquitous in many fields of application such as social network analysis, bioinformatics, and artificial intelligence.
Signals associated with nodes or edges of a graph arise in a number of applications including sensor network intrusion, disease outbreak detection and virus detection in communication networks.
There are numerous applications of higher-order tensors in machine learning [22, 29], signal processing [10, 9], computer vision [16, 17], data mining [1, 2], and numerical linear algebra [14, 21].
Probabilistic modeling of ranking data is an extensively studied problem with a rich body of past work [1, 2, 3, 4, 5, 6, 7, 8, 9].
This paper establishes the asymptotic normality of a nonparametric estimator of the f -divergence between two distributions from a finite number of samples.
Graphical models provide compact representations of multivariate distributions using graphs that represent Markov conditional independencies in the distribution.
Modern deep neural networks exhibit a curious phenomenon: when trained on images, they all tend to learn first-layer features that resemble either Gabor filters or color blobs.
Coverage functions are a special class of the more general submodular functions which play important role in combinatorial optimization with many interesting applications in social network analysis [1], machine learning [2], economics and algorithmic game theory [3], etc.
Clustering is a fundamental form of data analysis that is applied in a wide variety of domains, from astronomy to zoology.
The US Coast Guard, the Federal Air Marshal Service, the Los Angeles Airport Police, and other major security agencies are currently using game-theoretic algorithms, developed in the last decade, to deploy their resources on a regular basis [13].
Bayesian approaches to machine learning problems inevitably call for the frequent approximation of computationally intractable integrals of the form Z Z = h`i = `(x) π(x) dx, (1) where both the likelihood `(x) and prior π(x) are non-negative.
A fundamental problem in sequential decision making is controlling an agent when the environmental dynamics are only partially known.
The multivariate Gaussian (Normal) distribution is ubiquitous in statistical applications in machine learning, signal processing, computational biology, and others.
Advancements in sensory technologies and digital storage media have led to a prevalence of “Big Data” collections that have inspired an avalanche of recent efforts on “scalable” machine learning (ML).
Most tasks in natural language processing and understanding involve looking at words, and could benefit from word representations that do not treat individual words as unique symbols, but instead reflect similarities and dissimilarities between them.
Event time data is often modeled as an inhomogeneous Poisson process, whose rate λ(t) as a function of time t has to be learned from the data.
Many important quantities in machine learning and statistics can be viewed as integral functionals of one of more continuous probability densities; that is, quanitities of the form Z F (p1 , · · · , pk ) = f (p1 (x1 ), .
Articulated pose estimation is one of the fundamental challenges in computer vision.
In this paper we study the problem of graph transduction on a simple, undirected graph G = (V, E), with vertex set V = [n] and edge set E ⊆ V ×V .
State-space models (SSMs) are a widely used class of models that have found success in applications as diverse as robotics, ecology, finance and neuroscience (see, e.
Sparse-Group Lasso (SGL) [5, 16] is a powerful regression technique in identifying important groups and features simultaneously.
In recent years there has been a great deal of interest in the problem of learning a low rank matrix from a set of linear measurements.
A growing fraction of Internet advertising is sold through automated real-time ad exchanges.
In recent years object detectors have undergone an impressive transformation [11, 32, 14].
The Dantzig Selector (DS) [3, 5] provides an alternative to regularized regression approaches such as Lasso [19, 22] for sparse estimation.
Memories are thought to be encoded in the joint, persistent activity of groups of neurons.
Since it was raised in 2009, Curriculum Learning (CL) [1] has been attracting increasing attention in the field of machine learning and computer vision [2].
In realistic industrial machine learning applications the datasets range from 1TB to 1PB.
In many classification problems, the input is represented as a set of features.
Despite being introduced over a decade ago, random forests remain one of the most popular machine learning tools due in part to their accuracy, scalability, and robustness in real-world classification tasks [3].
Multitask learning (MTL) captures and exploits the relationship among multiple related tasks and has been empirically and theoretically shown to be more effective than learning each task independently.
We consider a general class of online decision-making problems, where a learner sequentially decides which actions to take from a given decision set and suffers some loss associated with the decision and the state of the environment.
Information retrieval systems require us to rank a set of samples according to their relevance to a query.
Gaussian processes have been shown to be flexible models that are able to capture complicated structure, without succumbing to over-fitting.
Learning to rank is a problem of ordering a set of items according to their relevances to a given context [8].
Stochastic optimization techniques have been extensively employed for online machine learning on data which is uncertain, noisy or missing.
Maximum a posteriori (MAP) inference in Markov random fields (MRFs) is an important problem with abundant applications in computer vision [1], computational biology [2], natural language processing [3], and others.
Modern learning applications frequently require a level of fine-grained control over prediction performance that is not offered by traditional “per-point” performance measures such as hinge loss.
There is significant value in the ability to associate natural language descriptions with images.
There are many learning problems where classifiers must make accurate decisions quickly.
Recent years have unveiled central contact points between the areas of statistical and online learning.
Symmetric Positive Definite (SPD) matrices, in particular covariance matrices, have been playing an increasingly important role in many areas of machine learning, statistics, and computer vision, with applications ranging from kernel learning [12], brain imaging [9], to object detection [24, 23].
Distributed word representations have enjoyed success in several NLP tasks [1, 2].
Prior knowledge is a crucial component of any learning system as without a form of prior knowledge learning is provably impossible [1].
Probabilistic Graphical Models (PGMs) provide a principled approach to approximate constraint optimization for NP-hard problems.
Background.
Large-scale clustering of data points in metric spaces is an important problem in mining big data sets.
Pairwise classification aims to determine if two examples belong to the same class.
Many planning problems from diverse areas such as urban planning, social networks, and transportation can be cast as stochastic network design, where the goal is to take actions to enhance connectivity in a network with some stochastic element [1–8].
In many applications, one obtains measurements (xi , yi ) for which the response y is related to x via some mixture of known kernel functions fθ (x), and the goal is to recover the mixture parameters θk and their associated weights: yi = K X wk fθk (x) + i (1) k=1 where fθ (x) is a known kernel function parameterized by θ, and θ = (θ1 , .
There are multiple scenarios where we might wish to reconstruct a symmetric positive semidefinite (SPSD) matrix from a sampling of its entries.
Many domains in AI and machine learning (e.
Probabilistic graphical models have been extensively studied as a powerful tool for modeling a set of conditional independencies in a probability distribution [12].
We prove finite sample bounds for k-nearest neighbor (k-NN) density estimation, and subsequently apply these bounds to the related problem of mode estimation.
Spatio-temporal data provide unique information regarding “where” and “when”, which is essential to answer many important questions in scientific studies from geology, climatology to sociology.
The number of photographs being uploaded online is growing at an unprecedented rate.
Graph-based techniques for clustering have become very popular in machine learning as they allow for an easy integration of pairwise relationships in data.
Since data is often partitioned across multiple servers [20, 7, 18], there is an increased interest in computing on it in the distributed model.
Hierarchical Dirichlet Process (HDP) mixture models were first introduced by Teh et al.
Let us consider the problem of learning a classifier from positive and unlabeled data (PU classification), which is aimed at assigning labels to the unlabeled dataset [1].
Factored multi-agent MDPs [4] offer a powerful mathematical framework for studying multi-agent sequential decision problems in the presence of uncertainty.
It is often the case that an observed signal is a linear combination of some other target signals that one wishes to resolve from each other and from background noise.
Stochastic Gradient Descent (SGD) algorithms are gaining more and more importance in the Machine Learning community as efficient and scalable machine learning tools.
Undirected graphical models, also known as Markov random fields (MRFs), are a powerful class of statistical models, that represent distributions over a large number of variables using graphs, and where the structure of the graph encodes Markov conditional independence assumptions among the variables.
One of the most fundamental challenges in neuroscience is the “large-scale integration problem”: how does distributed neural activity lead to precise, unified cognitive moments [1].
Structure-from-motion is the ability to perceive the 3D shape of objects solely from motion cues.
Choice is a fundamental behavior of humans and has been studied extensively in Artificial Intelligence and related areas.
Dropout training [1] is an increasingly popular method for regularizing learning algorithms.
Sequential Monte Carlo (SMC) inference techniques require blocking barrier synchronizations at resampling steps which limit parallel throughput and are costly in terms of memory.
Topic models [1] assume that each document in a text corpus is generated from an ad-mixture of topics, where, each topic is a distribution over words in a Vocabulary.
Many brain circuits are known to maintain information over short periods of time in the firing of their neurons [15].
Markov random fields are widely used as priors for solving a variety of vision problems such as image restoration and stereo [5, 8].
The power of Online Convex Optimization (OCO) framework is in its ability to generalize many problems from the realm of online and statistical learning, and supply universal tools to solving them.
Object localization is often formulated as a binary classification problem, where a learned classifier determines the existence or absence of a target object within a candidate window of every location, size, and aspect ratio.
Structure from motion (SfM) is the task of jointly reconstructing 3D scenes and camera poses from a set of images.
The Higgs boson was observed for the first time in 2011-2012 and will be the central object of study when the Large Hadron Collider (LHC) comes back online to collect new data in 2015.
Remarkably, recent advances [1, 2] have shown that it is possible to minimise strongly convex finite sums provably faster in expectation than is possible without the finite sum structure.
In online learning problems [4] we aim to sequentially select actions from a given set in order to optimize some performance measure.
The main motivation behind current sparse estimation methods and regularized empirical risk minimization is the principle of parsimony, which states that simple explanations should be preferred over complex ones.
Logistic regression (LR) is a popular and well established classification method that has been widely used in many domains such as machine learning [4, 7], text mining [3, 8], image processing [9, 15], bioinformatics [1, 13, 19, 27, 28], medical and social sciences [2, 17] etc.
Spectral tensor methods and tensor decomposition are emerging themes in machine learning, but they remain global rather than “online.
Gaussian mixture models (GMMs) [1, 2, 3] have become a popular signal model for compressive sensing [4, 5] of imagery and video, partly because the information domain in these problems can be decomposed into subdomains known as pixel/voxel patches [3, 6].
Differential privacy [17] is a cryptographically motivated definition of privacy that has recently gained significant attention in the data mining and machine learning communities.
Blind deconvolution is an important inverse problem that gains increasing attentions from various fields, such as neural signal analysis [3, 10] and computational imaging [6, 8].
Recent work in computer vision relies heavily on manually labeled datasets to achieve satisfactory performance.
Neurons in sensory cortices organize into highly-interconnected circuits that share common input, dynamics, and function.
The growing amount of sparsely and noisily labeled image data demands robust detection methods that can cope with a minimal amount of supervision.
The term biclustering has been used to describe several distinct problems variants.
Olfaction is perhaps the most widespread sensory modality in the animal kingdom, often crucial for basic survival behaviours such as foraging, navigation, kin recognition, and mating.
Gaussian Processes (GPs) provide a flexible nonparametric prior over functions which can be used as a probabilistic module in both supervised and unsupervised machine learning problems.
Principal component analysis (PCA) is a common procedure for preprocessing and denoising, where a low rank approximation to the input matrix (such as the covariance matrix) is carried out.
Although the history of microfinance systems takes us back to as early as the 18th century, the foundation of the modern microfinance movement was laid in the 1970s by Muhammad Yunus, a then-young Economics professor in Bangladesh.
Sparse inverse covariance estimation has received tremendous attention in the machine learning, statistics and optimization communities.
Tensor data appears naturally in a number of applications [1, 2].
A full 90% of all the data in the world has been generated over the last two years and this expansion rate will not diminish in the years to come [17].
Fisher vector coding is a coding method derived from the Fisher kernel [1] which was originally proposed to compare two samples induced by a generative model.
Hierarchically structured clustering models offer a natural representation for many forms of data.
Clustering [1] broadly refers to the problem of identifying data points that are similar to each other.
Submodular functions [1] are a rich class of set functions F : 2V → R, investigated originally in game theory and combinatorial optimization.
For mixture modeling, there is a wide selection of nonparametric Bayesian priors, such as the Dirichlet process [1] and the more general family of normalized random measures with independent increments (NRMIs) [2, 3].
Nearest neighbor classifiers are among the oldest and the most widely used tools in machine learning.
Large-scale Bayesian inference remains intractable for many models, such as logistic regression, sparse linear models, or dynamical systems with non-Gaussian observations.
The geometric structure of a data domain can be described with a graph [11], where neighbor data points are represented by vertices related by an edge.
As we enter the age of “big data”, datasets are growing to ever increasing sizes and there is an urgent need for scalable machine learning algorithms.
A large body of recent work demonstrates that many discrete problems in machine learning can be phrased as the optimization of a submodular set function [2].
There is an increasing demand for systems that learn long-term, high-dimensional data streams.
The growing popularity of online crowdsourcing services (e.
In the last decade many algorithms for numerical linear algebra problems have been proposed, often providing substantial gains over more traditional algorithms based on the singular value decomposition (SVD).
The promise of deep learning is to discover rich, hierarchical models [2] that represent probability distributions over the kinds of data encountered in artificial intelligence applications, such as natural images, audio waveforms containing speech, and symbols in natural language corpora.
You are given a training set with 1M labeled points.
Machine learning, and especially probabilistic modeling, can be difficult to apply.
Estimating depth is an important component of understanding geometric relations within a scene.
Semi-supervised learning considers the problem of classification when only a small subset of the observations have corresponding class labels.
Recent advances in convolutional neural nets [2] dramatically improved the state-of-the-art in image classification.
Clustering and outlier detection are often studied as separate problems [1].
Optimal decision-making constitutes making optimal use of sensory information to maximize one’s overall reward, given the current task contingencies.
A typical workflow for converting a discrete optimization problem over the set of permutations of n objects into a continuous relaxation is as follows: (1) use permutation matrices to represent permutations; (2) relax to the convex hull of the set of permutation matrices — the Birkhoff polytope; (3) relax other constraints to ensure convexity/continuity.
Ensembles of models have long been used as a way to obtain robust performance in the presence of noise.
Segregating a speaker of interest in a multi-speaker environment is an effortless task we routinely perform.
To improve scalability of the widely used ordinary least squares algorithm, a number of randomized approximation algorithms have recently been proposed.
In recent years, developments in neural recording technologies have enabled the recording of populations of neurons from multiple brain areas simultaneously [1–7].
Stochastic variational inference (SVI) lets us scale up Bayesian computation to massive data [1].
Many practical tasks involve finding models that are both simple and capable of explaining noisy observations.
Latent Dirichlet allocation (LDA) [5] is a generative model successfully used in various applications such as text analysis [5], image analysis [15], genometrics [6, 4], human activity analysis [12], and collaborative filtering [14, 20]1 .
Feature selection (FS) is a fundamental and classic problem in machine learning [10, 4, 12].
Differential equations are a basic feature of dynamical systems.
Auctions have long been an active area of research in Economics and Game Theory [Vickrey, 2012, Milgrom and Weber, 1982, Ostrovsky and Schwarz, 2011].
In the last decade, estimating low rank matrices has attracted increasing attention in the machine learning community owing to its successful applications in a wide range of domains including subspace clustering [13], collaborative filtering [9] and visual texture analysis [25], to name a few.
In this paper, we study active learning of classiﬁers in an agnostic setting, where no assumptions are made on the true function that generates the labels.
Expectation-maximization (EM) [10], sampling methods [13], and matrix factorization [20, 25] are three algorithms commonly used to produce maximum likelihood (or maximum a posteriori (MAP)) estimates of models with latent variables/factors, and thus are used in a wide range of applications such as clustering, topic modeling, collaborative filtering, structured prediction, feature engineering, and time series analysis.
It is an impressive yet alarming fact that there is far more video being captured—by consumers, scientists, defense analysts, and others—than can ever be watched or browsed efficiently.
Interdependent Security (IDS) games [1] model the interaction among multiple agents where each agent chooses whether to invest in some form of security to prevent a potential loss based on both direct and indirect (transfer) risks.
Graphical models are a convenient tool to illustrate the dependencies among a collection of random variables with potentially complex interactions.
A sensible criterion for integration of partially redundant information from multiple senses is that no information about the underlying cause be lost.
Machine learning approaches have proven highly effective for statistical pattern recognition problems, such as those encountered in speech or vision.
Multivariate longitudinal ordinal/count data arise in many areas, including economics, opinion polls, text mining, and social science research.
Markov logic [4] uses weighted first order formulas to compactly encode uncertainty in large, relational domains such as those occurring in natural language understanding and computer vision.
Subset selection is a core task in many real-world applications.
Probabilistic graphical models (PGMs) such as Markov random fields (MRFs) and Bayesian networks (BNs) are widely used as a knowledge representation tool for reasoning under uncertainty.
There has been significant recent interest in extending multi-armed bandit techniques to address problems with more complex information structures, in which sampling one action can inform the decision-maker’s assessment of other actions.
We consider supervised multitask learning problems [1, 6, 7] in which the tasks are indexed by a pair of indices known as multilinear multitask learning (MLMTL) [17, 19].
Sparse representation represents a signal as the linear combination of a small number of atoms chosen out of a dictionary, and it has achieved a big success in various image processing and computer vision applications [1, 2].
The generation of random samples from a posterior distribution is a pervasive problem in Bayesian statistics which has many important applications in machine learning.
We are given n random variables V = {V1 , V2 , .
Subspace clustering is a classic problem where one is given points in a high-dimensional ambient space and would like to approximate them by a union of lower-dimensional linear subspaces.
Bayesian statistics provides a natural way to manage model complexity and control overfitting, with modern problems involving complicated models with a large number of parameters.
The estimation of a probability density function (pdf) from a random sample is a ubiquitous problem in statistics.
Data clustering is a classic unsupervised learning technique, whose goal is dividing input data into disjoint sets.
The central theme in approaches like kernel machines [1] and spectral clustering [2, 3] is the use of symmetric matrices that encode certain similarity relations between pairs of data instances.
Clustering algorithms aim to find a meaningful grouping of the samples at hand in an unsupervised manner for exploratory data analysis.
We define a set of simple linear learning problems described by an n dimensional square matrix M with ±1 entries.
To solve complex problems in real-time, intelligent agents have to make efficient use of their finite computational resources.
Kernel methods have become standard for building non-linear models from simple feature representations, and have proven successful in problems ranging across classification, regression, structured prediction and feature extraction [16, 20].
We consider a reinforcement learning agent that takes sequential actions within an uncertain environment with an aim to maximize cumulative reward [1].
Information constraints play a key role in machine learning.
Existing clustering methods fall roughly into two categories.
In an online combinatorial decision problem, the decision space is a set of combinatorial structures, such as subsets, trees, paths, permutations, etc.
Large neural networks have recently demonstrated impressive performance on a range of speech and vision tasks.
Multi-armed bandit (MAB) is a predominant model for characterizing the tradeoff between exploration and exploitation in decision-making problems.
Semantic labeling aims at getting pixel-wise dense labeling of an image in terms of semantic concepts such as tree, road, sky, water, foreground objects etc.
In linear regression, the goal is to predict the real-valued labels of data points in Euclidean space using a linear function.
Big data analysis challenges both statistics and computation.
Deep learning has recently achieved significant advances in several areas of perceptual computing, including speech recognition [1], image analysis and object detection [2, 3], and natural language processing [4].
Bayesian optimization techniques form a successful approach for optimizing black-box functions [5].
Several real-world applications routinely encounter multi-way data with structure which can be modeled as low-rank tensors.
A Markov random field (MRF) is a graph whose vertices are random variables, and whose edges specify a neighborhood over the random variables.
As vision techniques like segmentation and object recognition begin to mature, there has been an increasing interest in broadening the scope of research to full scene understanding.
Automatic music transcription is the task of transcribing a musical audio signal into a symbolic representation (for example MIDI or sheet music).
The nearest neighbor classifier for non-parametric classification is perhaps the most intuitive learning algorithm.
As the number of digital images which are available online is constantly increasing due to rapid advances in digital camera technology, image processing tools and photo sharing platforms, similaritypreserving binary codes have received significant attention for image search and retrieval in largescale image collections [1, 2].
Object categorization is a challenging problem that requires drawing boundaries between groups of objects in a seemingly continuous space.
`1 -regularized M -estimators have attracted considerable interest in recent years due to their ability to fit large-scale statistical models, where the underlying model parameters are sparse.
Among pre-processing methods, data partitioning is one of the most fundamental.
All branches of experimental science are plagued by missing data.
There has been significant interest recently in developing discriminative feature-learning models, in which the labels are utilized within a max-margin classifier.
In a classical transfer learning setting, we have sufficient fully labeled data from the source domain (or the training domain) where we fully observe the data points X tr , and all corresponding labels Y tr are known.
Many problems in Computer Vision, Natural Language Processing and Computational Biology involve mappings from an input space X to an exponentially large space Y of structured outputs.
Consider the problem of determining the connectivity structure of subsurface aquifers in a large ground-water system from time-series measurements of the concentration of tracers injected and measured at multiple spatial locations.
Sketching has emerged as a powerful dimensionality reduction technique for accelerating statistical learning techniques such as `p -regression, low rank approximation, and principal component analysis (PCA) [12, 5, 14].
Undirected probabilistic graphical models, also known as Markov Random Fields (MRFs), are a natural framework for modelling in networks, such as sensor networks and social networks [24, 11, 20].
Stochastic and online gradient descent methods have proved to be extremely useful for solving largescale machine learning problems [1, 2, 3, 4].
In statistical analyses involving data from individuals, there is an increasing tension between the need to share the data and the need to protect sensitive information about the individuals.
An integer-valued1 function f : 2X → Z defined over subsets of some finite ground set X of n elements is submodular if it satisfies the following diminishing marginal returns property: for every S ⊆ T ⊆ X and i ∈ X \ T , f (S ∪ {i}) − f (S) ≥ f (T ∪ {i}) − f (T ).
Over the past decades, our knowledge of how neural systems process static information has advanced considerably, as is well documented by the receptive field properties of neurons.
Many learning tasks require separating a time series into a linear combination of a larger number of “source” signals.
When faced with large datasets, it is commonly observed that using all the data with a simpler algorithm is superior to using a small fraction of the data with a more computationally intense but possibly more effective algorithm.
The adversarial multi-armed bandit problem [4] is a T -round prediction game played by a randomized player in an adversarial environment.
Given a set of individual preferences from multiple decision makers or judges, we address the problem of computing a consensus ranking that best represents the preference of the population collectively.
Recurrent Neural Networks (RNN) constitute a powerful computational tool for sequences modelling and prediction [1].
It is often the case that our geometric intuition, derived from experience within a low dimensional physical world, is inadequate for thinking about the geometry of typical error surfaces in high-dimensional spaces.
A cognitive map, as originally conceived by Tolman [46], is a geometric representation of the environment that can support sophisticated navigational behavior.
Artificial neural networks with several hidden layers, called deep neural networks, have become popular due to their unprecedented success in a variety of machine learning tasks (see, e.
Over the past decade, progress has been made in developing non-asymptotic bounds on the estimation error of structured parameters based on norm regularized regression.
Policy search methods can be divided into model-based algorithms, which use a model of the system dynamics, and model-free techniques, which rely only on real-world experience without learning a model [10].
Differential Dynamic Programming (DDP) is a powerful trajectory optimization approach.
Consider the problem of sequentially recommending content for a set of users.
Matching local visual features is a core problem in computer vision with a vast range of applications such as image registration [28], image alignment and stitching [6] and structure-from-motion [1].
When statistical predictors are deployed in a live production environment, feedback loops can become a concern.
Consider a learner who in each round t = 1, 2, .
Branch-and-bound (B&B) [1] is a systematic enumerative method for global optimization of nonconvex and combinatorial problems.
Structured prediction models are popularly used to solve structure dependent problems in a wide variety of application domains including natural language processing, bioinformatics, speech recognition, and computer vision.
The F1 -measure, defined as the harmonic mean of the precision and recall of a binary decision rule [20], is a traditional way of assessing the performance of classifiers.
This paper addresses the problem of solving large state-space Markov Decision Processes (MDPs)[16] in an infinite time horizon and discounted reward setting.
There has been a growing interest in high-dimensional statistical problems, where the number of parameters d is comparable to or even larger than the sample size n, spurred in part by many modern science and engineering applications.
Building rich generative models that are capable of extracting useful, high-level latent representations from high-dimensional sensory input lies at the core of solving many AI-related tasks, including object recognition, speech perception and language understanding.
Stochastic variational inference (SVI) is a powerful method for scaling up Bayesian computation to massive data sets [1].
The high speed of human sensory perception [1] is puzzling given its inherent computational complexity: sensory inputs are noisy and ambiguous, and therefore do not uniquely determine the state of the environment for the observer, which makes perception akin to a statistical inference problem.
Until recently, much of the emphasis in the theory of high-dimensional statistics has been on “first order” problems, such as estimation and prediction.
The performance of face recognition systems depends heavily on facial representation, which is naturally coupled with many types of face variations, such as view, illumination, and expression.
Recent years have seen a surge of work at the intersection of social choice and machine learning.
1.
Models of natural language need the ability to compose the meaning of words and phrases in order to understand complex utterances such as facts, multi-word entities, sentences or stories.
In this paper we introduce the Translation-invariant Matrix-T process (TiMT) for estimating Gaussian graphical models (GGMs) from pairwise distances.
The learnablity of regular languages is a classic topic in computational learning theory.
A standard optimization criterion for an infinite horizon Markov decision process (MDP) is the expected sum of (discounted) costs (i.
Recommender systems exploit fragmented information available from each user.
Biological systems face the difficult task of devising effective control strategies based on partial information communicated between sensors and actuators across multiple distributed networks.
It once seemed obvious that the running time of an algorithm should increase with the size of the input.
Extracting clusters or communities in networks have numerous applications and constitutes a fundamental task in many disciplines, including social science, biology, and physics.
In this paper we develop a probabilistic model of articles and reader behavior data.
Modern data-science applications increasingly require distributed learning algorithms to extract information from many data repositories stored at different locations with minimal interaction.
Bipartite ranking (scoring) amounts to rank (score) data from binary labels.
The explosion in both size and velocity of data has brought new challenges to the design of statistical algorithms.
Method of Moments (MoM) based algorithms [1, 2, 3] for learning latent variable models have recently become popular in the machine learning community.
Gaussian processes (GPs, [1]) are a popular choice in practical Bayesian non-parametric modeling.
Humans are able to routinely estimate unknown world states from ambiguous and noisy stimuli, and anticipate upcoming events by learning the temporal dynamics of relevant states of the world from incomplete knowledge of the environment.
In many important applications, we are faced with the problem of sampling from high dimensional probability measures [19].
In this paper, we study online learning problems within a drifting-games framework, with the aim of developing a general methodology for designing learning algorithms based on a minimax analysis.
Deep Neural Networks (DNNs) are extremely powerful machine learning models that achieve excellent performance on difficult problems such as speech recognition [13, 7] and visual object recognition [19, 6, 21, 20].
The predominant paradigm of modeling time series is based on state-space models, in which a hidden state evolves according to some predefined dynamical law, and an observation model maps the state to the dataspace.
Bayesian inference in statistical models involving a large number of latent random variables is in general a difficult problem.
The traditional approach to fitting a Gaussian mixture model onto the data involves using the wellknown expectation-maximization algorithm to estimate component parameters [7].
How our visual system achieves robust performance against corruptions is a mystery.
The goal of supervised machine learning is to use available source data to make predictions with the smallest possible error (loss) on unlabeled target data.
Recent years have witnessed the emergence of big graphs in a large variety of real applications, such as the web and social network services.
Determining connectivity in populations of neurons is fundamental to understanding neural computation and function.
In recent years, many computer vision and natural language processing (NLP) tasks have benefited from the use of dense representations of inputs by allowing superficially different inputs to be related to one another [26, 9, 7, 4].
We have recently seen a revival of attention given to convolutional neural networks (CNNs) [22] due to their high performance for large-scale visual recognition tasks [15, 21, 30].
Recent progress in large-scale techniques for recording neural activity has made it possible to study the joint firing statistics of 102 up to 105 cells at single-neuron resolution.
Many image and video degradation processes can be modeled as translation-invariant convolution.
Topic modeling offers a suite of useful tools that automatically learn the latent semantic structure of a large collection of documents.
This paper consider the following optimization problem: def minimize f (x) = g(x) + h(x), x∈Rd (1) d where Pn g is the average dof the smooth convex functions g1 , .
The success of machine learning has led to its widespread use as a workhorse in a wide variety of domains, from text and language recognition to trading agent design.
In the standard formulation of graph clustering, we are given an unweighted graph and seek a partitioning of the nodes into disjoint groups such that members of the same group are more densely connected than those in different groups.
Structure sparsity induced regularization terms [1, 8] have been widely used recently for feature learning purpose, due to the inherent sparse structures of the real world data.
In many real-world applications, the performance measure used to evaluate a learning model is non-decomposable and cannot be expressed as a summation or expectation of losses on individual data points; this includes, for example, the F-measure used in information retrieval [1], and several combinations of the true positive rate (TPR) and true negative rate (TNR) used in class imbalanced classification settings [2–5] (see Table 1).
Structure learning in Markov networks, also known as undirected graphical models or Markov random fields, has attracted considerable interest in computational statistics, machine learning, and artificial intelligence.
Without any prior knowledge, what can be automatically learned from high-dimensional data? If the variables are uncorrelated then the system is not really high-dimensional but should be viewed as a collection of unrelated univariate systems.
Deep convolutional neural networks (CNNs) [1] with max-pooling layers [2] trained by backprop [3] on GPUs [4] have become the state-of-the-art in object recognition [5, 6, 7, 8], segmentation/detection [9, 10], and scene parsing [11, 12] (for an extensive review see [13]).
Graphical models [1, 2, 3] are a popular and important means of representing certain conditional independence relations between random variables.
Modern data analysis has seen an explosion in the size of the datasets available to analyze.
Drawing samples from arbitrary probability distributions is a core problem in statistics and machine learning.
Understanding the world in a single glance is one of the most accomplished feats of the human brain: it takes only a few tens of milliseconds to recognize the category of an object or environment, emphasizing an important role of feedforward processing in visual recognition.
Coordinate descent methods have received extensive attention in recent years due to their potential for solving large-scale optimization problems arising from machine learning and other applications.
Convolutional neural networks (CNNs) trained via backpropagation were recently shown to perform well on image classification tasks with millions of training images and thousands of categories [1, 2].
Given a matrix X ∈ Rm×n , biclustering or submatrix localization is the problem of identifying a subset of the rows and columns of X such that the bicluster or submatrix consisting of the selected rows and columns are “significant” compared to the rest of X.
One fundamental goal of any learning algorithm is to strike a right balance between underfitting and overfitting.
Progress on the path from shallow bag-of-words information retrieval algorithms to machines capable of reading and understanding documents has been slow.
Statistical relational models such as Markov logic [5] have the power to represent the rich relational structure as well as the underlying uncertainty, both of which are the characteristics of several real world application domains.
In this paper, we develop a latent variable model and efﬁcient spectral algorithm motivated by the recent emergence of very large data sets of chromatin marks from multiple human cell types [7, 9].
Not only are our data growing in volume and dimensionality, but the understanding that we wish to gain from them is increasingly sophisticated.
While early human case studies revealed the importance of the hippocampus in episodic memory [1, 2], the discovery of “place cells” in rats [3] established its role for spatial representation.
Interactive real-time controllers that are capable of generating complex, stable and realistic movements have many potential applications including robotic control, animation and gaming.
Gaussian Mixture Models (GMMs) are a mainstay in a variety of areas, including machine learning and signal processing [4, 10, 16, 19, 21].
Tensors are useful representational objects to model a variety of problems such as graphical models with latent variables [1], audio classification [20], psychometrics [8], and neuroscience [3].
Bayesian networks are probabilistic graphical models representing joint probability distributions of random variables.
Understanding differences between populations is a common task across disciplines, from biomedical data analysis to demographic or textual analysis.
Entropies, divergences, and mutual informations are classical information-theoretic quantities that play fundamental roles in statistics, machine learning, and across the mathematical sciences.
The task of selecting a set of items subject to constraints on the size or the cost of the set is versatile in machine learning problems.
Bayesian inference is a powerful framework for analyzing data.
Many problems in machine learning can be written as a stochastic optimization problem minimize E[f˜(x)] over x ∈ Rn , where f˜ is a random objective function.
This work tackles the challenge of constructing fully empirical bounds on the mixing time of Markov chains based on a single sample path.
Since large numbers of high-definition displays have sprung up, generating high-resolution videos from previous low-resolution contents, namely video super-resolution (SR), is under great demand.
Testing whether two random variables are identically distributed without imposing any parametric assumptions on their distributions is important in a variety of scientific applications.
The last few years have seen tremendous progress in learning useful image representations [6].
Probabilistic graphical models such as Bayesian networks and Markov random fields provide a useful framework and powerful tools for machine learning.
Computer vision researchers often go through great lengths to remove dataset biases from their models [32, 20].
Recently, supervised learning has been developed and used successfully to produce representations that have enabled leaps forward in classification accuracy for several tasks [1].
Being rooted in information retrieval [16], the so-called F-measure is nowadays routinely used as a b = (b performance metric in various prediction tasks.
Let M ⇤ 2 Rm⇥n be a rank k matrix with k much smaller than m and n.
It is often desirable to model discrete data in terms of continuous latent structure.
Enormous amounts of astronomical data are collected by a range of instruments at multiple spectral resolutions, providing information about billions of sources of light in the observable universe [1, 10].
Topic models have emerged as flexible and important tools for the modelisation of text corpora.
Methods for neuroimaging research can be grouped by discovering neurobiological structure or assessing the neural correlates associated with mental tasks.
Variational methods are a popular alternative to Markov chain Monte Carlo (MCMC) methods for Bayesian inference.
Symmetry-breaking is an approach to speeding up satisfiability testing by adding constraints, called symmetry-breaking predicates (SBPs), to a theory [7, 1, 16].
Any matrix A ∈ Rn×d with rank r can be written using a singular value decomposition (SVD) as A = UΣVT .
Many models of visual saliency have been proposed in the last decade with differences in defining principles and also divergent objectives.
Applications such as speech recognition [1], medical diagnosis [2], optical character recognition [3], machine translation [4], and scene labeling [5] have two properties: (i) they are instances of structured prediction, where the predicted output is a complex structured object; and (ii) they are user-facing applications for which it is important to provide accurate estimates of confidence.
Most interactive systems (e.
Log-linear models, a general class that includes conditional random fields (CRFs) and generalized linear models (GLMs), offer a flexible yet tractable approach modeling conditional probability distributions p(x|y) [1, 2].
The brain is faced with the persistent challenge of decision making under uncertainty due to noise in the sensory inputs and perceptual ambiguity .
In many machine learning tasks, data is represented in a high-dimensional Euclidean space.
Consider the following simple game.
We follow the standard mathematical abstraction of this problem (Candes & Fernandez-Granda [4, 3]): consider a d-dimensional signal x(t) modeled as a weighted sum of k Dirac measures in Rd : x(t) = k X wj µ(j) , (1) j=1 where the point sources, the µ(j) ’s, are in Rd .
Graphical models are a popular modeling tool for both discrete and continuous distributions.
Several variants of learning-to-rank problems have recently been studied in an online setting, with preferences over alternatives given in the form of stochastic pairwise comparisons [6].
A critical task in data analysis is to determine how similar two data samples are.
A hallmark of an intelligent agent is to learn new information as the world unfolds and to improvise by fusing the new information with prior knowledge.
Clustering is a fundamental machine learning problem with widespread applications.
Recurrent neural networks (RNN) have recently been trained successfully for time series modeling, and have been used to achieve state-of-the-art results in supervised tasks including handwriting recognition [12] and speech recognition [13].
When faced with a complex target distribution, one often turns to RMarkov chain Monte Carlo (MCMC) [1] to approximate intractable expectations EP [h(Z)] = X p(x)h(x)dx with asympPn totically exact sample estimates EQ [h(X)] = i=1 q(xi )h(xi ).
The task of completing the missing entries of a matrix from an incomplete subset of (potentially noisy) entries is encountered in many applications including recommendation systems, data imputation, covariance matrix estimation, and sensor localization among others.
The contextual bandit problem [1, 2, 3] is an important extension of the classic multi-armed bandit (MAB) problem [4], where the agent can observe a set of features, referred to as context, before making a decision.
One of the most basic problems in statistical hypothesis testing is the question of distinguishing whether two unknown distributions are very similar, or significantly different.
Likelihood-based approaches to learning dynamical systems, such as EM [1] and MCMC [2], can be slow and suffer from local optima.
A good distance metric is often the key to an effective machine learning algorithm.
High-dimensional tensor-valued data are prevalent in many fields such as personalized recommendation systems and brain imaging research [1, 2].
We consider a generalization of the one-bit quantized regression problem, where we seek to recover the regression coefficient β ∗ ∈ Rp from one-bit measurements.
A central challenge in machine learning is to extract useful information from massive data.
Background Many learning applications, ranging from language-processing staples such as speech recognition and machine translation to biological studies in virology and bioinformatics, call for estimating large discrete distributions from their samples.
A wide range of machine learning and signal processing problems can be formulated as the minimization of a composite objective: min F (x) := f (x) + kBxk x∈X (1) where X is closed and convex, f is convex and can be either smooth, or nonsmooth yet enjoys a particular structure.
Tensor is a natural way to express higher order interactions for a variety of data and tensor decomposition has been successfully applied to wide areas ranging from chemometrics, signal processing, to neuroimaging; see [15, 18] for a survey.
There has been significant recent interest in deep learning.
Complex machine learning tools such as deep learning are gaining increasing popularity and are being applied to a wide variety of problems.
In the primate and human retina, roughly 20 distinct classes of retinal ganglion cells (RGCs) send distinct visual information to diverse targets in the brain [18, 7, 6].
We study the problem of monitoring several time series so as to maintain a precise belief while minimising the cost of sensing.
Over the last decade, graph kernels have become a popular approach to graph comparison [4, 5, 7, 9, 12, 13, 14], which is at the heart of many machine learning applications in bioinformatics, imaging, and social-network analysis.
Discovering causal effects is a fundamentally important yet very challenging task in various disciplines, from public health research and sociological studies, economics to many applications in the life sciences.
Our visual system is designed to perceive a physical world that is full of dynamic content.
Analogy is the task of mapping information from a source to a target.
The great success of neural networks is due in part to the simplicity of the backpropagation algorithm, which allows one to efficiently compute the gradient of any loss function defined as a composition of differentiable functions.
We consider the problem of distributed convex learning and optimization, where a set of m machines, each with access to a different local convex function Fi : Rd 7→ R and a convex domain W ⊆ Rd , attempt to solve the optimization problem m min F (w) where F (w) = w∈W 1 X Fi (w).
Over recent years, the landscape of computer vision has been drastically altered and pushed forward through the adoption of a fast, scalable, end-to-end learning framework, the Convolutional Neural Network (CNN) [18].
Low rank matrix completion approaches are among the most widely used collaborative filtering methods, where a partially observed matrix is available to the practitioner, who needs to impute the missing entries.
Consider the following convex optimization problem min f (x) = g(x1 , · · · , xK ) + K  hk (xk ), s.
Deep neural networks currently demonstrate state-of-the-art performance in many domains of largescale machine learning, such as computer vision, speech recognition, text processing, etc.
Considerable research has been devoted to developing probabilistic models for high-dimensional time-series data, such as video and music sequences, motion capture data, and text streams.
Many machine learning tasks can be framed as learning a function given noisy information about its inputs and outputs.
Consider the problem of regret minimization in non-stochastic multi-armed bandits, as defined in the classic paper of Auer, Cesa-Bianchi, Freund, and Schapire [5].
The Multi-Armed Bandit (MAB) problem is one of the most popular settings encountered in the sequential decision-making literature [Rob52, LR85, EDMM06, Sco10, BCB12] with applications across multiple disciplines.
We consider the following problem of high dimensional linear regression: y = Xθ∗ + ω , (1) where y ∈ Rn is the response vector, X ∈ Rn×p has independent isotropic sub-exponential random rows, ω ∈ Rn has i.
Time series forecasting plays a crucial role in a number of domains ranging from weather forecasting and earthquake prediction to applications in economics and finance.
Many learning tasks require labeling large datasets.
Many modern fMRI studies of the human brain use data from multiple subjects.
The central idea behind Bayesian nonparametrics (BNPs) is the replacement of classical finitedimensional prior distributions with general stochastic processes, allowing for an open-ended number of degrees of freedom in a model [8].
Aggregating pairwise comparisons and partial rankings are important problems with applications in econometrics [1], psychometrics [2, 3], sports ranking [4, 5] and multiclass classification [6].
The goal of a metric learning algorithm is to capture the idiosyncrasies in the data mainly by defining a new space of representation where some semantic constraints between examples are fulfilled.
Partial monitoring is a general framework for sequential decision making problems with imperfect feedback.
There have been many recent advances in the recovery of communities in networks, under “blockmodel” assumptions [19, 18, 9].
Graphical models provide a powerful framework for reasoning with probabilistic information.
Probabilistic modeling has emerged as a powerful tool for data analysis.
Tree structured group Lasso (TGL) [13, 30] is a powerful regression technique in uncovering the hierarchical sparse patterns among the features.
A recent confluence of results from game theory and learning theory gives a simple explanation for why good outcomes in large families of strategically-complex games can be expected.
Matrix factorization (MF) techniques have emerged as a powerful tool to perform collaborative filtering in large datasets [1].
Online social networks, such as Twitter or Weibo, have become large information networks where people share, discuss and search for information of personal interest as well as breaking news [1].
Many machine learning tasks involve careful tuning of a regularization parameter that controls the balance between an empirical loss term and a regularization term.
Recently, attention-based recurrent networks have been successfully applied to a wide variety of tasks, such as handwriting synthesis [1], machine translation [2], image caption generation [3] and visual object classification [4].
Given a label budget, what is the best way to learn a classifier? Active learning approaches to this question are known to yield exponential improvements over supervised learning under strong assumptions [7].
The central problem of this paper is computational complexity in a setting where the number of classes k for multiclass prediction is very large.
Deep neural networks (DNNs) have recently been achieving state of the art results in many fields.
Building a good generative model of natural images has been a fundamental problem within computer vision.
We consider the problem of robust Principal Component Analysis (PCA).
In structured output prediction, it is important to learn a model that can perform probabilistic inference and make diverse predictions.
Markov random fields (MRFs) are used in many areas of computer science such as vision and speech.
In this paper we develop SLEEC (Sparse Local Embeddings for Extreme Classification), an extreme multi-label classifier that can make significantly more accurate and faster predictions, as well as scale to larger problems, as compared to state-of-the-art embedding based approaches.
In interactive submodular set cover (ISSC) [10, 11, 9], the goal is to interactively satisfy all plausible submodular functions in as few actions as possible.
We formulate hierarchical image segmentation from the perspective of estimating an ultrametric distance over the set of image pixels that agrees closely with an input set of noisy pairwise distances.
This paper constructs an algorithmic framework for the following convex optimization template: f ‹ :“ min tf pxq : Ax ´ b P Ku , xPX (1) where f : Rp Ñ R Y t`8u is a convex function, A P Rnˆp , b P Rn , and X and K are nonempty, closed and convex sets in Rp and Rn respectively.
Stochastic optimal control (SOC) is a general and powerful framework with applications in many areas of science and engineering.
The dueling bandit problem [1] arises naturally in domains where feedback is more reliable when given as a pairwise preference (e.
Multi-task learning (MTL) advocates sharing relevant information among several related tasks during the training stage.
Finding the global maximizer of a non-concave objective function based on sequential, noisy observations is a fundamental problem in various real world domains e.
Learning problems on graph-structured data have received significant attention in recent years [11, 17, 20].
In matrix completion, one has access to a matrix with only a few observed entries, and the task is to estimate the entire matrix using the observed entries.
We consider a convex optimization problem, minimizex∈X f (x), where X ⊆ Rn is convex and closed, f is a C 1 convex function, and ∇f is assumed to be Lf -Lipschitz.
Problem setting The problem of Compressive Phase Retrieval (CPR) is generally stated as the problem of estimating a k-sparse vector x? ∈ Rd from noisy measurements of the form 2 yi = |hai , x? i| + zi (1) for i = 1, 2, .
Discriminative methods pursue a direct mapping from the input to the output space for a classification or a regression task.
Scene labeling (or scene parsing) is an important step towards high-level image interpretation.
The rapidly growing data dimension has brought new challenges to statistical variable selection, a crucial technique for identifying important variables to facilitate interpretation and improve prediction accuracy.
Dynamic causal systems are a major focus of scientific investigation in diverse domains, including neuroscience, economics, meteorology, and education.
Machine learning has recently made great strides in many application areas, fueling a growing demand for machine learning systems that can be used effectively by novices in machine learning.
It is surely no surprise to the reader that modern machine learning algorithms thrive on large amounts of data — preferably labeled.
Undirected Graphical Models (also known as Markov Random Fields) provide a flexible framework to represent networks of random variables and have been used in a large variety of applications in machine learning, statistics, signal processing and related fields [2].
The greedy algorithm is simple and easy-to-implement, and can be applied to solve a wide range of complex optimization problems, either with exact solutions (e.
In the traditional economic approach to identifying a revenue-maximizing auction, one first posits a prior distribution over all unknown information, and then solves for the auction that maximizes expected revenue with respect to this distribution.
Recurrent neural networks can be used to process sequences, either as input, output or both.
Modeling notions such as coverage, representativeness, or diversity is an important challenge in many machine learning problems.
Modern classification problems often involve the prediction of multiple labels simultaneously associated with a single instance e.
One of the challenging aspects of deep learning is the optimization of the training criterion over millions of parameters: the difficulty comes from both the size of these neural networks and because the training objective is non-convex in the parameters.
This paper studies the problem of recovering communities in the general stochastic block model with linear size communities, for constant and logarithmic degree regimes.
The goal of machine learning is to produce hypotheses or models that generalize well to the unseen instances of the problem.
Training deep networks is a challenging problem [16, 2] and various heuristics and optimization algorithms have been suggested in order to improve the efficiency of the training [5, 9, 4].
An active learner is given a hypothesis class, a large set of unlabeled examples and the ability to interactively make label queries to an oracle on a subset of these examples; the goal of the learner is to learn a hypothesis in the class that ﬁts the data well by making as few oracle queries as possible.
There has been a steep rise in recent work [6, 7, 9–12, 25, 27, 29] on “variance reduced” stochastic gradient algorithms for convex problems of the finite-sum form: Xn min f (x) := n1 fi (x).
Gaussian process models are attractive for machine learning because of their flexible nonparametric nature.
Economist Thomas Sowell remarked that “The first lesson of economics is scarcity: There is never enough of anything to fully satisfy all those who want it.
We investigate the problem of constructing, in a memory and computationally efficient manner, an accurate estimate of the optimal rank k approximation M (k) of a large (m × n) matrix M ∈ [0, 1]m×n .
Delivering personalized user experiences is believed to play a crucial role in the long-term engagement of users to modern web-services [26].
We consider the low-rank approximation of symmetric positive semi-definite (SPSD) matrices that arise in machine learning and data analysis, with an emphasis on obtaining good statistical guarantees.
We consider the problem of optimizing the average of a finite but large sum of smooth functions, n min f (x) = x∈Rd 1X fi (x).
A restricted Boltzmann machine (RBM) [1, 2] is a type of undirected neural network with surprisingly many applications.
Kernel methods [17] have enjoyed tremendous success in solving several fundamental problems of machine learning ranging from classification, regression, feature extraction, dependency estimation, causal discovery, Bayesian inference and hypothesis testing.
Robust Least Squares Regression (RLSR) addresses the problem of learning a reliable set of regression coefficients in the presence of several arbitrary corruptions in the response vector.
Modern statistical inference demands scalability to massive datasets and high-dimensional models.
Determinantal point processes (DPPs) are point processes [1] that encode repulsiveness using algebraic arguments.
Neural networks have become ubiquitous in applications ranging from computer vision [1] to speech recognition [2] and natural language processing [3].
Graphical models are a flexible and widely used tool for modeling and inference in high dimensional settings.
Our brains analyze high-dimensional datasets streamed by our sensory organs with efficiency and speed rivaling modern computers.
With increasingly efﬁcient data collection methods, scientists are interested in quickly analyzing ever larger data sets.
Hidden Markov Models (HMM’s) are among the most widely adopted latent-variable models used to model time-series datasets in the statistics and machine learning communities.
In complex, chronic diseases such as autism, lupus, and Parkinson’s, the way the disease manifests may vary greatly across individuals [1].
Semi-supervised learning is now a standard methodology in machine learning.
Most reinforcement learning (RL) algorithms learn a value function—a function that estimates the expected return obtained by following a given policy from a given state.
We focus on the following minimization problem, n minimize f (✓) := 1X fi (✓), n i=1 (1.
Multi-Armed Bandit (MAB) problems [1] constitute the most fundamental sequential decision problems with an exploration vs.
Kernel methods such as nonlinear support vector machines (SVMs) [1] provide a powerful framework for nonlinear learning, but they often come with significant computational cost.
Causality is a fundamental concept in sciences and philosophy.
For several decades there has been much interest in understanding the manner in which ideas, language, and information cascades spread through society.
Consider the problem of minimizing a convex function over some convex domain.
Humans are good at considering “what-if?” questions about objects in their environment.
Markowitz’s mean-variance analysis sets the basis for modern portfolio optimization theory [1].
Ensemble-based learning is a very successful approach to learning classifiers, including well-known methods like boosting [1], bagging [2], and random forests [3].
In active learning, we are given a sample space X , a label space Y, a class of models that map X to Y, and a large set U of unlabelled samples.
A function f : 2S → R+ is called submodular if f (X) + f (Y ) ≥ f (X ∪ Y ) + f (X ∩ Y ) for all X, Y ⊆ S, where S is a finite ground set.
We consider a general global optimization problem: maximize f (x) subject to x ∈ Ω ⊂ RD where f : Ω → R is a non-convex black-box deterministic function.
Many high-dimensional datasets comprise points derived from a smooth, lower-dimensional manifold embedded within the high-dimensional space of measurements and possibly corrupted by noise.
Nowadays data of huge scale are prevalent in many applications of statistical learning and data mining.
Sequential Monte Carlo (SMC) is a class of algorithms that draw samples from a target distribution of interest by sampling from a series of simpler intermediate distributions.
As the number of classes increases, two important issues emerge: class overlap and multilabel nature of examples [9].
In the late 1940s, Wald and colleagues developed a sequential test called the sequential probability ratio test (SPRT; [7]).
Expectation Propagation (EP, 1) is an efficient approximate inference algorithm that is known to give good approximations, to the point of being almost exact in certain applications [2, 3].
We study a natural asynchronous stochastic gradient method for the solution of minimization problems of the form Z minimize f (x) := EP [F (x; W )] = F (x; ω)dP (ω), (1) Ω where x 7→ F (x; ω) is convex for each ω ∈ Ω, P is a probability distribution on Ω, and the vector x ∈ Rd .
In many machine learning problems, the statistical risk functional is an expectation over d-tuples (d ≥ 2) of observations, rather than over individual points.
Machine learning aims to find regularities in data to perform various tasks.
Numerous graphics algorithms have been established to synthesize photorealistic images from 3D models and environmental variables (lighting and viewpoints), commonly known as rendering.
We consider the estimation of generalized linear models (GLMs) [1], under high-dimensional settings where the number of variables p may greatly exceed the number of observations n.
Stochastic search algorithms [1, 2, 3, 4] are black box optimizers of an objective function that is either unknown or too complex to be modeled explicitly.
You step out of your house and notice a group of people looking up.
The multi-armed bandit is the simplest class of problems that exhibit the exploration/exploitation dilemma.
The expectation-maximization (EM) algorithm [12] is the most popular approach for calculating the maximum likelihood estimator of latent variable models.
Computing integrals is a core challenge in machine learning and numerical methods play a central role in this area.
Estimating expectations using Markov Chain Monte Carlo (MCMC) is a fundamental approximate inference technique in Bayesian statistics.
Learning deep structured models has attracted considerable research attention recently.
Combining image understanding and natural language interaction is one of the grand dreams of artificial intelligence.
This paper studies a class of problems that share a common high-level objective: from a number n of probabilistic distributions, find the k ones whose means are the greatest by a certain metric.
The spectral distribution of light reflected off a surface is a function of an intrinsic material property of the surface—its reflectance—and also of the spectral distribution of the light illuminating the surface.
Recently, there is increasing interest in the field of multimodal learning for both natural language and vision.
Text classification is a classic topic for natural language processing, in which one needs to assign predefined categories to free-text documents.
According to Ghahramani [9], models that have a nonparametric component give us more flexiblity that could lead to better predictive performance.
A broad class of learning problems fits into the framework of obtaining a sequence of independent random samples from a unknown distribution, and then (approximately) recovering this distribution using as few samples as possible.
Consider the following detection problem.
We consider a general class of stochastic optimization problems formulated as ξ ∗ = arg min Eτ ∼p(·|ξ) [J(τ )], ξ (1) where ξ defines a vector of decision variables, τ represents the system response defined through the density p(τ |ξ), and J(τ ) defines a positive cost function which can be non-smooth and nonconvex.
Linear Programming (LP) has been studied since the early 19th century and has become one of the representative tools of numerical optimization with wide applications in machine learning such as `1 -regularized SVM [1], MAP inference [2], nonnegative matrix factorization [3], exemplarbased clustering [4, 5], sparse inverse covariance estimation [6], and Markov Decision Process [7].
Networks are the simplest representation of relationships between entities, and as such have attracted significant attention recently.
Figure 1 shows an example of an image restoration problem.
Multi-label learning refers to the problem setting in which the goal is to assign to an object (e.
Age-related brain diseases, such as Parkinson’s or Alzheimer’s disease (AD) are complex diseases with multiple effects on the metabolism, structure and function of the brain.
The problem of measuring and harnessing dependence between random variables is an inescapable statistical problem that forms the basis of a large number of applications in machine learning, including rate distortion theory [4], information bottleneck methods [28], population coding [1], curiositydriven exploration [26, 21], model selection [3], and intrinsically-motivated reinforcement learning [22].
We treat the problem of optimizing a function f : X → R given a finite budget of n noisy evaluations.
In this paper, we introduce an unsupervised learning method that fits well with supervised learning.
Semidefinite programming has become a key optimization tool in many areas of applied mathematics, signal processing and machine learning.
Recent advances in object detection are driven by the success of region proposal methods (e.
Principal Component Analysis (PCA) reduces data dimensionality by projecting it onto principal subspaces spanned by the leading eigenvectors of the sample covariance matrix.
Gaussian graphical models (GGMs) form a powerful class of statistical models for representing distributions over a set of variables [1].
Consider test preparation software that tutors students for a national advanced placement exam taken at the end of a year, or maximizing business revenue by the end of each quarter.
Latent Dirichlet Allocation (LDA) [5], among various forms of topic models, is an important probabilistic generative model for analyzing large collections of text corpora.
Normalized random measures (NRMs) form a broad class of discrete random measures, including Dirichlet proccess (DP) [1] normalized inverse Gaussian process [2], and normalized generalized Gamma process [3, 4].
Directed generative models are naturally interpreted as specifying sequential procedures for generating data.
Modeling large-scale multivariate count data is an important challenge that arises in numerous applications such as neuroscience, systems biology and amny others.
Gaussian process (GP) models have become an important component of the machine learning literature.
Nowcasting convective precipitation has long been an important problem in the field of weather forecasting.
Recently a number of methods have been developed for applying Bayesian learning to large datasets.
In numerous machine learning and data analysis applications, the input data are modelled as a matrix A ∈ Rm×n , where m is the number of objects (data points) and n is the number of features.
There is broad interest in learning and exploiting lower-dimensional structure in high-dimensional data.
Some of the recent progress on the theoretical foundations of online learning has been motivated by the parallel developments in the realm of statistical learning.
Neural networks today are achieving state-of-the-art performance in competitions across a range of fields [1][2][3].
Low rank matrix completion is an important topic in machine learning and has been successfully applied to many practical applications [22, 12, 11].
Many network metrics have been introduced to measure the similarity between any two vertices.
Typical multi-class application domains such as natural language processing [1], information retrieval [2], image annotation [3] and web advertising [4] involve tens or hundreds of thousands of classes, and yet these datasets are still growing [5].
Learning features that are able to discriminate is a classical problem in data analysis.
Stochastic gradient descent (SGD) [1] is currently the standard in machine learning for the optimization of highly multivariate functions if their gradient is corrupted by noise.
Online advertisement is currently the fastest growing form of advertising.
A large number of machine learning and signal processing problems are formulated as the minimization of a composite objective function F : Rp → R: n o minp F (x) , f (x) + ψ(x) , (1) x∈R where f is convex and has Lipschitz continuous derivatives with constant L and ψ is convex but may not be differentiable.
You may remember that, on January 15, 2009, in New York City, a commercial passenger plane struck a flock of geese within two minutes of taking off from LaGuardia Airport.
One of the central problems in computational learning theory is the efficient learning of polynomials f (x) : x ∈ {−1, 1}n → R.
Probabilistic graphical models are an elegant framework for reasoning about multiple variables with structured dependencies.
Finding optimal or close-to-optimal policies in large Markov Decision Processes (MDPs) requires the use of approximation.
The last decade has witnessed fast growing attention in research of high-dimensional data: images, videos, DNA microarray data and data from many other applications all have the property that the dimensionality can be comparable or even much larger than the number of samples.
Optimal transport distances (Villani, 2008), a.
Boosting algorithms [21] are ensemble methods that convert a learning algorithm for a base class of models with weak predictive power, such as decision trees, into a learning algorithm for a class of models with stronger predictive power, such as a weighted majority vote over base models in the case of classification, or a linear combination of base models in the case of regression.
Over the past years, advances in adopting methods from algebraic topology to study the “shape” of data (e.
Neural spiking activity recorded from populations of cortical neurons can exhibit substantial variability in response to repeated presentations of a sensory stimulus [1].
Despite the tremendous growth of available data over the past decade, the lack of fully annotated data, which is an essential part of success of any traditional supervised learning algorithm, demands methods that allow good generalization from limited amounts of training data.
Nearest neighbor search is a key algorithmic problem with applications in several fields including computer vision, information retrieval, and machine learning [4].
The problem of data partitioning is of great importance to many machine learning (ML) and data science applications as is evidenced by the wealth of clustering procedures that have been and continue to be developed and used.
Deep neural networks are a flexible family of models that easily scale to millions of parameters and datapoints, but are still tractable to optimize using minibatch-based stochastic gradient ascent.
Good statistical models of populations are often very different from good models of individuals.
Community detection in graphs, also known as graph clustering, is a problem where one wishes to identify subsets of the vertices of a graph such that the connectivity inside the subset is in some way denser than the connectivity of the subset with the rest of the graph.
Many scenarios involve classification systems constrained by measurement acquisition budget.
How should we combine opinions (or beliefs) about hidden truths (or uncertain future events) furnished by several individuals with potentially diverse information sets into a single group judgment for decision or policy-making purposes? This has been a fundamental question across disciplines for a long time (Surowiecki [2005]).
In spite of the many great successes of deep learning, efficient optimization of deep networks remains a challenging open problem due to the complexity of the model calculations, the non-convex nature of the implied objective functions, and their inhomogeneous curvature [6].
Summarizing large data sets using pairwise co-occurrence frequencies is a powerful tool for data mining.
For many problems in information retrieval and learning to rank, the performance of a predictor is evaluated based on the combination of predictions it makes for multiple variables.
Combinatorial optimization [16] has many real-world applications.
Two grand challenges in artificial intelligence research have been to build models that can make multiple computational steps in the service of answering a question or completing a task, and models that can describe long term dependencies in sequential data.
Unsupervised learning seeks to induce good latent representations of a data set.
Subspace clustering was originally proposed to solve very specific computer vision problems having a union-of-subspace structure in the data, e.
Risk-sensitive optimization considers problems in which the objective involves a risk measure of the random cost, in contrast to the typical expected cost objective.
A variety of tasks in machine learning can be formulated in the form of an energy minimization problem, known also as maximum a posteriori (MAP) or maximum likelihood estimation (MLE) inference in an undirected graphical models (related to Markov or conditional random fields).
Statistical classification, a core task in many modern data processing and prediction problems, is the problem of predicting labels for a given feature vector based on a set of training data instances containing feature vectors and their corresponding labels.
In many data-rich domains such as computer vision, neuroscience and social networks consisting of multi-modal and multi-relational data, tensors have emerged as a powerful paradigm for handling the data deluge.
In perceptual decision making participants have to identify a noisy stimulus.
In many applications we are interested in computing similarities between structured objects such as graphs.
Non-linear Measurements.
The success of deep learning is to a large part based on advanced and efficient input representations [1, 2, 3, 4].
In large-scale Bayesian learning, diffusion based sampling methods have become increasingly popular.
The asynchronous parallel optimization recently received many successes and broad attention in machine learning and optimization [Niu et al.
Markov chain Monte Carlo sampling is among the most general methods for probabilistic inference.
Sparse learning with convex regularization has been successfully applied to a wide range of applications including marker genes identification [19], face recognition [22], image restoration [2], text corpora understanding [9] and radar imaging [20].
Prediction algorithms studied in this paper belong to the class of Venn–Abers predictors, introduced in [1].
Parameter estimation of probabilistic models on discrete space is a popular and important issue in the fields of machine learning and pattern recognition.
Many studies and theories in neuroscience posit that high-dimensional populations of neural spike trains are a noisy observation of some underlying, low-dimensional, and time-varying signal of interest.
Variational inference is a computationally efficient approach for approximating posterior distributions.
Causal discovery is the process to identify the causal relationships among a set of random variables.
The majority of available data in modern machine learning applications come in a raw and unlabeled form.
Over the last few years, heuristics for non-convex optimization have emerged as one of the most fascinating phenomena for theoretical study in machine learning.
We give general conditions for the convergence of the EM method for high-dimensional estimation.
Scaling up nonlinear component analysis has been challenging due to prohibitive computation and memory requirements.
Consider the common compressed sensing (CS) model yi = hai , x∗ i + σεi , i = 1, .
Stochastic variational inference has emerged as a promising and flexible framework for performing large scale approximate inference in complex probabilistic models.
The classic multi-armed bandit (MAB) problem, generally attributed to the early work of Robbins (1952), poses a generic online decision scenario in which an agent must make a sequence of choices from a fixed set of options.
Undirected graphical models, or Markov random fields [13], have been extensively studied and applied in fields ranging from computational biology [15, 28], to natural language processing [16, 20] and computer vision [9, 17].
Control of non-linear dynamical systems with continuous state and action spaces is one of the key problems in robotics and, in a broader context, in reinforcement learning for autonomous agents.
Convolutional neural networks (CNNs) (LeCun et al.
The reinforcement learning problem in Markov decision processes (MDPs) involves an agent using its observed rewards to learn an optimal policy that maximizes its expected total reward for a given task.
Given a high-dimensional dataset YD×N = (y1 , .
A firm that relies on the ability to make difficult predictions can gain a lot from a large collection of data.
Gaussian process (GP) regression models have become a standard tool in Bayesian signal estimation due to their expressiveness, robustness to overfitting and tractability [1].
Bandit with mixing arms.
We consider the problem of classification of a binary response given p covariates.
Learning generative models of sequences is a long-standing machine learning challenge and historically the domain of dynamic Bayesian networks (DBNs) such as hidden Markov models (HMMs) and Kalman filters.
Independent Component Analysis refers to a class of methods aiming at recovering statistically independent signals by observing their unknown linear combination.
Inverse optimal control (IOC) [13], also known as inverse reinforcement learning [18, 1] and inverse planning [3], has become a powerful technique for learning to control or make decisions based on expert demonstrations [1, 20].
Markov chain Monte Carlo (MCMC) has become a defacto tool for Bayesian posterior inference.
Neural circuits can be reconstructed by analyzing 3D brain images from electron microscopy (EM).
Encoding signals or building similarity kernels that are invariant to the action of a group is a key problem in unsupervised learning, as it reduces the complexity of the learning task and mimics how our brain represents information invariantly to symmetries and various nuisance factors (change in lighting in image classification and pitch variation in speech recognition) [1, 2, 3, 4].
In undirected graphical models, maximum likelihood learning is intractable in general.
A fundamental primitive in Bayesian learning is the ability to sample from the posterior distribution.
Generative models have become ubiquitous in machine learning and statistics and are now widely used in fields such as bioinformatics, computer vision, or natural language processing.
We consider a general problem that is pervasive in machine learning, namely optimization of an empirical or regularized convex risk function.
The discovery of matched instances in different domains is an important task, which appears in natural language processing, information retrieval and data mining tasks such as finding the alignment of cross-lingual sentences [1], attaching tags to images [2] or text documents [3], and matching user identifications in different databases [4].
In this paper we consider a primal-dual pair of structured convex optimization problems which has in several variants of varying degrees of generality attracted a lot of attention in the past few years in the machine learning and optimization communities [4, 22, 20, 23, 21, 27].
A number of problems in Computer Vision and Machine Learning involve searching for a set of bounding boxes or rectangular windows.
Deep Neural Networks (DNN) have substantially pushed the state-of-the-art in a wide range of tasks, especially in speech recognition [1, 2] and computer vision, notably object recognition from images [3, 4].
Object detection is one of the most foundational tasks in computer vision [21].
There are two roads to an accurate AI system today: (i) gather a huge amount of labeled training data [1] and do supervised learning [2]; or (ii) use crowdsourcing to directly perform the task [3, 4].
As a simple and intuitive representation, the Euclidean space <d has been widely used in various learning tasks.
Neural associative memories with exponential storage capacity and large (potentially linear) fraction of error-correction guarantee have been the topic of extensive research for the past three decades.
Mixture models play a central role in machine learning and statistics, with diverse applications including bioinformatics, speech, natural language, and computer vision.
Learning the structure of a Bayesian network from data is NP-hard [2].
In recent years, sparse and low rank learning has been a hot research topic and leads to a wide variety of applications in signal/image processing, statistics and machine learning.
Bandit convex optimization [11, 5] is the following online learning problem.
Decentralized computation and estimation have many applications in sensor and peer-to-peer networks as well as for extracting knowledge from massive information graphs such as interlinked Web documents and on-line social media.
Statistical model criticism or checking1 is an important part of a complete statistical analysis.
Long Short-Term Memory (LSTM) networks [1, 2] are recurrent neural networks (RNNs) initially designed for sequence processing.
A directed acyclic graph (DAG) G(V, E) defines a partial order on V where u precedes v if there is a directed path from u to v.
Bayesian methods are popular for their success in analyzing complex data sets.
Recurrent Neural Networks (RNNs) have been used for learning functions over sequences from examples for more than three decades [3].
Recently there has been a hike of interest in automatically generating natural language descriptions for images in the research of computer vision, natural language processing, and machine learning (e.
The benefits of using the Stochastic Gradient Descent (SGD) scheme for learning could not be stressed enough.
Deep networks have proven extremely successful across a broad range of applications.
A common task in supervised learning is to select the model that best fits the data.
The goal of disease progression modeling is to learn a model for the temporal evolution of a disease from sequences of clinical measurements obtained from a longitudinal sample of patients.
The recent success of deep feature learning in the supervised setting has inspired renewed interest in feature learning in weakly supervised and unsupervised settings.
Kernel methods provide an elegant and effective framework to develop nonparametric statistical approaches to learning [1].
In a variety of applications, one needs to process data of rich structure that can be conveniently represented by a hypergraph, where associations of the data items, represented by vertices, are represented by hyperedges, i.
Truly intelligent systems can learn and make decisions without human intervention.
Density ridges [10, 22, 15, 6] are one-dimensional curve-like structures that characterize high density regions.
Developing learning algorithms for distributed compositional semantics of words has been a longstanding open problem at the intersection of language understanding and machine learning.
We use the term “active learning” to refer to algorithms that employ adaptive data collection in order to accelerate machine learning.
Online learning in stochastic environments is a sequential decision problem where in each time step a learner chooses an action from a given finite set, observes some random feedback and receives a random payoff.
The quintessential scientific question is whether an unknown object has some property, i.
Trace regression models of the form yi = tr(Xi⊤ Σ∗ ) + εi , i = 1, .
Tractable learning [1] is a promising new machine learning paradigm that focuses on learning probability distributions that support efficient querying.
Orthogonal NMF The success of Nonnegative Matrix Factorization (NMF) in a range of disciplines spanning data mining, chemometrics, signal processing and more, has driven an extensive practical and theoretical study [1, 2, 3, 4, 5, 6, 7, 8].
Empirical risk minimization (ERM) is a domininant framework for supervised machine learning, and a key component of many learning algorithms.
Embedding structured data, such as graphs, in geometric spaces, is a central problem in machine learning.
Online learning is a well-established learning paradigm which has both theoretical and practical appeals.
Over the years, deep learning approaches (see [5, 26] for survey) have shown great success in many visual perception problems (e.
Computationally demanding simulators are used across the full spectrum of scientific and industrial applications, whether one studies embryonic morphogenesis in biology, tumor growth in cancer research, colliding galaxies in astronomy, weather forecasting in meteorology, climate changes in the environmental science, earthquakes in seismology, market movement in economics, turbulence in physics, brain functioning in neuroscience, or fabrication processes in industry.
We start with a general discussion of the tension between sample size and computational efficiency in statistical and learning problems.
Convolutional neural networks (CNNs) [15] are neural networks that can make use of the internal structure of data such as the 2D structure of image data through convolution layers, where each computation unit responds to a small region of input data (e.
Suppose we are given a response vector y = [yi ]1≤i≤m generated from a quadratic transformation of an unknown object x ∈ Rn /Cn , i.
Many modern applications of neural networks have to deal with data represented, or representable, as very large sparse vectors.
Markov chains are one of the workhorses of stochastic modeling, finding use across a variety of applications – MCMC algorithms for simulation and statistical inference; to compute network centrality metrics for data mining applications; statistical physics; operations management models for reliability, inventory and supply chains, etc.
As the machine learning (ML) community continues to accumulate years of experience with live systems, a wide-spread and uncomfortable trend has emerged: developing and deploying ML systems is relatively fast and cheap, but maintaining them over time is difficult and expensive.
One of the most challenging problems in large-scale machine learning is how to parallelize the training of large models that use a form of stochastic gradient descent (SGD) [1].
Principal component analysis (PCA) is a tool for providing a low-rank approximation to a data matrix D 2 Rn⇥d , with the aim of reducing dimension or capturing the main directions of variation in the data.
The goal of visual texture synthesis is to infer a generating process from an example texture, which then allows to produce arbitrarily many new samples of that texture.
Detecting the emergence of abrupt change-points is a classic problem in statistics and machine learning.
Accurate recovery of structured sparse signal/parameter vectors from noisy linear measurements has been extensively studied in the field of compressed sensing, statistics, etc.
Convolutional neural networks, trained end-to-end, have been shown to substantially outperform previous approaches to various supervised learning tasks in computer vision (e.
Max-margin learning has been effective on learning discriminative models, with many examples such as univariate-output support vector machines (SVMs) [5] and multivariate-output max-margin Markov networks (or structured SVMs) [30, 1, 31].
Bayesian nonparametric (BNP) stochastic processes are streaming priors – their unique feature is that they specify, in a probabilistic sense, that the complexity of a latent model should grow as the amount of observed data increases.
What happens when players in a game interact with one another, all of them acting independently and selfishly to maximize their own utilities? If they are smart, we intuitively expect their utilities — both individually and as a group — to grow, perhaps even to approach the best possible.
Statistical learning and stochastic optimization with exp-concave loss functions captures several fundamental problems in statistical machine learning, which include linear regression, logistic regression, learning support-vector machines (SVMs) with the squared hinge loss, and portfolio selection, amongst others.
Recurrent neural networks (RNNs) are powerful tools for modeling sequential data, yet training them by back-propagation through time [37, 27] can be difficult [9].
Clustering items according to some notion of similarity is a major primitive in machine learning.
Generalized Linear Models (GLMs) play a crucial role in numerous statistical and machine learning problems.
Multi-label classification, where each instance can belong to multiple labels simultaneously, has significantly attracted the attention of researchers as a result of its various applications, ranging from document classification and gene function prediction, to automatic image annotation.
In recommendation systems and revenue management, it is important to predict preferences on items that have not been seen by a user or predict outcomes of comparisons among those that have never been compared.
Principal components analysis constructs a low dimensional subspace of the data such that projection of the data onto this subspace preserves as much information as possible (or equivalently maximizes the variance of the projected data).
Recurrent neural networks (RNNs) offer a compelling tool for processing natural language input in a straightforward sequential manner.
Decision making within the Markov decision process (MDP) framework typically involves the minimization of a risk-neutral performance objective, namely the expected total discounted cost [3].
Multi-party computation (MPC) is a generic framework where multiple parties share their information in an interactive fashion towards the goal of computing some functions, potentially different at each of the parties.
In recent years, there has been an increasing appreciation of the shared mathematical foundations between prediction markets and a variety of techniques in machine learning.
Due to the development of advanced warning systems, cameras are available onboard of almost every new car produced in the last few years.
Logistic regression is one of the most frequently used classification methods [1].
Extensive-form imperfect-information games are a general model for strategic interaction.
Dirichlet process mixture models (DPMM) have been widely used for clustering data Neal (1992); Rasmussen (2000).
The computational burden of solving high dimensional regularized regression problem has lead to a vast literature in the last couple of decades to accelerate the algorithmic solvers.
Personalized medicine has long been a critical application area for machine learning [1–3], in which automated decision making and diagnosis are key components.
In time series prediction, tracking, and filtering problems, a learner sees a stream of (possibly noisy, vector-valued) data and needs to predict the future path.
Machine learning applications often require efficient statistical procedures to process potentially massive amount of high dimensional data.
Recent work in materials design used neural networks to predict the properties of novel molecules by generalizing from examples.
Graphical models such as Bayesian networks, Markov random fields and deep generative models provide a powerful framework for reasoning about complex dependency structures over many variables [see e.
Deep models, understood as multilayer modular networks, have been gaining significant interest from the machine learning community, in part because of their ability to obtain state-of-the-art performance in a wide variety of tasks.
Inference tasks encountered in natural language processing, graph inference and manifold learning employ the singular value decomposition (SVD) as a first step to reduce dimensionality while retaining useful structure in the input.
Diffusion networks capture the underlying mechanism of how events propagate throughout a complex network.
In machine learning applications, direct sampling with the entire large-scale dataset is computationally infeasible.
Computer-assisted education promises open access to world class instruction and a reduction in the growing cost of learning.
A central problem in systems neuroscience is to build flexible and accurate models of the sensory encoding process.
We consider minimization of functions of form n X  −1 φi x⊤ P (w) = n i w + R (w) i=1 where the convex φi corresponds to a loss of w on some data xi , R is a convex regularizer and P is µ strongly convex, so that P (w′ ) ≥ P (w) + hw′ − w, ▽P (w)i + µ2 kw′ − wk2 .
The task of inferring a hidden dynamic state based on partial noisy observations plays an important role within both applied and natural domains.
Semantic segmentation is a technique to assign structured semantic labels—typically, object class labels—to individual pixels in images.
A myriad of probabilistic logic languages have been proposed in recent years [5, 12, 17].
In machine learning and related areas we often need to optimise multiple performance measures, such as per-class classification accuracies, precision and recall in information retrieval, etc.
Structured output prediction has been an important topic in machine learning.
Graphical Models (GMs) provide a useful representation for reasoning in a number of scientific disciplines [1, 2, 3, 4].
Non-linear vector-valued transforms of the form, f (x, M) = s(Mx), where s is an elementwise nonlinearity, x is an input vector, and M is an m × n matrix of parameters are building blocks of complex deep learning pipelines and non-parametric function estimators arising in randomized kernel methods [20].
Splitting methods such as ADMM [1, 2, 3] have recently become popular for solving problems in distributed computing, statistical regression, and image processing.
Differential Privacy.
Developing automated yet practical approaches to Bayesian inference is a problem that has attracted considerable attention within the probabilisitic machine learning community (see e.
We study inference on factor graphs using Gibbs sampling, the de facto Markov Chain Monte Carlo (MCMC) method [8, p.
Generalization of bounded policy iteration (BPI) to ﬁnitely-nested interactive partially observable Markov decision processes (I-POMDP) [1] is currently the leading method for inﬁnite-horizon selfinterested multiagent planning and obtaining ﬁnite-state controllers as solutions.
In the past a few years, deep learning has been very successful in addressing many aspects of visual perception problems such as image classification, object detection, face recognition [1, 2, 3], to name a few.
Policy gradient algorithms maximize the expectation of cumulative reward by following the gradient of this expectation with respect to the policy parameters.
Decision trees and forests [5, 21, 4] have a long and rich history in machine learning [10, 7].
The hidden Markov model (HMM) [1, 2] is widely used to segment sequential data into interpretable discrete states.
Subset selection is to select a subset of size k from a total set of n variables for optimizing some criterion.
We are interested in the problem of learning from intractable supervision.
Classical supervised learning problems, such as binary and multiclass classification, share a number of characteristics.
In the 19th century, Helmholtz proposed that perception could be understood as unconscious inference [1].
Syntactic constituency parsing is a fundamental problem in linguistics and natural language processing that has a wide range of applications.
We consider the problem of learning to predict a non-negative measure over a finite set.
Multidimensional recurrent neural networks (MDRNNs) constitute an efficient architecture for building a multidimensional context into recurrent neural networks [1].
Deep learning has led to remarkable breakthroughs in learning hierarchical representations from images.
Visual systems have perfected the art of sensing through billions of years of evolution.
The last few years have seen convolutional neural networks (CNNs) emerge as an indispensable tool for computer vision.
Hierarchical clustering is an important method in cluster analysis where a data set is recursively partitioned into clusters of successively smaller size.
Our visual system is remarkably fast and accurate.
Many datasets in contemporary scientific applications possess some form of network structure [20].
A wide variety of research disciplines, including computer science, economic, biology and social science, involve causality analysis of a network of interacting random processes.
The multi-armed bandit (MAB) problem is perhaps the simplest example of a learning problem that exposes the tension between exploration and exploitation.
Boosting and support vector machines (SVM) are widely popular techniques for learning classifiers.
Time series, such as neural recordings, economic observations and biological imaging movies, are ubiquitous, containing rich information about the temporal patterns of physical quantities under certain conditions.
In recent years, convolutional neural networks (CNNs) have achieved great success to solve many problems in machine learning and computer vision.
Low rank matrix completion refers to the problem of recovering a low rank matrix by observing the values of only a tiny fraction of its entries.
In recent years, there have been many exciting advances in building an artificial agent, which can be trained with one learning algorithm, to solve many relatively large-scale, complicated tasks (see, e.
Human-decision making involves decomposing a task into a course of action.
(Weighted) Minwise Hashing (or Sampling), [2, 4, 17] is the most popular and successful randomized hashing technique, commonly deployed in commercial big-data systems for reducing the computational requirements of many large-scale applications [3, 1, 25].
In recent years, deep neural networks have been shown to perform extremely well on a variety of tasks including classification [21], semantic segmentation [13], machine translation [27] and speech recognition [16].
The functions of the brain likely rely on the concerted interaction of its microscopic, mesoscopic and macroscopic systems.
We study the reinforcement learning (RL) problem where an agent interacts with an unknown environment.
The following optimization problem, which minimizes the sum of cost functions over samples from a finite training set, appears frequently in machine learning: n 1X fi (x), min F (x) ≡ n i=1 (1) where n is the sample size, and each fi : Rd → R is the cost function corresponding to the i-th sample data.
Recent successes in deep learning have shown that neural networks trained by first-order gradient based optimization are capable of achieving amazing results in diverse domains like computer vision, speech recognition, and language modelling [7].
Learning and anticipation are central features of cerebellar computation and function (Bastian, 2006): the cerebellum learns from experience and is able to anticipate events, thereby complementing a reactive feedback control by an anticipatory feed-forward one (Hofstoetter et al.
Word embeddings are a powerful approach for analyzing language (Bengio et al.
Distance metric learning aims to learn an embedding representation of the data that preserves the distance between similar data points close and dissimilar data points far on the embedding space [15, 30].
Optimization of convex functions over a convex domain is a well studied problem in machine learning, where a variety of algorithms exist to solve the problem efficiently.
More and more data for machine learning nowadays are acquired from distributed, unmonitored and strategic data sources and the quality of these collected data is often unverifiable.
Collaborative preference completion is the task of jointly learning bipartite (or dyadic) preferences of set of entities for a shared list of items, e.
A simulator-based model is a data-generating process described by a computer program, usually with some free parameters we need to learn from data.
Modeling long-term behavior is a key challenge in many learning problems that require complex decision-making.
Interest in recurrent neural networks (RNNs) has greatly increased in recent years, since larger training databases, more powerful computing resources, and better training algorithms have enabled breakthroughs in both processing and modeling of temporal sequences.
The multivariate normal distribution is a fundamental building block in many machine learning algorithms, and its well-known density can compactly be written as   1 2 (1) p(x | µ, Σ) ∝ exp − distΣ (µ, x) , 2 where dist2Σ (µ, x) denotes the Mahalanobis distance for covariance matrix Σ.
The rate with which a learning algorithm converges as more data comes in play a central role in machine learning.
Traditionally, machine learning is concerned with predictions: assuming data is generated from some model, the goal is to predict the behavior of the model on data similar to that observed.
Tensor modeling is widely used for capturing the higher order relations between several data sources.
Recurrent neural networks (RNNs) have been shown to achieve promising results on many difficult sequential learning problems [1, 2, 3, 4, 5].
The use of deep feedforward neural networks in machine learning applications has become widespread and has drawn considerable research attention in the past few years.
Consider problems where we need to adaptively make a sequence of decisions while taking into account the outcomes of previous decisions.
The problem of establishing maps (e.
Many of machine learning’s successes have come from supervised learning, which typically involves employing annotators to label large quantities of data per task.
We are in the climax of driverless vehicles research where the perception and learning are no longer trivial problems due to the transition from controlled test environments to real world complex interactions with other road users.
With the availability of cheap computing power, modern cameras can rely on computational postprocessing to extend their capabilities under the physical constraints of existing sensor technology.
We propose the following model for multi-way graph partitioning.
Clustering is an important problem which is prevalent in a variety of real world problems.
The optimization of an unknown function based on noisy observations is a fundamental problem in various real world domains, e.
Stochastic variational inference (Blei et al.
Decision making with partial feedback, motivated by applications including personalized medicine [21] and content recommendation [16], is receiving increasing attention from the machine learning community.
Recently, deep convolutional neural networks [17, 26, 30] have propelled unprecedented advances in artificial intelligence including object recognition, speech recognition, and image captioning.
Every supervised learning algorithm with the ability to generalize from training examples to unseen data points has some type of inductive bias [5].
Variational inference is an umbrella term for algorithms that cast Bayesian inference as optimization [10].
In this paper, we propose a general framework for classification of sparse and irregularly-sampled time series.
The covariance matrix adaptation evolution strategy, CMA-ES [Hansen and Ostermeier, 2001], is recognized as one of the most competitive derivative-free algorithms for real-valued optimization [Beyer, 2007; Eiben and Smith, 2015].
In recent years, tensor decomposition has emerged as a powerful tool to solve many challenging problems in unsupervised [1], supervised [18] and reinforcement learning [4].
Perception problems rarely exist in a vacuum.
Deep neural networks (DNN), especially deep Convolutional Neural Networks (CNN), made remarkable success in visual tasks [1][2][3][4][5] by leveraging large-scale networks learning from a huge volume of data.
For high-dimensional structured estimation problems [3, 15], considerable advances have been made in accurately estimating a sparse or structured parameter θ ∈ Rp even when the sample size n is far smaller than the ambient dimensionality of θ, i.
We consider the problem of sampling-based planning in a Markov decision process (MDP) when a generative model (oracle) is available.
Canonical correlation analysis (CCA, [1]) and its extensions are ubiquitous techniques in scientiﬁc research areas for revealing the common sources of variability in multiple views of the same phenomenon.
Clustering is a fundamental task in machine learning that aims to assign closely related entities to the same group.
Visual Question Answering (VQA) [2, 6, 14, 15, 27] has emerged as a prominent multi-discipline research problem in both academia and industry.
Recommendation systems have emerged as a crucial feature of many electronic commerce systems.
Gibbs sampling, or Glauber dynamics, is a Markov chain Monte Carlo method that draws approximate samples from multivariate distributions that are difficult to sample directly [9; 15, p.
Area Under the ROC Curve (AUC) [8] is a widely used metric for measuring classification performance.
Probabilistic programming systems (PPS) allow probabilistic models to be represented in the form of a generative model and statements for conditioning on data [4, 9, 10, 16, 17, 29].
Recurrent neural networks (RNNs) are able to represent long-term dependencies in sequential data, by adapting and propagating a deterministic hidden (or latent) state [5, 16].
Determinantal Point Processes (DPPs) are discrete probability models over the subsets of a ground set of N items.
Reinforcement learning (RL) studies how an agent can maximize its cumulative reward in a previously unknown environment, which it learns about through experience.
Similarities measure the closeness of connections between objects and usually are reflected by distances.
Humans naturally perceive the world as being structured into different objects, their properties and relation to each other.
Numerical solvers for differential equations are essential tools in almost all disciplines of applied mathematics, due to the ubiquity of real-world phenomena described by such equations, and the lack of exact solutions to all but the most trivial examples.
Classification with abstention is a key learning scenario where the algorithm can abstain from making a prediction, at the price of incurring a fixed cost.
In Bayesian optimization [19] (BO), we wish to optimize a derivative-free expensive-to-evaluate function f with feasible domain A ⊆ Rd , min f (x), x∈A with as few function evaluations as possible.
Generating realistic images from informal descriptions would have a wide range of applications.
The trade-off between exploration and exploitation has been an ever-present trope in the online learning literature.
Recurrent neural networks (RNNs) have become to be the generative models of choice for sequential data (Graves, 2012) with impressive results in language modeling (Mikolov, 2010; Mikolov and Zweig, 2012), speech recognition (Bahdanau et al.
Probabilistic inference in high-order discrete graphical models has been an ongoing computational challenge, and all existing methods rely on exploiting specific structure: either low-treewidth or pairwise graphical models, or functional properties of the distribution such as log-submodularity.
Practical data sets generally have missing or corrupted entries.
Recurrent neural networks, such as the Long Short-Term Memory (LSTM) [11], have proven to be powerful sequence learning models [6, 18].
Consider the following nonconvex and nonsmooth constrained optimization problem N 1 X gi (z) + g0 (z) + p(z), min f (z) := z∈Z N i=1 (1.
With the expansion of online social platforms, user-generated content has become increasingly influential.
Bayesian optimization (BO), as applied to so-called blackbox objectives, is a modernization of 197080s statistical response surface methodology for sequential design [3, 14].
Procedural modeling, or the use of randomized procedures to generate computer graphics, is a powerful technique for creating visual content.
Representing and reasoning about objects, relations and physics is a “core” domain of human common sense knowledge [25], and among the most basic and important aspects of intelligence [27, 15].
In the last two decades, a collection of highly related dynamic models including observable operator models (OOMs) [1–3], predictive state representations [4–6] and reduced-rank hidden Markov models [7, 8], have become powerful and increasingly popular tools for analysis of dynamic data.
k-means++ (Arthur & Vassilvitskii, 2007) is one of the most widely used methods to solve k-Means clustering.
An essential element of supervised learning systems is the representation of input data.
Parallel optimization algorithms often feature synchronization steps: all processors wait for the last to finish before moving on to the next major iteration.
There is a wide range of problems in applied machine learning from web data mining [1] to protein function prediction [2] where the input space is a space of graphs.
GMs express factorization of the joint multivariate probability distributions in statistics via graph of relations between variables.
As machine learning becomes more widely adopted in security and in security-sensitive tasks, it is important to consider what happens when some aspect of the learning process or the training data is compromised [1–4].
Depth from a single RGB image is a fundamental problem in vision.
Since its early beginning [24, 34], the PAC-Bayesian theory claims to provide “PAC guarantees to Bayesian algorithms” (McAllester [24]).
In recent years Optimal Transport (OT) [1] has received a lot of attention in the machine learning community [2, 3, 4, 5].
Action recognition in video is an intensively researched area, with many recent approaches focused on application of Convolutional Networks (ConvNets) to this task, e.
Social media and social networking sites are increasingly used by people to express their opinions, give their “hot takes”, on the latest breaking news, political issues, sports events, and new products.
High-dimensional data, which are ubiquitous in computer vision, image processing, bioinformatics and social networks, often lie in low-dimensional subspaces corresponding to different categories they belong to [1, 2, 3, 4, 5, 6].
“If we use, to achieve our purposes, a mechanical agency with whose operation we cannot interfere effectively .
Combining the outputs of multiple predictors is in many cases of interest a successful strategy to improve the capabilities of artificial intelligence systems, ranging from agent architectures [19], to committee learning [13, 15, 8, 9].
Timely clinical state estimation can significantly improve the quality of care for patient’s by informing clinicians of patient’s that have entered a high-risk clinical state.
Restricted Boltzmann machines (RBMs) are two-layer latent variable models that use a layer of hidden units h to model the distribution of visible units v [Smolensky, 1986, Hinton, 2002].
Scalable optimization methods are critical for many machine learning applications.
The Multi-Armed Bandit (MAB) game is one where in each round the player chooses an action, also referred to as an arm, from a pre-determined set.
Latent Dirichlet Allocation (LDA) [3] recently emerged as the dominant framework for topic modeling as well as many other applications with latent groups.
The classical Multi-armed Bandit (MAB) problem provides a framework to reason about sequential decision settings, but specifically where the learner’s chosen decision is intimately tied to information content received as feedback.
Arithmetic circuits (ACs) have been a central representation for probabilistic graphical models, such as Bayesian networks and Markov networks.
Deep generative models (DGMs) characterize the distribution of observations with a multilayered structure of hidden variables under nonlinear transformations.
In this paper, we provide a statistical framework for performing nonparametric regression over latent variable models.
There is currently a wide gap between theory and practice of active learning with oracle interaction.
Typical inference queries to make predictions and learn probabilistic models from data include the maximum a posteriori (MAP) inference task, which computes the most likely assignment of a set of variables, as well as the marginal inference task, which computes the probability of an event according to the model.
We consider the Streaming Submodular Cover (SSC) problem, where we seek to find the smallest subset that achieves a certain utility, as measured by a monotone submodular function.
Approximate inference, that is approximating posterior distributions and likelihood functions, is at the core of modern probabilistic machine learning.
Modeling nonlinear dynamical systems using data is fundamental in a variety of engineering and scientific fields.
Numerical integration, or quadrature, is a fundamental task in the construction of various statistical and machine learning algorithms.
Algorithm design often requires making simplifying assumptions about the input data.
Structured prediction has become prevalent with wide applications in Natural Language Processing (NLP), Computer Vision, and Bioinformatics to name a few, where one is interested in outputs of strong interdependence.
Consider a binary classification problem, in which we are given an ensemble of individual classifiers to aggregate into the most accurate predictor possible for data falling into two classes.
The task of image restoration is to recover a clean image from its corrupted observation, which is known to be an ill-posed inverse problem.
Given a large collection of text data, e.
The k-means problem is to find k centroids to minimise the mean distance between samples and their nearest centroids.
Joint matrix decomposition problems appear frequently in signal processing and machine learning, with notable applications in independent component analysis [7], canonical correlation analysis [20], and latent variable model estimation [5, 4].
Perhaps the most common way to sell items is using a “posted price” mechanism in which the seller publishes the price of an item in advance, and buyers that wish to obtain the item decide whether to acquire it at the given price or to forgo the purchase.
Learning programs from examples is a central problem in artificial intelligence, and many recent approaches draw on techniques from machine learning.
Recurrent neural networks (RNNs) are artificial neural networks where connections between units can form cycles.
Today’s robots are required to operate in variable and often unknown environments.
Our brain perceives the external world with multiple sensory modalities, including vision, audition, olfaction, tactile, vestibular perception and so on.
Tensors, a.
A fundamental goal of sensory neuroscience involves building accurate neural encoding models that predict the response of a sensory area to a stimulus of interest.
Following the seminal work of H OGWILD ! [17], many studies have demonstrated that near linear speedups are achievable on several machine learning tasks via asynchronous, lock-free implementations [25, 13, 8, 16].
Word embeddings are dense vector representations of words with semantic and relational information.
Kernel methods are widely used in nonlinear learning [8], but they are computationally expensive for large datasets.
Natural perception can extract complete interpretations of sensory data in a coherent and efficient manner.
Discrete choice models describe and predict decisions between distinct alternatives.
Frequently, tasks in machine learning can be expressed as the problem of optimizing an objective function f (✓) defined over some domain ✓ 2 ⇥.
Knowledge of the underlying parameters of the spreading model is crucial for understanding the global properties of the dynamics and for development of effective control strategies for an optimal dissemination or mitigation of diffusion [1, 2].
Partial monitoring (PM) games are repeated games played between a learner and an adversary over discrete time points.
Many problems in artificial intelligence (AI) and machine learning (ML) involve designing agents that interact with stochastic environments.
Cleaning noise-corrupted data, i.
As an important class of statistical models for exploring the interrelationship among a large number of random variables, undirected graphical models (UGMs) have enjoyed popularity in a wide range of scientific and engineering domains, including statistical physics, computer vision, data mining, and computational biology.
We consider minimizing the average of m ( 2 convex functions: ) m 1 X min F (x) := fi (x) x2X m i=1 (1) where X ✓ Rd is a closed, convex set, and where the algorithm is given access to the following gradient (or subgradient in the case of non-smooth functions) and prox oracle for the components: ⇥ ⇤ hF (x, i, ) = fi (x), rfi (x), proxfi (x, ) (2) where ⇢ proxfi (x, ) = arg min fi (u) + u2X 2 kx uk 2 (3) A natural question is how to leverage the prox oracle, and how much benefit it provides over gradient access alone.
Using reinforcement learning to train neural network controllers has recently led to rapid progress on a number of challenging control tasks [15, 17, 26].
Clustering and the closely related problem of vector quantization are fundamental problems in machine learning and data mining.
In recent years, Deep Neural Networks (DNNs), especially Convolutional Neural Networks (CNNs), have demonstrated highly competitive results on object recognition and image classification [1, 2, 3, 4].
The mixed linear regression (MLR) [7, 9, 29] models each observation as being generated from one of the K unknown linear models; the identity of the generating model for each data point is also unknown.
Policy search algorithms based on supervised learning from a computational or human “teacher” have gained prominence in recent years due to their ability to optimize complex policies for autonomous flight [16], video game playing [15, 4], and bipedal locomotion [11].
Two-photon calcium imaging is a powerful technique for monitoring the activity of thousands of individual neurons simultaneously in awake, behaving animals [1, 2].
Markov chains are a simple and incredibly rich tool for modeling, and act as a backbone in numerous applications—from Pagerank for web search to language modeling for machine translation.
Gaussian graphical models describe well interactions in many real-world systems.
Regret analysis is a general technique for designing and analyzing algorithms for sequential decision problems in adversarial or stochastic settings (Shalev-Shwartz, 2012; Bubeck and Cesa-Bianchi, 2012).
In this paper, we investigate a new approach to reducing supervised learning to game playing.
Let X ∈ <p and Y ∈ <q be random vectors, where p and q are positive integers.
For solving a broad range of large-scale statistical learning problems, e.
In stochastic bandit optimisation, we wish to optimise a payoff function f : X → R by sequentially querying it and obtaining bandit feedback, i.
In classical statistical inference, we are typically interested in characterizing how more data points improve the accuracy, with little restrictions or considerations on computational aspects of solving the inference problem.
Deep embedding methods aim at learning a compact feature embedding f (x) ∈ Rd from image x using a deep convolutional neural network (CNN).
Deep networks [1, 2] continue to post impressive successes in a wide range of tasks, and the Rectified Linear Unit (ReLU) [3, 4] is arguably the most used simple nonlinearity.
Function learning underlies many intuitive judgments, such as the perception of time, space and number.
The recent success of supervised learning algorithms has been partially attributed to the large-scale datasets [16, 22] on which they are trained.
Proteins perform most of the functions in the cells of living organisms, acting as enzymes to perform complex chemical reactions, recognizing foreign particles, conducting signals, and building cell scaffolds – to name just a few.
Breakthroughs in modern technology have allowed more sequential data to be collected in higher resolutions.
A common task in probabilistic modelling is to compute the distribution of f (X), given a measurable function f and a random variable X.
Principal component analysis (PCA) aims to find a low rank subspace that best-approximates a data matrix Y ∈ Rd1 ×d2 .
Recurrent neural networks (RNNs) are sequence-based models of key importance for natural language understanding, language generation, video processing, and many other tasks [1–3].
Numerous problems in data analysis are formulated as the question of embedding high-dimensional metric spaces into “simpler" spaces, typically of lower dimension.
Asynchronous parallel optimization has recently become a popular way to speedup machine learning algorithms using multiple processors.
In biomedical image analysis, a fundamental problem is the segmentation of 3D images, to identify target 3D objects such as neuronal structures [1] and knee cartilage [15].
Convolutionnal Neural Networks (CNN) have been very successful for many different tasks in computer vision.
Clustering is a fundamental task in machine learning with widespread applications in data mining, computer vision, and social network analysis.
Demand forecasting plays a central role in supply chain management, driving automated ordering, in-stock management, and facilities planning.
One of the fundamental tasks in statistical learning is probability estimation.
Given a proper convex cone K ⊂ Rn , let ψ : K 7→ R be an upper semi-continuous concave function.
Object detection, tracking, and motion prediction are fundamental problems in computer vision, and predicting the effect of physical interactions is a critical challenge for learning agents acting in the world, such as robots, autonomous cars, and drones.
Since the seminal work of Robbins [11], the multi-armed bandit has become an attractive framework for studying exploration-exploitation trade-offs inherent to tasks arising in online advertising, finance and other fields.
During their browsing experience, users are constantly provided – without having asked for it – with clickable content spread over web pages.
What is clutter? While it seems easy to make sense of a cluttered desk vs an uncluttered desk at a glance, it is hard to quantify clutter with a number.
Optimizing an objective function is a central component of many algorithms in machine learning and engineering.
It is now well-established that principal component analysis (PCA) is quite sensitive to outliers, with even a single corrupted data element carrying the potential of grossly biasing the recovered principal subspace.
A key reason for the success of graphical models is the existence of fast algorithms that exploit the graph structure to perform inference, such as Pearl’s belief propagation [19] and related propagation algorithms [13, 16, 23] (which we refer to collectively as “message passing” algorithms), and variable elimination [27].
Many real-world networks contain subsets of variables densely connected to one another, a property called modularity (Fig 1A); however, standard network inference methods do not incorporate this property.
Cortical regions in the brain are anatomically connected, and the joint neural activity in connected regions are believed to underlie various perceptual and cognitive functions.
Matrix completion has been a basis of many machine learning approaches for computer vision [6], recommender systems [21, 24], signal processing [19, 27], and among many others.
Research on word embeddings has drawn significant interest in machine learning and natural language processing.
In many important prediction problems from different areas of application (medicine, environmental monitoring, etc.
Mathematics underpins all scientific disciplines.
Over the past decade, the notion of embedding probability measures in a Reproducing Kernel Hilbert Space (RKHS) [1, 13, 18, 17] has gained a lot of attention in machine learning, owing to its wide applicability.
Supervised learning, the task of inferring a function that predicts a target Y from a feature vector X = (X1 , .
In this paper we propose a non-parametric pool-based active learning algorithm for general metric spaces, which outputs a nearest-neighbor classifier.
Many applications require a predictor to make coherent decisions.
The high computational complexity makes kernel methods unfeasible to deal with large-scale data.
Obama was the first US president in history who successfully leveraged online social media in presidential campaigning, which has been popularized and become a ubiquitous approach to electoral politics (such as in the on-going 2016 US presidential election) in contrast to the decreasing relevance of traditional media such as TV and newspapers [1, 2].
Visual question answering (VQA) is a new research direction as intersection of computer vision and natural language processing.
State-of-the-art machine translation (MT) systems, including both the phrase-based statistical translation approaches [6, 3, 12] and the recently emerged neural networks based translation approaches [1, 5], heavily rely on aligned parallel training corpora.
Knowledge bases are attracting considerable interest both from industry and academia [2, 6, 15, 10].
The efficient reduction of a constrained convex optimization problem to a constrained linear optimization problem is an appealing algorithmic concept, in particular for large-scale problems.
Based on the softmax representation, the probability of a variable y to take the value k ∈ {1, .
Working with structured data is challenging.
Recent advances in image modelling with neural networks [30, 26, 20, 10, 9, 28, 6] have made it feasible to generate diverse natural images that capture the high-level structure of the training data.
Modeling non-stationary temporal data sources is a fundamental problem in signal processing, statistical data compression, quantitative finance and model-based reinforcement learning.
Mapping neuroanatomy, in the pursuit of linking hypothesized computational models consistent with observed functions to the actual physical structures, is a long-standing fundamental problem in neuroscience.
A Graphical Model (GM) describes a probability distribution over a set of random variables which factorizes over the edges of a graph.
In everyday life we constantly face tasks we must perform in the presence of sensory uncertainty.
A first order requirement in many estimation tasks is that the training and testing samples are from the same underlying distribution and the associated features are directly comparable.
Large-scale recording technologies are revolutionizing the field of neuroscience [e.
Human decision-making is not perfectly rational.
Humans exhibit impressive abilities of intercepting moving targets as exemplified in sports such as baseball [6].
Online learning is a sequential decision-making problem where learner repeatedly chooses an action in response to adversarially chosen losses for the available actions.
Access to positive, negative and unlabeled examples is a standard assumption for most semisupervised binary classification techniques.
Recent work [21] shows that it is often possible to construct an input mislabeled by a neural net by perturbing a correctly labeled input by a tiny amount in a carefully chosen direction.
Complex networks emerge in a plethora of disciplines including computer science, social sciences, biology and etc.
A fundamental problem in network science and machine learning is to discover structures in large, complex networks (e.
Let G = (N , E) denote a connected undirected graph of N computing nodes, where N , {1, .
Visual spatial attention refers to the narrowing of processing in the brain to particular objects in particular locations so as to mediate everyday tasks.
Monte Carlo methods are the gold standard in Bayesian posterior inference due to their asymptotic convergence properties; however convergence can be slow in large models due to poor mixing.
Many machine learning applications involve finding the minimizer of optimization problems of the form n X min F (w) := fi (w) + R(w) (1) w∈C i=1 where fi (w) is a smooth convex function, R(w) is a regularizer, and C ⊆ Rd is a convex constraint set (e.
Linear models are one of the foundations of modern machine learning due to their strong learning guarantees and efficient solvers [Koltchinskii, 2011].
As a central optimization problem with a wide variety of applications, online resource allocation problems have attracted a large body of research in networking, distributed computing, and electronic commerce.
Tensors, or multidimensional arrays, are generalizations of matrices (from binary interactions) to high-order interactions between multiple entities.
High dimensional problems where the regressor belongs to a small number of groups play a critical role in many machine learning and signal processing applications, such as computational biology and multitask learning.
This work proposes a coding-theory inspired computation technique for speeding up computing linear transforms of high-dimensional data by distributing it across multiple processing units that compute shorter dot products.
What kind of data should we use to train a supervised learner ? A recent result has shown that minimising the popular logistic loss over examples with linear classifiers (in supervised learning) is equivalent to the minimisation of the exponential loss over sufficient statistics about the class known as Rademacher observations (rados, [Nock et al.
Collecting data from non-expert workers on crowdsourcing platforms such as Amazon Mechanical Turk, Zooinverse, Planet Hunters, etc.
1.
Over the past decade deep learning has achieved significant advances in many application areas [1].
We present algorithms for stochastic structured prediction under bandit feedback that obey the following learning protocol: On each of a sequence of iterations, the learner receives an input, predicts an output structure, and receives partial feedback in form of a task loss evaluation of the predicted structure.
Open-ended learning theory in cognitive psychology has been a topic of considerable interest for many researchers.
Estimating entropies and divergences of probability distributions in a consistent manner is of importance in a number of problems in machine learning.
Methods for online convex optimization (OCO) [28, 12] make it possible to optimize parameters sequentially, by processing convex functions in a streaming fashion.
Factorization machines (FMs) [13, 14] are a supervised learning approach that can use second-order feature combinations efficiently even when the data is very high-dimensional.
Many machine learning applications require dealing with data-sets having complex structures, e.
Sparse Regularization This paper studies sparse linear regression problems of the form y = Φx0 + w, where x0 ∈ Rn is the unknown vector to estimate, supposed to be non-zero and sparse, w ∈ Rm is some additive noise and the design matrix Φm×n is in general rank deficient corresponding to a noisy underdetermined linear system of equations, i.
The k-nearest neighbors (k-NN) algorithm [1, 2], and Nadarays-Watson estimation [3, 4] are the cornerstones of non-parametric learning.
The power of joint learning in multiple tasks arises from the transfer of relevant knowledge across said tasks, especially from information-rich tasks to information-poor ones.
Quantum computation is an emerging technology that utilizes quantum effects to achieve significant, and in some cases exponential, speed-ups of algorithms over their classical counterparts.
The popular stochastic gradient methods are well suited for minimizing expected-value objective functions or the sum of a large number of loss functions.
A fundamental challenge in understanding sensory data is learning to disentangle the underlying factors of variation that give rise to the observations [1].
The Atari Reinforcement Learning research program [21] has highlighted a critical deficiency of practical reinforcement learning algorithms in settings with rich observation spaces: they cannot effectively solve problems that require sophisticated exploration.
Density estimation is one of the fundamental problems in statistics.
Energy efficiency is becoming one of the most important issues in our society.
In many statistical inference problems, the task is to detect, from given data, a global structure such as low-rank structure or clustering.
Thanks to the large amount of accessible training data and computational power of GPUs, deep learning models, especially convolutional neural networks (CNNs), have been successfully applied to various computer vision (CV) applications such as image classification [19], human face verification [20], object recognition, and object detection [7, 17].
Many problems in science and engineering can be formulated as a sequential decision-making problem under uncertainty.
Asynchronous parallel optimization received substantial successes and extensive attention recently, for example, [5, 25, 31, 33, 34, 37].
Although the study of neural connectivity is over a century old, starting with pioneering neuroscientists who identified the importance of networks for determining brain function, most knowledge of anatomical neural network structure is limited to either detailed description of small subsystems [2, 9, 14, 26] or to averaged connectivity between larger regions [7, 21].
Many of the major machine learning breakthroughs of the last decade have been catalyzed by the release of a new labeled training dataset.
Understanding object motions and scene dynamics is a core problem in computer vision.
Gaussian Processes (GPs) [1] are a flexible class of probabilistic models.
Convolutional neural networks (CNNs) [1] are effective tools for image analysis [2], with most CNNs trained in a supervised manner [2, 3, 4].
Document distances are a key component of many text retrieval tasks such as web-search ranking [24], book recommendation [16], and news categorization [25].
Many modern classification systems, including internet applications (such as web-search engines, recommendation systems, and spam filtering) and security & surveillance applications (such as widearea surveillance and classification on large video corpora), face the challenge of prediction-time budget constraints [21].
Many problems in computational sciences require to compare probability measures or histograms.
Most modern computer vision systems follow a familiar architecture, processing inputs from lowlevel features up to task specific high-level features.
Unsupervised learning is the task of learning structure from unlabelled examples.
Probabilistic inference is one of the main building blocks for decision making under uncertainty.
The basic machine learning problem of minimizing a regularizer plus a loss function comes in numerous different variations and names.
In the past years, deep neural networks such as convolutional or recurrent ones have become highly popular for solving various prediction problems, notably in computer vision and natural language processing.
Modeling often has two goals: first, to learn a flexible representation of complex high-dimensional data, such as images or speech recordings, and second, to find structure that is interpretable and generalizes to new tasks.
Humans learn new concepts with very little supervision – e.
A long tradition of research in social psychology recognizes volunteering as the hallmark of human altruistic action, aimed at improving the survival of a group of individuals living together [15].
Although statistical learning theory mainly focuses on establishing universal rate bounds (i.
Hierarchical models with multiple layers of latent variables are emerging as a powerful class of generative models of data in a range of domains, ranging from images to text [1, 18].
Deep Neural Networks (DNNs) have substantially pushed Artificial Intelligence (AI) limits in a wide range of tasks (LeCun et al.
An important first step in many neuroscience experiments is to train animals to perform a particular sensory, cognitive, or motor task.
Access to positive, negative and unlabeled examples is a standard assumption for most semisupervised binary classification techniques.
Recommender systems have been helpful to users for making decisions in diverse domains such as movies, wines, food, news among others [19, 23].
Structured output prediction is ubiquitous in machine learning.
Modern data analysis always addresses enormous data sets in recent years.
We are interested in the class of problems that require the prediction of a structured output y ∈ Y given an input x ∈ X .
The recently introduced sequence-to-sequence model has shown success in many tasks that map sequences to sequences, e.
Multi-item, multi-bidder auctions have been studied extensively in economics, operations research, and computer science.
Visual question-answering tasks provide a testbed to cultivate the synergistic proposals which handle multidisciplinary problems of vision, language and integrated reasoning.
Automated techniques from statistics and machine learning are increasingly being used to make decisions that have important consequences on people’s lives, including hiring [24], lending [10], policing [25], and even criminal sentencing [7].
The standard view of perceptual decision making across psychology and neuroscience is of a competitive process that accumulates sensory evidence for the choices up to a threshold (bound) that triggers the decision [1, 2, 3].
Pattern recognition and models of associative memory [1] are closely related.
Suppose that X ∈ Rn1 ×n2 is a rank-r matrix with r much smaller than n1 and n2 .
We consider the problem of structural learning of Bayesian networks with bounded treewidth, adopting a score-based approach.
Undirected probabilistic graphical models are widely used to explore and represent dependencies between random variables.
Time series analysis is a central problem in many applications such as demand forecasting and climatology.
A signed graph is a graph with positive and negative edge weights.
Many situations in our daily life require us to make repeated decisions which result in some losses corresponding to our chosen actions.
It is common in machine learning to encounter optimization problems involving millions of parameters and very large datasets.
Over the past decade, exploiting low-dimensional structure in high-dimensional problems has become a highly active area of research in machine learning, signal processing, and statistics.
Continuous dynamical systems theory lends itself as a framework for both qualitative and quantitative understanding of neural models [1, 2, 3, 4].
Convolutional neural networks [19] offer an efficient architecture to extract highly meaningful statistical patterns in large-scale and high-dimensional datasets.
The dueling bandit problem [1] is a variant of the classical multi-armed bandit (MAB) problem, where the feedback comes in the form of pairwise comparison.
This work studies the problem of detecting the community structure of a dynamic network according to the framework of evolving graphs [3].
In recent years, network data have appeared in a growing number of applications, such as online social networks, biological networks, and networks representing communication patterns.
Variational inference (vi) is a technique for approximating the posterior distribution in probabilistic models (Jordan et al.
Positive-unlabeled (PU) learning, where a binary classifier is trained from P and U data, has drawn considerable attention recently [1, 2, 3, 4, 5, 6, 7, 8].
Matrix completion is the problem of recovering a low rank matrix from partially observed entries.
Principal Components Analysis (PCA) is among the most frequently used tools for dimension reduction.
Structured matrix recovery has found a wide spectrum of applications in real world, e.
Consider a system of m quadratic equations 2 yi = |hai , xi| , T i ∈ [m] := {1, 2, .
Is there a difference between doing something and showing someone else how to do something? Consider cooking a chicken.
We propose a stochastic optimization method for the three-composite minimization problem: minimize f (x) + g(x) + h(x), x∈Rd (1) where f : Rd → R and g : Rd → R are proper, lower semicontinuous convex functions that admit tractable proximal operators, and h : Rd → R is a smooth function with restricted strong convexity.
Boltzmann machines [1] are powerful generative models that can be used to approximate a large class of real-world data distributions, such as handwritten characters [9], speech segments [7], or multimodal data [16].
The k-means problem and its variants constitute one of the most popular paradigms for clustering [15].
Humans can effortlessly manipulate previously unseen objects in novel ways.
In modern science and technology applications, it has become routine to collect complex datasets with a huge number p of variables and/or enormous sample size n.
Visual similarity learning is the foundation for numerous computer vision subtasks ranging from low-level image processing to high-level object recognition or posture analysis.
Learning and using environmental statistics in choice-selection under uncertainty is a fundamental survival skill.
In recent years, there has been a surge of interest in machine learning methods that involve discrete optimization.
Many real-world networks cannot be studied directly because they are obscured in some way, are too large, or are too difficult to measure.
Data summarization, a central challenge in machine learning, is the task of finding a representative subset of manageable size out of a large dataset.
Differential privacy [DMNS06] is a stability condition on a randomized algorithm, designed to guarantee individual-level privacy during data analysis.
Generative adversarial networks [1] (GANs) are a class of methods for learning generative models based on game theory.
Consider players repeatedly playing a game, all acting independently to minimize their cost or maximize their utility.
Bayesian optimization (BO) [1] provides a powerful framework for automating design problems, and finds applications in robotics, environmental monitoring, and automated machine learning, just to name a few.
Our launching point is the optimization problem min kxk0 s.
Capturing and summarizing the global shape of a cloud of points is at the heart of many data processing applications such as novelty detection, outlier detection as well as related unsupervised learning tasks such as clustering and density estimation.
Neural network (NN) learning has underpinned state of the art empirical results in numerous applied machine learning tasks, see for instance [25, 26].
The growing amount of data available nowadays allowed us to increase the confidence in the models induced by machine learning methods.
Decision tree-based methods, such as random forests and gradient-boosted trees, have a rich and successful history in the machine learning literature.
Deep feed-forward embeddings play a crucial role across a wide range of tasks and applications in image retrieval [1, 8, 15], biometric verification [3, 5, 13, 17, 22, 25, 28], visual product search [21], finding sparse and dense image correspondences [20, 29], etc.
Convolutional neural networks (ConvNets) [1, 2] achieve state-of-the-art accuracy on a variety of computer vision tasks, including classification, object localization, detection, recognition and scene labeling [3, 4].
In this paper, we consider the following optimization problem: min F (x) , f (x) + g(x) x∈Ω1 (1) where g(x) is a convex (but not necessarily smooth) function, Ω1 is a closed convex set and f (x) is a convex but non-smooth function which can be explicitly written as f (x) = max hAx, ui − φ(u) u∈Ω2 (2) where Ω2 ⊂ Rm is a closed convex bounded set, A ∈ Rm×d and φ(u) is a convex function, and h·, ·i is scalar product.
Practical classiﬁcation problems usually involve corrupted labels.
Many machine learning tasks reduce to Finite Sum Minimization (FSM) problems of the form n min F (w) := w∈Rd 1X fi (w), n i=1 (1) where fi are L-smooth and µ-strongly convex.
Modern state-of-the-art object detection systems [1, 2] usually adopt a two-step pipeline: extract a set of class-independent object proposals at first and then classify these object proposals with a pre-trained classifier.
A quadratic function is one of the most important function classes in machine learning, statistics, and data mining.
Probabilistic generative models describe a probability distribution over a given domain X , for example a distribution over natural language sentences, natural images, or recorded waveforms.
One of the most remarkable engineering marvels of nature is the ability of many species such as bats, toothed whales and dolphins to navigate and identify preys and predators by echolocation, i.
We consider the problem of recovering a complex-valued signal (xt )t∈Z from the noisy observations yτ = xτ + σζτ , −n ≤ τ ≤ n.
Determining the subset (or assortment) of items to offer is a key decision problem that commonly arises in several application contexts.
A lot of efforts have been devoted to structure design of convolutional neural network (CNN).
We study the following rich class of (possibly nonconvex) finite-sum optimization problems: n min x2X ⇢M f (x) , 1X fi (x), n i=1 (1) where (M, g) is a Riemannian manifold with the Riemannian metric g, and X ⇢ M is a geodesically convex set.
Modern machine learning applications require computational approaches that are at the same time statistically accurate and numerically efficient [2].
Given two large matrices A and B we study the problem of finding a low rank approximation of their product AT B, using only one pass over the matrix elements.
How can we reliably obtain information from humans, given that the humans themselves are unreliable, and might even have incentives to mislead us? Versions of this question arise in crowdsourcing (Vuurens et al.
The Laplace-Beltrami operator is a fundamental and widely studied mathematical tool carrying a lot of intrinsic topological and geometric information about the Riemannian manifold on which it is defined.
A variety of tasks in machine learning, computer vision and other disciplines can be formulated as energy minimization problems, also known as Maximum-a-Posteriori (MAP) or Maximum Likelihood (ML) estimation problems in undirected graphical models (Markov or Conditional Random Fields).
Markov Chain Monte Carlo (MCMC) sampling [1] stands as a fundamental approach for probabilistic inference in many computational statistical problems.
Bayesian inference provides a powerful tool for modeling complex data and reasoning under uncertainty, but casts a long standing challenge on computing intractable posterior distributions.
There has been great interest in multi-view learning, in which data are obtained from various information sources.
State-of-the-art classifiers, especially deep networks, have shown impressive classification performance on many challenging benchmarks in visual tasks [9] and speech processing [7].
Recent efforts to estimate the 2.
Rapid advances in 3D sensing technology have made 3D data ubiquitous and easily accessible, rendering them an important data source for high 10.
Low rank matrix recovery problem is heavily studied and has numerous applications in collaborative filtering, quantum state tomography, clustering, community detection, metric learning and multi-task learning [21, 12, 9, 27].
Computing a concise, yet diverse and representative subset of a large collection of elements is a central problem in many areas.
Learning goal-directed behavior with sparse feedback from complex environments is a fundamental challenge for artificial intelligence.
A long-standing challenge in machine learning is to learn flexible monotonic functions [1] for classification, regression, and ranking problems.
Unsupervised learning can be described as the general problem of extracting value from unlabelled data which exists in vast quantities.
Communication is a fundamental aspect of intelligence, enabling agents to behave as a group, rather than a collection of individuals.
A prevalent family [9, 7, 19] of deep networks for object detection can be divided into two subnetworks by the Region-of-Interest (RoI) pooling layer [7]: (i) a shared, “fully convolutional” subnetwork independent of RoIs, and (ii) an RoI-wise subnetwork that does not share computation.
A large body of recent developments in optimization have focused on minimization of convex finite sums of the form: n 1X f (x) = fi (x), n i=1 a very general class of problems including the empirical risk minimization (ERM) framework as a special case.
Problem Statement Conventional automatic speech recognition (ASR) is performed by highly supervised systems which utilize large amounts of training data and expert knowledge.
Modern technological advances now enable scientists to simultaneously record hundreds or thousands of variables in fields ranging from neuroscience and genomics to health care and economics.
Understanding the 3D world is at the heart of successful computer vision applications in robotics, rendering and modeling [19].
The field of social dynamics is concerned primarily with interactions among individuals and the resulting group behaviors.
In many areas of data science, high-dimensional signals contain rich structure.
Confronted with the continuous flow of experience, the brain takes amorphous sensory inputs and translates them into coherent objects and scenes.
Image super-resolution (SR) aiming to recover a high-resolution (HR) image from a single lowresolution (LR) image, has important applications in image processing and computer vision, ranging from high-definition (HD) televisions and surveillance to medical imaging.
We study online decision making problems where a learner chooses an action based on some side information (context) and incurs some cost for that action with a goal of incurring minimal cost over a sequence of rounds.
In the last 10 years, the amount of data available is growing at an unprecedented rate.
As machine learning increasingly affects decisions in domains protected by anti-discrimination law, there is much interest in algorithmically measuring and ensuring fairness in machine learning.
How language and communication emerge among intelligent agents has long been a topic of intense debate.
Computing the dominant eigenvectors of matrices and graphs is one of the most fundamental tasks in various machine learning problems, including low-rank approximation, principal component analysis, spectral clustering, dimensionality reduction and matrix completion.
Feature selection is one of the fundamental problems in machine learning research [1, 2].
Significant progress has been recently made on developing inference tools to complement the feature selection methods that have been intensively studied in the past decade [6, 5, 9].
In neuro-imaging, inter-subject variability is often handled as a statistical residual and discarded.
Clustering is a central problem in the analysis and exploration of data.
Many problems in real-world applications involve predicting a collection of random variables that are statistically related.
The most acknowledged methods of measuring importance of nodes in graphs are based on random walk models.
Kernel methods have long been effective in generalizing linear statistical approaches to nonlinear cases by embedding a sample to the reproducing kernel Hilbert space (RKHS) [1].
The method of random projections (RPs) is an important approach to linear dimensionality reduction [23].
Probabilistic techniques are central to data analysis, but can be difficult to apply, combine, and compare.
In recent years, convolutional neural networks (CNNs) trained on large scale datasets have achieved remarkable performance on traditional vision problems such as image classification [8, 18, 26], object detection and localization [5, 16] and others.
Explosive growth in the size of modern datasets has fueled the recent interest in distributed statistical learning.
Magnetic Resonance Imaging (MRI) is a non-invasive imaging technique providing both functional and anatomical information for clinical diagnosis.
From just a single snapshot, humans are often able to imagine how a scene will visually change over time.
Robust statistical estimators [5, 7] (in particular, resistant estimators), such as the median, are an essential tool in data analysis since they are provably immune to outliers.
A common goal for standard classification problems in machine learning is to find a classifier that minimizes the zero-one loss.
Many machine-learning algorithms rely on a-priori access to data to properly tune relevant hyperparameters [Bergstra et al.
Stochastic Gradient Descent (SGD) based optimization methods are widely used for many different learning problems.
Stochastic multi-armed bandits (MAB) have a rich history in sequential decision making [1, 2, 3].
The singular value decomposition (SVD) of a rank-r matrix A ∈ Rd×n corresponds to decomposing A = V ΣU > where V ∈ Rd×r , U ∈ Rn×r are two column orthonormal matrices, and Σ = diag{σ1 , .
Thanks to the growing availability of large-scale datasets and computation power, Deep Learning has recently generated a quasi-revolution in many fields, such as Computer Vision and Natural Language Processing.
We consider the problem of predicting online the entries in an m ⇥ n binary matrix U .
We consider the problem of efficiently estimating the coefficients of generalized linear models (GLMs) when the number of observations n is much larger than the dimension of the coefficient vector p, (n p 1).
We consider modeling the joint distribution Pr(y1 , .
In modern high dimensional data analysis tasks, a routinely faced challenge is that the number of collected samples is substantially smaller than the dimensionality of features.
Offline handwriting recognition consists in recognizing a sequence of characters in an image of handwritten text.
Stochastic multi-armed bandit (MAB) is a classical online learning problem typically specified as a player against m machines or arms.
A touchstone problem for computational linguistics is to translate natural language descriptions into executable programs.
Deep networks have significantly improved the state of the art for a wide variety of machine-learning problems and applications.
Bayesian networks learned from data are broadly used for classification, clustering, feature selection, and to determine associations and dependencies between random variables, in addition to discovering causes and effects; see, e.
Many clustering applications require models that assume cluster sizes grow linearly with the size of the data set.
We live in a three-dimensional world, yet our observations of it are typically in the form of twodimensional projections that we capture with our eyes or with cameras.
We consider online sequential decision problems.
Deep convolutional neural networks (CNNs) have achieved great success in a wide range of problems in the last few years.
Most learning and inference algorithms in the probabilistic topic modeling literature can be delineated along two major lines: the variational approximation popularized in the seminal paper of Blei et al.
Feature construction has been and remains an important topic for reinforcement learning.
Deep generative models with latent variables can capture image information in a probabilistic manner to answer questions about structure and uncertainty.
Contexts contribute semantic clues for action recognition in video.
A hallmark of empirical risk minimization (ERM) on large datasets is that evaluating descent directions requires a complete pass over the dataset.
Crowdsourcing platforms provide labor markets in which pieces of micro-tasks are electronically distributed to a pool of workers.
Until recently, neural data analysis techniques focused primarily upon the analysis of single neurons and small populations.
Visual markers (also known as visual fiducials or visual codes) are used to facilitate humanenvironment and robot-environment interaction, and to aid computer vision in resource-constrained and/or accuracy-critical scenarios.
Modern data science applications increasingly involve learning complex probabilistic models over massive datasets.
Given a couple (X, Y ) of random variables, where Y takes scalar values, a common aim in statistics and machine learning is to estimate the conditional expectation E [Y | X = x] as a function of x.
When using optimization in machine learning, leveraging the natural separability of the objective functions has led to many algorithmic advances; the most common example is the separability as a sum of individual loss terms corresponding to individual observations, which leads to stochastic gradient descent techniques.
In the contextual bandit problem [8, 2], the decision maker observes a sequence of contexts (or features).
The data matrix is X ∈ Rn×d (a row xiT ∈ R1×d is a data point in d dimensions).
Many researchers, particularly in economics, psychology, and the social sciences, use linear structural equation models (SEMs) to describe the causal and statistical relationships between a set of variables, predict the effects of interventions and policies, and to estimate parameters of interest.
This paper describes swapout, a stochastic training method for general deep networks.
Life-long learning is an emerging object of study in machine learning, statistics, and many other domains [2, 11].
Submodular functions provide efficient and flexible tools for learning on discrete data.
Correspondence estimation is the workhorse that drives several fundamental problems in computer vision, such as 3D reconstruction, image retrieval or object recognition.
Deep feed-forward and recurrent neural networks have been shown to be remarkably effective in a wide variety of problems.
Deep learning methods have taken by storm areas such as computer vision, natural language processing and speech recognition.
As the reinforcement learning community has shifted its focus from heuristic methods to methods that have performance guarantees, PAC exploration algorithms have received significant attention.
The sensory data that enters our brain through our sensors has a high intrinsic dimensionality and it is complex and ambiguous.
What makes a 3D generative model of object shapes appealing? We believe a good generative model should be able to synthesize 3D objects that are both highly varied and realistic.
L2 quantities (i.
The pervasiveness of big data has made scalable machine learning increasingly important, especially for deep models.
The human percept of a visual scene is highly structured.
For large-scale machine learning applications, n, the number of training data examples, is usually very large.
Can we measure the accuracy of a model at test time without any ground truth labels, and without assuming the test distribution is close to the training distribution? This is the problem of unsupervised risk estimation (Donmez et al.
Convolutional neural networks (CNNs) [15] have proven extremely successful for a wide range of computer vision problems and other applications.
Deep learning [13, 16] is currently the state of the art machine learning technique in many application areas such as computer vision or natural language processing.
Distributions over subsets of objects arise in a variety of machine learning applications.
We study nonconvex, nonsmooth, finite-sum optimization problems of the form n min x2Rd F (x) := f (x) + h(x), where f (x) := 1X fi (x), n i=1 (1) and each fi : Rd ! R is smooth (possibly nonconvex) for all i 2 {1, .
We study the problem of minimizing a convex function f over a feasible set X , a closed convex subset of E = Rn .
The multi-armed bandit problem (MAB) is a sequential learning task in which an algorithm takes at each stage a decision (or, “pulls an arm”).
Maximum entropy principle The maximum entropy principle [Jay57] states that given mean parameters, i.
Hidden Markov models (HMMs) [1] are one of the most popular statistical models for analyzing time series data in various application domains such as speech recognition, medicine, and meteorology.
Sum-product networks (SPNs) are new deep graphical model architectures that admit exact probabilistic inference in linear time in the size of the network [14].
Game theory provides a powerful framework for the design and analysis of multiagent systems that involve strategic interactions [see, e.
Hyperparameter optimization is crucial for obtaining good performance in many machine learning algorithms, such as support vector machines, deep neural networks, and deep reinforcement learning.
We consider the Online Linear Optimization (OLO) [4, 25] setting.
Ordinary recurrent neural networks typically have two types of memory that have very different time scales, very different capacities and very different computational roles.
Bayesian non-parametric ideas have played a major role in various intricate applications in statistics and machine learning.
Recently neural networks (NN) have achieved state-of-the-art performance in various applications ranging from computer vision [12] to natural language processing [20].
Network analysis has been widely used in various fields to characterize the interdependencies between a group of variables, such as molecular entities including RNAs and proteins in genetic networks [3].
In this paper, consider the recovery from linear noisy measurements of β ? ∈ Rp , which satisfies the following structural sparsity that the linear transformation γ ? := Dβ ? for some D ∈ Rm×p has most of its elements being zeros.
The recently introduced variational autoencoder (VAE) [10, 19] provides a framework for deep generative models.
Minimizing a convex function over the set of positive semidefinite matrices with unit trace, aka the spectrahedron, is an important optimization task which lies at the heart of many optimization, machine learning, and signal processing tasks such as matrix completion [1, 13], metric learning [21, 22], kernel matrix learning [16, 9], multiclass classification [2, 23], and more.
In active learning, the learner is given an input space X , a label space L, and a hypothesis class H such that one of the hypotheses in the class generates ground truth labels.
Submodular functions are attractive models of many physical processes primarily because they possess an inherent naturalness to a wide variety of problems (e.
Studying the anatomy of individual neurons and the circuits they form is a classical approach to understanding how nervous systems function since Ramón y Cajal’s founding work.
Recently, so-called adaptive stochastic optimization algorithms have gained popularity for large-scale convex and non-convex optimization problems.
Online social platforms and service websites, such as Reddit, Netflix and Amazon, are attracting thousands of users every minute.
In a large and complex environment, such as a city, we often need to be able to flexibly plan so that we can reach a wide variety of goal locations from different start locations.
Active learning is a problem setting for sequentially selecting unlabeled instances to be labeled, and it has been studied with much practical interest as an efficient way to reduce the annotation cost.
As a family of brain inspired models, deep neural networks (DNNs) have substantially advanced a variety of artificial intelligence tasks including image classification [13, 19, 11], natural language processing, speech recognition and face recognition.
Recently there has been a surge of interest in training neural networks to generate images.
Large datasets provide great opportunities to learn rich statistical representations, for accurate predictions and new scientific insights into our modeling problems.
In this paper, we study the problem of non-negative matrix factorization (NMF), where given a matrix Y ∈ Rm×N , the goal to find a matrix A ∈ Rm×n and a non-negative matrix X ∈ Rn×N such that Y ≈ AX.
The traditional analysis of algorithms is based on a worst-case, minimax formulation.
Dropout has been widely used to avoid overfitting of deep neural networks with a large number of parameters [9, 16], which usually identically and independently at random samples neurons and sets their outputs to be zeros.
Decision tree [16] is a widely used machine learning algorithm, since it is practically effective and the rules it learns are simple and interpretable.
Learning theory traditionally has been studied in a statistical framework, discussed at length, for example, by Shalev-Shwartz and Ben-David [2014].
Multi-Armed Bandit (MAB) problems have been studied extensively in the past, with two important special cases: the Stochastic Multi-Armed Bandit, and the Adversarial (Non-Stochastic) Multi-Armed Bandit.
Sparsity is a critical property for the success of regression methods, especially in high dimension.
Gaussian processes (GPs) are nonparametric statistical models widely used for probabilistic reasoning about functions.
We study the general problem of eliciting and aggregating information for categorical questions.
In scientific and engineering fields researchers often times face the problem of quantifying the relationship between a given outcome Y and corresponding predictor vector X, based on a sample {(Yi , Xi> )> }ni=1 of n observations.
The various existing kernel methods can conveniently be applied to any type of data, for which a kernel is available that adequately measures the similarity between any two data objects.
Several diverse domains such as judiciary, health care, and insurance rely heavily on human decision making.
In computational learning theory, one of the fundamental challenges is to understand how different information complexity measures arising from different learning models relate to each other.
Online learning represents a family of effective and scalable learning algorithms for incrementally building a predictive model from a sequence of data samples [1].
We address the problem of discovering features of distinct probability distributions, with which they can most easily be distinguished.
Price optimization is a central research topic with respect to revenue management in marketing science [10, 16, 18].
State estimation is an important component of mobile robotic applications, including autonomous driving and flight [22].
Let G = (V, E) be a d-dimensional grid graph, i.
Since Fisher’s 1922 paper (Fisher, 1922), maximum likelihood estimators (MLE) have become one of the most popular tools in many areas of science and engineering.
Many canonical machine learning problems boil down to solving a convex empirical risk minimization problem of the form m min F (w) = w∈W 1 X fi (w), m i=1 (1) where each individual function fi (·) is convex (e.
An important facet of neural data analysis concerns characterizing the tuning properties of neurons, defined as the average firing rate of a cell conditioned on the value of some external variables, for instance the orientation of an image patch in a V1 cell, or the position of the animal within an environment for hippocampal cells.
A multiagent economy is comprised of agents interacting under specific economic rules.
With the proliferation of online social networks, the problem of optimally influencing the opinions of individuals in a population has garnered tremendous attention [1–3].
For supervised learning, the back-propagation algorithm (BP), see [2], has achieved great success in training deep neural networks.
Finite mixture models are widely used in variety of statistical settings, as models for heterogeneous populations, as flexible models for multivariate density estimation and as models for clustering.
We are interested in a specific setting of imitation learning—the problem of learning to perform a task from expert demonstrations—in which the learner is given only samples of trajectories from the expert, is not allowed to query the expert for more data while training, and is not provided a reinforcement signal of any kind.
Recently there has been growing appreciation for tensor methods in machine learning.
Facial behavior is a powerful means to express emotions and to perceive the intentions of a human.
Machine learning has made significant progress in understanding both theoretical and practical aspects of solving a single prediction problem from a set of annotated examples.
Large-scale datasets, comprising tens or hundreds of millions of observations, are becoming the norm in scientific and commercial applications ranging from population genetics to advertising.
Algorithms for dimensionality reduction usually aim to project an input set of d-dimensional vectors (database records) onto a k ≤ d − 1 dimensional affine subspace that minimizes the sum of squared distances to these vectors, under some constraints.
The fields of object recognition, speech recognition, machine translation have been revolutionized by the emergence of massive labeled datasets [31, 42, 10] and learned deep representations [17, 33, 10, 35].
Unsupervised nonlinear feature learning, or unsupervised representation learning, is one of the biggest challenges facing machine learning.
Methods of feature selection is an important topic of machine learning [8, 2, 17], since they improve performance of learning systems while reducing their computational costs.
Successful object recognition systems, such as Convolutional Neural Networks (CNN), extract “distinctive patterns” that describe an object (e.
Deep learning has been a great practical success in many fields, including the fields of computer vision, machine learning, and artificial intelligence.
The oldest and most reliable method for recording neural activity involves lowering an electrode into the brain and recording the local electrical activity around the electrode tip.
Two phenomena are generally considered important for modelling complex networks.
Online learning methods are highly successful at rapidly reducing the test error on large, highdimensional datasets.
Bandit convex optimization (BCO) is a key framework for modeling learning problems with sequential data under partial feedback.
Markov Chain Monte Carlo (MCMC) techniques are one of the most popular family of algorithms in Bayesian machine learning.
Calcium imaging has become one of the most widely used techniques for recording activity from neural populations in vivo [1].
In traditional machine learning, it is assumed that data are identically drawn from a single distribution.
Tensors are a powerful tool for dealing with multi-modal and multi-relational data.
Recently there has been a resurgence of new structural designs for recurrent neural networks (RNNs) [1, 2, 3].
Humans are adept at a wide array of complicated sensory inference tasks, from recognizing objects in an image to understanding phonemes in a speech signal, despite significant variations such as the position, orientation, and scale of objects and the pronunciation, pitch, and volume of speech.
The efficient coding hypothesis [1, 2] plays a fundamental role in understanding neural codes, particularly in early sensory processing.
Digital crowdsourcing (CS) is a modern approach to perform certain large projects using small contributions of a large crowd.
Classic optical character recognition (OCR) tools focus on reading text from well-prepared scanned documents.
In this paper we study the facility location problem: we are given sets V of size n, I of size m and a benefit matrix of nonnegative numbers C 2 RI⇥V , where Civ describes the benefit that element i receives from element v.
Medical drug testing, policy setting, and other scientific processes are commonly framed and analysed in the language of sequential experimental design and, in special cases, as bandit problems (Robbins, 1952; Chernoff, 1959).
Temporal events modeling is a classic machine learning problem that has drawn enormous research attentions for decades.
Many social phenomena, such as the spread of diseases, behaviors, technologies, or products, can naturally be modeled as the diffusion of a contagion across a network.
Sequentially observed count vectors y (1) , .
Markov random fields (MRFs) [10] are widely used across different domains from computer vision and natural language processing to computational biology, because they are a general tool to describe distributions that involve multiple variables.
Recent successes of deep neural networks have spanned many domains, from computer vision [1] to speech recognition [2] and many other tasks.
In statistical learning or other data-based decision-making problems, it is desirable to give solutions that come with guarantees on performance, at least to some specified confidence level.
The primary objective of linear regression is to determine the relationships between multiple variables and how they may affect a certain outcome.
Most active learning theory is based on interacting with a L ABEL oracle: An active learner observes unlabeled examples, each with a label that is initially hidden.
Training deep, directed generative models with many layers of latent variables poses a challenging problem.
Over the last decade, deep convolutional neural networks (CNNs) have revolutionized supervised learning for tasks such as object recognition, action recognition, and semantic segmentation [3, 15, 6, 19].
Recurrent Neural Networks (RNNs) have been found to be successful in a variety of sequence learning problems [4, 3, 9], including those involving long term dependencies (e.
We consider semideﬁnite programs (SDP’s) of the form f∗ = min �C, X� X∈Sn×n subject to A(X) = b, X � 0, (SDP) where �C, X� = Tr(C �X), C ∈ Sn×n is the symmetric cost matrix, A : Sn×n → Rm is a linear operator capturing m equality constraints with right hand side b ∈ Rm and the variable X is symmetric, positive semideﬁnite.
To allow for efficient navigation and search, modern information systems rely on the usage of nonhierarchical tags, keywords, or labels to describe items and content.
Over the past decades, enormous human effort has been devoted to machine learning; preprocessing data, model selection, and hyperparameter optimization are some examples of critical and often expert-dependent tasks.
An important problem, for both humans and machines, is to extract relevant information from complex data.
One primary goal of cognitive neuroscience is to find a mapping from neural activity onto cognitive processes–that is, to identify functional networks in the brain and the role they play in supporting macroscopic functions.
Suppose we want to solve the following optimization problem min f (x) x∈Rn (1) in the variable x ∈ Rn , where f (x) is strongly convex with respect to the Euclidean norm with parameter µ, and has a Lipschitz continuous gradient with parameter L with respect to the same norm.
Survival analysis is a branch of statistics focused on the study of time-to-event data, usually called survival times.
We consider the standard K-armed adversarial bandit problem, which is a game played over T rounds between a learner and an adversary.
It is now a very frequent issue for companies to optimise their daily profits by choosing between one of two possible website layouts.
Structured prediction methods [1; 2; 3; 4; 5] are widely adopted techniques for learning mappings between context descriptions x ∈ X and configurations y ∈ Y.
The problem of timely risk assessment and decision-making based on a sequentially observed time series is ubiquitous, with applications in finance, medicine, cognitive science and signal processing [1-7].
Statistical relational learning (SRL) [8] aims at unifying logic and probability for reasoning and learning in noisy domains, described in terms of individuals (or objects), and the relationships between them.
In this paper, we consider the task of monocular depth estimation—i.
The paper concerns the problem of learning a joint distribution of multi-domain images from data.
The classical PAC learning framework of Valiant (1984) considers a learning problem with unknown true distribution p on X ⇥ Y , Y = {0, 1} and fixed concept class C consisting of (deterministic) functions f : X ! Y .
Let φ : R → R+ be a lower semi-continuous (lsc) and symmetric function with minimum value φ(0).
Humans are good at predicting another view from related views.
The broad adoption of Electronic Health Record (EHR) systems has opened the possibility of applying clinical predictive models to improve the quality of clinical care.
Numerous central problems in machine learning, statistics and operations research are special cases of stochastic optimization from i.
Exploration algorithms for Markov Decision Processes (MDPs) are typically concerned with reducing the agent’s uncertainty over the environment’s reward and transition functions.
Clustering is a challenging task particularly due to two impediments.
This work studies statistical learning theory using the point of view of compression.
Recovering a signal via a quadratic system of equations has gained intensive attention recently.
Community detection consists in extracting (a few) groups of similar items from a large global population, and has applications in a wide spectrum of disciplines including social sciences, biology, computer science, and statistical physics.
There has been significant interest and progress in recent years in developing algorithms for dueling bandit problems [1–11].
The stochastic block model (SBM) is widely used as a model for community detection and as a benchmark for clustering algorithms.
Unsupervised representation learning is one of the major themes of modern data science; a common theme among the various approaches is to extract maximally “informative" features via informationtheoretic metrics (entropy, mutual information and their variations) – the primary reason for the popularity of information theoretic measures is that they are invariant to one-to-one transformations and that they obey natural axioms such as data processing.
The human sensory system is devoted to the processing of sensory information to drive our perception of the environment [1].
In geometry processing, computer graphics, and vision, finding intrinsic correspondence between 3D shapes affected by different transformations is one of the fundamental problems with a wide spectrum of applications ranging from texture mapping to animation [25].
Longitudinal data is becoming increasingly important in medical research and practice.
Markov chain Monte Carlo (MCMC) is one of the most important classes of probabilistic inference methods and underlies a variety of approaches to automatic inference [e.
We initiate the systematic study of a general class of multi-dimensional prediction problems, where the learner wishes to predict the solution to an unknown linear program (LP), given some partial information about either the set of constraints or the objective.
Dynamics-based Markov Chain Monte Carlo methods (D-MCMCs) are sampling methods using dynamics simulation for state transition in a Markov chain.
Structured prediction covers a broad family of important learning problems.
Nearest neighbor (NN) search is a basic primitive of machine learning and statistics.
A fundamental problem in the theory of clustering is that of deﬁning a cluster.
With the high prevalence and abundance of Internet services, recommender systems are becoming increasingly important to attract users because they can help users make effective use of the information available.
Modeling of large-scale stochastic phenomena with both spatial and temporal (spatiotemporal) evolution is a fundamental problem in the applied sciences and social networks.
Discovering causal relations from data is at the foundation of the scientific method.
Recently recurrent neural networks (RNNs) have been used in many natural language processing (NLP) tasks, such as language modeling [14], machine translation [23], sentiment analysis [24], and question answering [26].
Recently, there has been an increasing interest in adapting machine learning and statistical methods to tensors.
Deep feedforward neural networks have achieved remarkable performance across many domains [1–6].
Variational methods have surpassed traditional methods such as Markov chain Monte Carlo [MCMC, 15] and mean-field coordinate ascent [25] as the de-facto standard approach for training directed graphical models.
Differential privacy is a notion of privacy that provides a statistical measure of privacy protection for randomized statistics.
