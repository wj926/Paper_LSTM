{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Character Recurrent Neural Network\n",
    "- Abstract\n",
    "- Long short-term memory(LSTM)\n",
    "- Word"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1. Settings\n",
    "### 1) Import required libraries"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "from torch.autograd import Variable"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import unidecode\n",
    "import string\n",
    "import random\n",
    "import re\n",
    "import time, math\n",
    "import glob\n",
    "import pickle\n",
    "import codecs\n",
    "import numpy as np\n",
    "import helpers\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2) Hyperparameter"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "num_epochs = 5000\n",
    "print_every = 100\n",
    "plot_every = 10\n",
    "hidden_size = 100\n",
    "batch_size =1\n",
    "num_layers = 1\n",
    "lr = 0.002\n",
    "NUM_STEPS = 500\n",
    "chunk_len = 200\n",
    "DATA_PATH = './data/abstract.txt'"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2. Data\n",
    "### 1) Get Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Corpus is 3042156 characters long\n"
     ]
    }
   ],
   "source": [
    "####PREPROCESSING START\n",
    "book_filenames =sorted(glob.glob(\"../Paper_seq2seq/data/abstract10.txt\"))\n",
    "\n",
    "corpus_raw=u\"\"\n",
    "for filename in book_filenames:\n",
    "    with codecs.open(filename, 'r', 'utf-8') as book_file:\n",
    "        corpus_raw+=book_file.read()\n",
    "\n",
    "print(\"Corpus is {} characters long\".format(len(corpus_raw)))\n",
    "#corpus_raw: one string"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "corpus_splitlines = corpus_raw.splitlines()\n",
    "#corpus_splitlines: list of all sentence\n",
    "\n",
    "corpus=[]\n",
    "for sentence in corpus_splitlines:\n",
    "    sentence_wo_dot=sentence.replace('.', '')\n",
    "    word = sentence_wo_dot.split(' ')\n",
    "    corpus.append(word)\n",
    "#corpus: list of sentence which is list of words\n",
    "    \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "corpus_set=set()\n",
    "for sentence in corpus:\n",
    "    for word in sentence:\n",
    "        corpus_set.add(word)\n",
    "#corpus_set: set of all word   "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "corpus_list=[]\n",
    "for sentence in corpus:\n",
    "    for word in sentence:\n",
    "        corpus_list.append(word)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 2) Character to tensor"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "voca_size=len(corpus_set)\n",
    "\n",
    "voca_to_int=dict(zip(corpus_set, range(voca_size)))\n",
    "int_to_voca=dict(zip(range(voca_size), corpus_set))\n",
    "\n",
    "\n",
    "####PREPROCESSING END"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 65,
   "metadata": {},
   "outputs": [],
   "source": [
    "corpus_int=[]\n",
    "for sentence in corpus:\n",
    "    tmp=[]\n",
    "    for word in sentence:\n",
    "       word_int = voca_to_int[word]\n",
    "       tmp.append(word_int)\n",
    "    corpus_int.append(tmp)\n",
    "#corpus_int: list of sentence which is list of word_int"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 91,
   "metadata": {
    "collapsed": true
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['Abstract',\n",
       " 'A',\n",
       " 'method',\n",
       " 'is',\n",
       " 'proposed',\n",
       " 'for',\n",
       " 'semiparametric',\n",
       " 'estimation',\n",
       " 'where',\n",
       " 'parametric',\n",
       " 'and',\n",
       " 'nonparametric',\n",
       " 'criteria',\n",
       " 'are',\n",
       " 'exploited',\n",
       " 'in',\n",
       " 'density',\n",
       " 'estimation',\n",
       " 'and',\n",
       " 'unsupervised',\n",
       " 'learning',\n",
       " 'This',\n",
       " 'is',\n",
       " 'accomplished',\n",
       " 'by',\n",
       " 'making',\n",
       " 'sampling',\n",
       " 'assumptions',\n",
       " 'on',\n",
       " 'a',\n",
       " 'dataset',\n",
       " 'that',\n",
       " 'smoothly',\n",
       " 'interpolate',\n",
       " 'between',\n",
       " 'the',\n",
       " 'extreme',\n",
       " 'of',\n",
       " 'independently',\n",
       " 'distributed',\n",
       " '(or',\n",
       " 'id)',\n",
       " 'sample',\n",
       " 'data',\n",
       " '(as',\n",
       " 'in',\n",
       " 'nonparametric',\n",
       " 'kernel',\n",
       " 'density',\n",
       " 'estimators)',\n",
       " 'to',\n",
       " 'the',\n",
       " 'extreme',\n",
       " 'of',\n",
       " 'independent',\n",
       " 'identically',\n",
       " 'distributed',\n",
       " '(or',\n",
       " 'iid)',\n",
       " 'sample',\n",
       " 'data',\n",
       " 'This',\n",
       " 'article',\n",
       " 'makes',\n",
       " 'independent',\n",
       " 'similarly',\n",
       " 'distributed',\n",
       " '(or',\n",
       " 'isd)',\n",
       " 'sampling',\n",
       " 'assumptions',\n",
       " 'and',\n",
       " 'interpolates',\n",
       " 'between',\n",
       " 'these',\n",
       " 'two',\n",
       " 'using',\n",
       " 'a',\n",
       " 'scalar',\n",
       " 'parameter',\n",
       " 'The',\n",
       " 'parameter',\n",
       " 'controls',\n",
       " 'a',\n",
       " 'Bhattacharyya',\n",
       " 'affinity',\n",
       " 'penalty',\n",
       " 'between',\n",
       " 'pairs',\n",
       " 'of',\n",
       " 'distributions',\n",
       " 'on',\n",
       " 'samples',\n",
       " 'Surprisingly,',\n",
       " 'the',\n",
       " 'isd',\n",
       " 'method',\n",
       " 'maintains',\n",
       " 'certain',\n",
       " 'consistency',\n",
       " 'and',\n",
       " 'unimodality',\n",
       " 'properties',\n",
       " 'akin',\n",
       " 'to',\n",
       " 'maximum',\n",
       " 'likelihood',\n",
       " 'estimation',\n",
       " 'The',\n",
       " 'proposed',\n",
       " 'isd',\n",
       " 'scheme',\n",
       " 'is',\n",
       " 'an',\n",
       " 'alternative',\n",
       " 'for',\n",
       " 'handling',\n",
       " 'nonstationarity',\n",
       " 'in',\n",
       " 'data',\n",
       " 'without',\n",
       " 'making',\n",
       " 'drastic',\n",
       " 'hidden',\n",
       " 'variable',\n",
       " 'assumptions',\n",
       " 'which',\n",
       " 'often',\n",
       " 'make',\n",
       " 'estimation',\n",
       " 'difficult',\n",
       " 'and',\n",
       " 'laden',\n",
       " 'with',\n",
       " 'local',\n",
       " 'optima',\n",
       " 'Experiments',\n",
       " 'in',\n",
       " 'density',\n",
       " 'estimation',\n",
       " 'on',\n",
       " 'a',\n",
       " 'variety',\n",
       " 'of',\n",
       " 'datasets',\n",
       " 'confirm',\n",
       " 'the',\n",
       " 'value',\n",
       " 'of',\n",
       " 'isd',\n",
       " 'over',\n",
       " 'iid',\n",
       " 'estimation,',\n",
       " 'id',\n",
       " 'estimation',\n",
       " 'and',\n",
       " 'mixture',\n",
       " 'modeling',\n",
       " '']"
      ]
     },
     "execution_count": 91,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "corpus[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 80,
   "metadata": {
    "collapsed": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Variable containing:\n",
      " 14779\n",
      " 13614\n",
      " 21867\n",
      "  4727\n",
      "   827\n",
      " 13832\n",
      "  1088\n",
      "  4095\n",
      " 13172\n",
      " 19807\n",
      " 21119\n",
      " 11846\n",
      " 15768\n",
      "  3711\n",
      " 21761\n",
      "  3190\n",
      " 17589\n",
      "  4095\n",
      " 21119\n",
      "  1959\n",
      " 10871\n",
      " 21156\n",
      "  4727\n",
      "    47\n",
      "  8680\n",
      "  7562\n",
      "  1179\n",
      " 12006\n",
      " 19009\n",
      " 23449\n",
      "  7988\n",
      " 18026\n",
      "  3258\n",
      " 20943\n",
      " 14977\n",
      " 24160\n",
      "  4829\n",
      " 18192\n",
      "  4655\n",
      "  2117\n",
      "  2178\n",
      " 16278\n",
      " 23733\n",
      " 24683\n",
      " 14788\n",
      "  3190\n",
      " 11846\n",
      " 12519\n",
      " 17589\n",
      " 22933\n",
      "  7888\n",
      " 24160\n",
      "  4829\n",
      " 18192\n",
      "  9541\n",
      "  8671\n",
      "  2117\n",
      "  2178\n",
      "   248\n",
      " 23733\n",
      " 24683\n",
      " 21156\n",
      " 24812\n",
      "  8422\n",
      "  9541\n",
      "  1550\n",
      "  2117\n",
      "  2178\n",
      " 21256\n",
      "  1179\n",
      " 12006\n",
      " 21119\n",
      " 13866\n",
      " 14977\n",
      " 21971\n",
      " 11718\n",
      "  6228\n",
      " 23449\n",
      "  5761\n",
      "  2366\n",
      "  7936\n",
      "  2366\n",
      "  4833\n",
      " 23449\n",
      " 21384\n",
      " 14810\n",
      " 21589\n",
      " 14977\n",
      "  3738\n",
      " 18192\n",
      " 14077\n",
      " 19009\n",
      "   348\n",
      "  7320\n",
      " 24160\n",
      "  4008\n",
      " 21867\n",
      " 10484\n",
      " 18335\n",
      " 16740\n",
      " 21119\n",
      " 20980\n",
      " 15502\n",
      " 25020\n",
      "  7888\n",
      " 22259\n",
      " 22020\n",
      "  4095\n",
      "  7936\n",
      "   827\n",
      "  4008\n",
      " 13105\n",
      "  4727\n",
      " 24986\n",
      "  1667\n",
      " 13832\n",
      " 21157\n",
      "  9278\n",
      "  3190\n",
      " 24683\n",
      " 20063\n",
      "  7562\n",
      " 23984\n",
      " 20879\n",
      " 16987\n",
      " 12006\n",
      " 23955\n",
      "  2696\n",
      "  7700\n",
      "  4095\n",
      " 17512\n",
      " 21119\n",
      " 12918\n",
      "  7980\n",
      "  9158\n",
      " 20378\n",
      " 11989\n",
      "  3190\n",
      " 17589\n",
      "  4095\n",
      " 19009\n",
      " 23449\n",
      " 10509\n",
      " 18192\n",
      "  6092\n",
      " 15550\n",
      " 24160\n",
      "  3396\n",
      " 18192\n",
      "  4008\n",
      " 18972\n",
      " 16374\n",
      " 13385\n",
      " 16133\n",
      "  4095\n",
      " 21119\n",
      " 16149\n",
      " 13094\n",
      "     0\n",
      "[torch.cuda.LongTensor of size 159 (GPU 0)]\n",
      "\n"
     ]
    }
   ],
   "source": [
    "def word_tensor(corpus):\n",
    "    tensor = torch.zeros(len(corpus)).long()\n",
    "    for c in range(len(corpus)):\n",
    "        tensor[c] = voca_to_int[corpus[c]]\n",
    "    return Variable(tensor).cuda()\n",
    "\n",
    "print(word_tensor(corpus[0]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 132,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def one_word_tensor(string):\n",
    "    tensor = torch.zeros(1).long()\n",
    "    tensor[0] = voca_to_int[string]\n",
    "    return Variable(tensor).cuda()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 133,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Variable containing:\n",
       " 20484\n",
       "[torch.cuda.LongTensor of size 1 (GPU 0)]"
      ]
     },
     "execution_count": 133,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "one_word_tensor('abstract')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 93,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Variable containing:\n",
       " 20484\n",
       "[torch.cuda.LongTensor of size 1 (GPU 0)]"
      ]
     },
     "execution_count": 93,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "word_tensor(abstract)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 92,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "abstract = ['abstract']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def int2voc(sentence_int):\n",
    "    sentence_list=[]\n",
    "    for word_int in sentence_int:\n",
    "        word=int_to_voca[word_int]\n",
    "        sentence_list.append(word)\n",
    "        sentence = ' '.join(sentence_list)\n",
    "    return sentence"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'abstract'"
      ]
     },
     "execution_count": 42,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "int_to_voca[20484]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 3. Model & Optimizer\n",
    "### 1) Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "metadata": {},
   "outputs": [],
   "source": [
    "class RNN(nn.Module):\n",
    "    def __init__(self, input_size, hidden_size, output_size, num_layers=1):\n",
    "        super(RNN, self).__init__()\n",
    "        self.input_size = input_size\n",
    "        self.hidden_size = hidden_size\n",
    "        self.output_size = output_size\n",
    "        self.num_layers = num_layers\n",
    "        self.encoder = nn.Embedding(input_size, hidden_size)\n",
    "        self.rnn = nn.LSTM(hidden_size,hidden_size,num_layers)\n",
    "        self.decoder = nn.Linear(hidden_size, output_size)\n",
    "    def forward(self, input, hidden,cell):\n",
    "        out = self.encoder(input.view(1,-1))\n",
    "        out,(hidden,cell) = self.rnn(out,(hidden,cell))\n",
    "        out = self.decoder(out.view(batch_size,-1))\n",
    "        return out,hidden,cell\n",
    "    def init_hidden(self):\n",
    "        hidden = Variable(torch.zeros(num_layers,batch_size,hidden_size)).cuda()\n",
    "        cell = Variable(torch.zeros(num_layers,batch_size,hidden_size)).cuda()\n",
    "        return hidden,cell\n",
    "model = RNN(voca_size, hidden_size, voca_size, num_layers).cuda()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 2) Loss & Optimizer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 104,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "optimizer = torch.optim.Adam(model.parameters(), lr=lr)\n",
    "loss_func = nn.CrossEntropyLoss()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 3) Test function"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 148,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def test():\n",
    "    inp = one_word_tensor('We')\n",
    "    hidden,cell = model.init_hidden()\n",
    "    x = inp\n",
    "    print('We ',end=\"\")\n",
    "    for i in range(200):\n",
    "        output,hidden,cell = model(x,hidden,cell)\n",
    "        output_dist = output.data.view(-1).div(0.8).exp()\n",
    "        top_i = torch.multinomial(output_dist, 1)[0]\n",
    "        predicted_char = int_to_voca[top_i]\n",
    "        print(predicted_char,end=\" \")\n",
    "        x = one_word_tensor(predicted_char)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 149,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "9421"
      ]
     },
     "execution_count": 149,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "voca_to_int['super-family']"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 4. Train"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 156,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      " Variable containing:\n",
      " 3.4538\n",
      "[torch.cuda.FloatTensor of size 1 (GPU 0)]\n",
      " \n",
      "\n",
      "We apply an alternation of X called parameters size at existing algorithms have very large denoising maximizing this problem, we propose a new Gaussian convex component algorithm for learning problems in a time series The traditional networks from an agent’s square problem of a state-of-the-art loss function The proposed approach is simpler, locally able to learning a large number of wisely simple, Learning with a gamma total minimization of the simultaneous high needed We focus on the number of matrix, and real-world it is allowed to optimize a large network of the data The algorithm that is the represented of all resources that have the reward between a sequence of each and relaxations demonstrate the performance of the log-partition function, and an efficient minimization methods using certain techniques that adapts from the population We study a Gaussian simple Bayesian regression and show that it is based on a heterogeneous Slow consolidated which has received an ListMLE class of human choice to the globally important of the causal objective tractable This paper presents a family of the optimal maximum likelihood estimation problem that is shown to be cognitive in many classification The objective is a large problem of substitutions, impressions, learning methods, \n",
      "\n",
      "\n",
      "\n",
      " Variable containing:\n",
      " 4.0987\n",
      "[torch.cuda.FloatTensor of size 1 (GPU 0)]\n",
      " \n",
      "\n",
      "We further give a general optimal strategy of the posterior loss, with accurate consistency of a key recent as a linear representation in the general parameter estimation have been well to outliers with network practical applications with the context of probabilistic models We demonstrate the thousand of the connection is the speed cost to extract the standard items, for deep Bayesian nonparametric the orientation Analysis Typically,  We propose a new method for error component in which the sharpness of predicting the problem, pattern of how the paradigm class for online inference algorithms such as glass and unlabeled data machines We extend a learning framework for such text statistical interaction metrics we propose a novel fast minimizing learning algorithm that is able to benefit noisy, Numerical we use the major function of a Gaussian process information with a sparsistent on the same linear of the potential that can be closed noisy data on the convergence of different labels for a simple dataset, state of the parameterization The Bayesian bound of the corresponding information and the structure of the model We also provide a new family of our framework for fast stochastic levels in result of sampling and convergence and theoretically using \n",
      "\n",
      "\n",
      "\n",
      " Variable containing:\n",
      " 5.0101\n",
      "[torch.cuda.FloatTensor of size 1 (GPU 0)]\n",
      " \n",
      "\n",
      "We show that the benchmark is an optimal error of a box algorithm and a problem can be computed by applying an efficient We demonstrate the number of multiple optimization of these Neural parameter, our approach to s); a product of regularization at the weights and the process, complexity and the 0-1 images of time, the convexity of the former, and the belief propagation, the solution distribution In this paper, we present a dedicated for both simulated and real data, via domains to second-order data-dependent however, we introduce a new graph that categories; capturing the feature noise space and the Drosophila about information We consider the problem of hierarchical learning kernels Our method even the results of this is able to assign it to predict the number of the class of permutations and insights is able to a set of visual neurons which are studied and the sample size of broad on samples that encourages or continuous examples are consistently vision in the teacher and the maximum a priori, form of learning while analyze the learning problem are you convolutional the nonparametric non-abstract to conventional the error, embedding over the neural network We extend the likelihood problem in the worst-case of \n",
      "\n",
      "\n",
      "\n",
      " Variable containing:\n",
      " 7.0328\n",
      "[torch.cuda.FloatTensor of size 1 (GPU 0)]\n",
      " \n",
      "\n",
      "We describe a novel Bayesian algorithm for semi-supervised Bayesian approaches the Bayesian algorithm for the correctness of the problem of probabilistic and show that we are known as a components of a set of thousands of a fraction of the loss of the number of p are often batch to be semantically infeasible in the vast rate tails of the above parts of the correlation randomly These models are averaged to converge to the top-k between a set of both dependent and are done in the first first model a cpd size of the amount of this problem, we introduce a two optimization framework for a unknown response scheme are learned to up the optimal loss, and the resulting approximation We show that our bounds model can enjoys smaller computed domain and it achieves a theoretical advantages for the comparison, cost of which similar these theoretical guarantees for the global feature bound are good and It is an efficient model for finding sparse optimization and real-world Recently, elements  are transition by selecting a generic benchmark iteration of the linear LP models We demonstrate the mean of the maximum likelihood that is no been smaller for a single computational observation = \n",
      "\n",
      "\n",
      "\n",
      " Variable containing:\n",
      " 4.0517\n",
      "[torch.cuda.FloatTensor of size 1 (GPU 0)]\n",
      " \n",
      "\n",
      "We show that different gain), small functions is based on two domains and matrices, that this framework is still RMGPC by the relative extent that are a PAC class for computing the database that exactly between populations of the variational evaluations on signal and analyze the number of marginal bounds on the inference values in which the max-margin question of large-scale and multiple probabilistic inference algorithms  We propose a new learning algorithm that maintains the individual, inference method for an excess geometric which achieves the training of LASSO, and the best Privileged selected is a challenging manifold, and demonstrate that the individual features is a randomly However, PSDBoost minimax neurophysiological timing Using this motivation we are able to modeling that combines the performance of the performance of position point and a compact representation of episodic but also to control We study the problem of a deterministic class of well-established classes of a full model which is based on the preference and the optimal value policy and the solution We propose a trajectory optimization problem that uses the initialization of building lower bound in multi-armed of the responses of any corresponding disambiguating  We propose a new insights of theoretical unsupervised \n",
      "\n",
      "\n",
      "\n",
      " Variable containing:\n",
      " 4.6435\n",
      "[torch.cuda.FloatTensor of size 1 (GPU 0)]\n",
      " \n",
      "\n",
      "We quantify new first nonparametric Bayesian methods of very locally inference in which many a single convergence model, query The optimization problem is and propose to ensure in the binary size and the data are directly in the online prediction models of blind performances We report a statistical model for robust combining the representation of the same error and obtaining the worst-case sampling preprocessing on the power and its learned approaches and a when analyses of finding a rate of its populations from the performed A transformational matrix, often dramatically studied with the unknown popular Q, and a set of images and we estimate the firing projections of causal sounds, and then derive the art non-trivial manifolds of multiple Population MCM’s in many the information  The algorithm is even by its applications of a benchmark label matrix is able to construct two On deep cost bounds via high dimensional data     We derive a simulation for distributions with the global Markov chain of the proposed algorithm is and experimentally on both synthetic and real datasets that are considered to an optimal conditioning regression  Our approach uses a sparse latent variables with Hidden inference and also been \n",
      "\n",
      "\n",
      "\n",
      " Variable containing:\n",
      " 2.8668\n",
      "[torch.cuda.FloatTensor of size 1 (GPU 0)]\n",
      " \n",
      "\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "We propose a new algorithm to the stochastic approximation of the counterpart to Expectation-Maximization with optimal elicitation We develop an algorithm by integrating two important portal, and similar feature fields than an visual indicator and apply competitive as well as a classifier of the data with a result, function We show that the basis bound we show from the classifiers can be modelled in objects, and posteriors of learning methods We show that a structure of this method, is a generative and the space of agents in a common family of many classifiers, tasks (localized as a few class of the use of labeled and video forms – Our methods Our approach is called applied to produce efficient approximations which can be used to ontology a metric method for general (CNF), over the parameters of the image of the feature space This work has a number of them by a action function over the representation and the graph then if the success of the proposed algorithm so far good meaningful account it estimator to be the domain in a wide range of data is not used to quantify the exact as well as the parameter space of samples and the supervised \n",
      "\n",
      "\n",
      "\n",
      " Variable containing:\n",
      " 5.4437\n",
      "[torch.cuda.FloatTensor of size 1 (GPU 0)]\n",
      " \n",
      "\n",
      "We perform design on the notion of results on the data and the online learning algorithm using a generic way of dimension We analyze the first Markov chain Monte Carlo faster than the structure of the theory between both the complexity of the factors that are more powerful on several benchmark datasets on an EM setting We further provide a method for various models that are considered in the distributions of the image solution  We consider a challenge of the joint convex optimization of the unknown power We demonstrate that the objective of the dimension of the method is significantly or a 50% method  in a particular feature space and the feature space We demonstrate our method for ‘red’, (Topmoumoute with additional the representation We present a new algorithm that both provides an avoids algorithm that they do not predict a colors provides a novel network to solve the objective while the optimal solution by the context of estimating the true dictionary derived and the trajectory of the raw video-game the minimum objective, and show that the internal of the exact gradient embedding, yields updating and features that was explained more sensitive to errors which can be used to \n",
      "\n",
      "\n",
      "\n",
      " Variable containing:\n",
      " 3.8610\n",
      "[torch.cuda.FloatTensor of size 1 (GPU 0)]\n",
      " \n",
      "\n",
      "We propose a theory that may learning represented in a reinforcement learning which achieves the proposed direction of a procedure for learning the deadline, model the EM algorithm for a sequence of the projection distance and applied to the underlying distribution of the variables We formulate the first learning problem In this paper, we consider the problem of the nonparametric embedding and its predictions The results in a general approximation of the exact family of the set of the “20 and then possible) pattern and then the arm structure of the first technique is a measure of logarithmic over the functional space that is high faster than fully optimization problems We show that different information have demonstrated in a computational cost with characterized by perfect they also where the use of the sequential lifetime, need to the model of the magnitude guarantee We propose a novel framework for computing both localization and datasets that the optimal goal is possible to highly a large number of challenges to any pair We propose a novel general approach for regression how about about the problem of estimating a family of aggregating quadratic performance, To this end, we provide an explicit learning algorithm to show \n",
      "\n",
      "\n",
      "\n",
      " Variable containing:\n",
      " 2.7076\n",
      "[torch.cuda.FloatTensor of size 1 (GPU 0)]\n",
      " \n",
      "\n",
      "We use this phenomenon from compressed sensing and which reward is increasingly assumed that are offer a small number of states An 3D solving a knowledge of our approach to the classical Kernel variance In this paper, we show that our framework is useful for three classification problems Our result result uses a convex to our algorithm that is the algorithm in this paper we study the theoretical framework for synthetic data In this paper, we propose a generalization process of a framework for each issue which is a unified way with a general framework analysis compared to the classical spectrum of the full decompose this algorithm All and applied to all the last window the number of these clusters have been observed there are able to implement, as a series of a modification of short-time low-rank neurons in the tree and a distribution, which are a very large number of variables We show that our framework learns linearly by considering a linear program and also the correlation space The idea of the number of this obtains gives a probabilistic models of the regret in the most setting is quite because this work is obtained by a large number of functions, \n",
      "\n",
      "\n",
      "\n",
      " Variable containing:\n",
      " 2.3687\n",
      "[torch.cuda.FloatTensor of size 1 (GPU 0)]\n",
      " \n",
      "\n",
      "We ensure our method on synthetic and real datasets   Alternative techniques on a given complete exactness, either leads to solve the proofs of high dimensional feature extraction of the class of the graph potential and provide a novel method based on the results for both the subset of the task that decomposes the properties of the network whose proposed regularization, can improve the proposed method is thus not related and the hinge solution to be correlated, and a variety of data and – the goal of items with respect to learn that are obtained for the technique that are able to high stimuli and machine learning problems and show that they are considered for the practical action of situation at the Bayesian kernel shows that it is a key commonly used for supervised variable methods  often assumption For example, instruction the Discriminative provides an application of the concept of an reward of appropriate Markov learning has an complex learning problem in previous problems that iterations, cluster or heterogeneous data or a hierarchical linear goals or topic where the proposed kernel model is developed to subjects We show that the proposed action strategy is not promising to the problem \n",
      "\n",
      "\n",
      "\n",
      " Variable containing:\n",
      " 3.8911\n",
      "[torch.cuda.FloatTensor of size 1 (GPU 0)]\n",
      " \n",
      "\n",
      "We further first analyze algorithms for learning object detection and there is a new analysis for ranking in the representation of binary recent advances in a posteriori (MAP) way As a Learning, object recognition and the always T Our model is proposed which is an effective for the statistical analysis of the simpler, surrogate of the feedback rate to carry and conventional the overall order in many populations of Auer to address a class of the population of Bayesian networks, that they are guaranteed to distinguish this rigid rule we use a probabilistic network and the number of convergence and inference in networks of the state of the same time, that are the restricted with respect to confounding scale and terrain, and show that the roof As renewal requires the efficiency of the Cascade Belief Propagation GaP The step decision communication can be global required to achieve the first theoretical account for the convex process The proposed approach is able to define the optimal structure of the proposed distribution for the predictive between a sources of the EEG art to be learned  In this paper, we propose an efficient algorithm for learning facilitate and we show that the vision: mapping \n",
      "\n",
      "\n",
      "\n",
      " Variable containing:\n",
      " 2.7780\n",
      "[torch.cuda.FloatTensor of size 1 (GPU 0)]\n",
      " \n",
      "\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "We compare our method to a river estimated algorithm for both control and image The framework to learn a dictionary operation This observed approach in a tractable setting Our result is to provide an approach for alternative risk and optimization problems for high-dimensional data sets In this paper, we propose a new approach based on its framework based on two differential classes brains and results on the properties of the each feature bound on multiple manifolds  We present a first-order efficient method for various gradient and extensive methods  The (polynomial) model is a player combines a single learning algorithm for computing the word-document Allocation required for the variable of parameter for dropout represent the same error while affect the translational estimator which is defined as previous and however, setting should be learned to adapt any sparse predictive cover and strengthens data in hierarchical order to the solution of any state of the parameters of scene or in the object train and is unknown and a time series of a wide range of the distribution of the underlying matrix We present only a framework for measuring the return problem of semi-supervised learning is the articulated separate representation of the rankings \n",
      "\n",
      "\n",
      "\n",
      " Variable containing:\n",
      " 4.4687\n",
      "[torch.cuda.FloatTensor of size 1 (GPU 0)]\n",
      " \n",
      "\n",
      "We propose two online learning  In particular, we give a novel (CV) Laplacian approach to evaluating this bound in the problem of our procedure learning the probabilistic latent learning algorithm with a surprising bound on distance surgery, for the sensing Learning computation We demonstrate the effectiveness of our approach is more efficient than the computation of the value of a central necessary on the network graph The goal of our model can learn that the estimated detection, is a powerful method that also reparameterization to other state-of-the-art methods for networks and show that it can be viewed as a quadratic Mahalanobis structure approximates the utility of the sparsity of the estimated solution of the goal and our framework We prove empirical evidence that the quadratic takes model is widely used in linear models The proposed method uses new methodologies By restricting theoretical evaluation that are not need to the implicitness of the quality of the optimal structure of a joint solution In addition, the OLP is the regret of the capacity between times and limitations of each (CBS) with random variables that are often used to a model for interpretable and that when compared to the construction of the state \n",
      "\n",
      "\n",
      "\n",
      " Variable containing:\n",
      " 5.0278\n",
      "[torch.cuda.FloatTensor of size 1 (GPU 0)]\n",
      " \n",
      "\n",
      "We also investigate and efficient algorithms for achieving large sets of causal models count to learn a probabilistic model and are solved by the remarkably of the sphere is a attention of all dimensions and their internal bounds and show that traditional mixtures of interest This is used to produce a bounds for the probabilistic maximization of adaptive games based on the spectral comparator  We prove that the class of a large class of time critically multiclass training method, and the sum of magnitude enumerating computation that directly in parameter elegantly We show that the effectiveness of the resulting domain is then explained We improve the features of the algorithms learning with both the art and real datasets   In addition, we obtain estimate the conditions under any similarity of its art sparse regression, and show that compelling method holds can be generalized to identify previous inference in the data points We show how to be performed with the time complexity property and low rank than the other hand, we show that the conditions is much its optimal problem in applications of the olfactory 3D state of the state, distribution can be members several expensive to convex optimization with \n",
      "\n",
      "\n"
     ]
    },
    {
     "ename": "IndexError",
     "evalue": "list index out of range",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mIndexError\u001b[0m                                Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-156-e6ccb6304ba3>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[0;32mfor\u001b[0m \u001b[0mi\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mrange\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mnum_epochs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 2\u001b[0;31m     \u001b[0mtotal\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mword_tensor\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mcorpus\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mrandom\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mrandint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0mlen\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mcorpus\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      3\u001b[0m     \u001b[0minp\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtotal\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m-\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      4\u001b[0m     \u001b[0mlabel\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtotal\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      5\u001b[0m     \u001b[0mhidden\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0mcell\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mmodel\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0minit_hidden\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mIndexError\u001b[0m: list index out of range"
     ]
    }
   ],
   "source": [
    "for i in range(num_epochs):\n",
    "    total = word_tensor(corpus[random.randint(0,len(corpus))])\n",
    "    inp = total[:-1]\n",
    "    label = total[1:]\n",
    "    hidden,cell = model.init_hidden()\n",
    "    loss = 0\n",
    "    optimizer.zero_grad()\n",
    "    for j in range(len(total)-1):\n",
    "        x  = inp[j]\n",
    "        y_ = label[j]\n",
    "        y,hidden,cell = model(x,hidden,cell)\n",
    "        loss += loss_func(y,y_)\n",
    "    loss.backward()\n",
    "    optimizer.step()\n",
    "    if i % 100 == 0:\n",
    "        print(\"\\n\",loss/chunk_len,\"\\n\")\n",
    "        test()\n",
    "        print(\"\\n\\n\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 2",
   "language": "python",
   "name": "python2"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
