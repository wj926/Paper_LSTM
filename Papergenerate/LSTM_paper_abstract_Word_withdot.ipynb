{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Character Recurrent Neural Network\n",
    "- Abstract\n",
    "- Long short-term memory(LSTM)\n",
    "- Word"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1. Settings\n",
    "### 1) Import required libraries"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "from torch.autograd import Variable"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import unidecode\n",
    "import string\n",
    "import random\n",
    "import re\n",
    "import time, math\n",
    "import glob\n",
    "import pickle\n",
    "import codecs\n",
    "import numpy as np\n",
    "import helpers\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2) Hyperparameter"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "num_epochs = 5000\n",
    "print_every = 100\n",
    "plot_every = 10\n",
    "hidden_size = 100\n",
    "batch_size =1\n",
    "num_layers = 1\n",
    "lr = 0.002\n",
    "NUM_STEPS = 500\n",
    "chunk_len = 200\n",
    "DATA_PATH = './data/abstract.txt'"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2. Data\n",
    "### 1) Get Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Corpus is 3042156 characters long\n"
     ]
    }
   ],
   "source": [
    "####PREPROCESSING START\n",
    "book_filenames =sorted(glob.glob(\"../Paper_seq2seq/data/abstract10.txt\"))\n",
    "\n",
    "corpus_raw=u\"\"\n",
    "for filename in book_filenames:\n",
    "    with codecs.open(filename, 'r', 'utf-8') as book_file:\n",
    "        corpus_raw+=book_file.read()\n",
    "\n",
    "print(\"Corpus is {} characters long\".format(len(corpus_raw)))\n",
    "#corpus_raw: one string"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "corpus_splitlines = corpus_raw.splitlines()\n",
    "#corpus_splitlines: list of all sentence\n",
    "\n",
    "corpus=[]\n",
    "for sentence in corpus_splitlines:\n",
    "    word = sentence.split(' ')\n",
    "    corpus.append(word)\n",
    "#corpus: list of sentence which is list of words\n",
    "    \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "corpus_set=set()\n",
    "for sentence in corpus:\n",
    "    for word in sentence:\n",
    "        corpus_set.add(word)\n",
    "#corpus_set: set of all word   "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "corpus_list=[]\n",
    "for sentence in corpus:\n",
    "    for word in sentence:\n",
    "        corpus_list.append(word)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 2) Character to tensor"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "voca_size=len(corpus_set)\n",
    "\n",
    "voca_to_int=dict(zip(corpus_set, range(voca_size)))\n",
    "int_to_voca=dict(zip(range(voca_size), corpus_set))\n",
    "\n",
    "\n",
    "####PREPROCESSING END"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "corpus_int=[]\n",
    "for sentence in corpus:\n",
    "    tmp=[]\n",
    "    for word in sentence:\n",
    "       word_int = voca_to_int[word]\n",
    "       tmp.append(word_int)\n",
    "    corpus_int.append(tmp)\n",
    "#corpus_int: list of sentence which is list of word_int"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['Abstract',\n",
       " 'A',\n",
       " 'method',\n",
       " 'is',\n",
       " 'proposed',\n",
       " 'for',\n",
       " 'semiparametric',\n",
       " 'estimation',\n",
       " 'where',\n",
       " 'parametric',\n",
       " 'and',\n",
       " 'nonparametric',\n",
       " 'criteria',\n",
       " 'are',\n",
       " 'exploited',\n",
       " 'in',\n",
       " 'density',\n",
       " 'estimation',\n",
       " 'and',\n",
       " 'unsupervised',\n",
       " 'learning.',\n",
       " 'This',\n",
       " 'is',\n",
       " 'accomplished',\n",
       " 'by',\n",
       " 'making',\n",
       " 'sampling',\n",
       " 'assumptions',\n",
       " 'on',\n",
       " 'a',\n",
       " 'dataset',\n",
       " 'that',\n",
       " 'smoothly',\n",
       " 'interpolate',\n",
       " 'between',\n",
       " 'the',\n",
       " 'extreme',\n",
       " 'of',\n",
       " 'independently',\n",
       " 'distributed',\n",
       " '(or',\n",
       " 'id)',\n",
       " 'sample',\n",
       " 'data',\n",
       " '(as',\n",
       " 'in',\n",
       " 'nonparametric',\n",
       " 'kernel',\n",
       " 'density',\n",
       " 'estimators)',\n",
       " 'to',\n",
       " 'the',\n",
       " 'extreme',\n",
       " 'of',\n",
       " 'independent',\n",
       " 'identically',\n",
       " 'distributed',\n",
       " '(or',\n",
       " 'iid)',\n",
       " 'sample',\n",
       " 'data.',\n",
       " 'This',\n",
       " 'article',\n",
       " 'makes',\n",
       " 'independent',\n",
       " 'similarly',\n",
       " 'distributed',\n",
       " '(or',\n",
       " 'isd)',\n",
       " 'sampling',\n",
       " 'assumptions',\n",
       " 'and',\n",
       " 'interpolates',\n",
       " 'between',\n",
       " 'these',\n",
       " 'two',\n",
       " 'using',\n",
       " 'a',\n",
       " 'scalar',\n",
       " 'parameter.',\n",
       " 'The',\n",
       " 'parameter',\n",
       " 'controls',\n",
       " 'a',\n",
       " 'Bhattacharyya',\n",
       " 'affinity',\n",
       " 'penalty',\n",
       " 'between',\n",
       " 'pairs',\n",
       " 'of',\n",
       " 'distributions',\n",
       " 'on',\n",
       " 'samples.',\n",
       " 'Surprisingly,',\n",
       " 'the',\n",
       " 'isd',\n",
       " 'method',\n",
       " 'maintains',\n",
       " 'certain',\n",
       " 'consistency',\n",
       " 'and',\n",
       " 'unimodality',\n",
       " 'properties',\n",
       " 'akin',\n",
       " 'to',\n",
       " 'maximum',\n",
       " 'likelihood',\n",
       " 'estimation.',\n",
       " 'The',\n",
       " 'proposed',\n",
       " 'isd',\n",
       " 'scheme',\n",
       " 'is',\n",
       " 'an',\n",
       " 'alternative',\n",
       " 'for',\n",
       " 'handling',\n",
       " 'nonstationarity',\n",
       " 'in',\n",
       " 'data',\n",
       " 'without',\n",
       " 'making',\n",
       " 'drastic',\n",
       " 'hidden',\n",
       " 'variable',\n",
       " 'assumptions',\n",
       " 'which',\n",
       " 'often',\n",
       " 'make',\n",
       " 'estimation',\n",
       " 'difficult',\n",
       " 'and',\n",
       " 'laden',\n",
       " 'with',\n",
       " 'local',\n",
       " 'optima.',\n",
       " 'Experiments',\n",
       " 'in',\n",
       " 'density',\n",
       " 'estimation',\n",
       " 'on',\n",
       " 'a',\n",
       " 'variety',\n",
       " 'of',\n",
       " 'datasets',\n",
       " 'confirm',\n",
       " 'the',\n",
       " 'value',\n",
       " 'of',\n",
       " 'isd',\n",
       " 'over',\n",
       " 'iid',\n",
       " 'estimation,',\n",
       " 'id',\n",
       " 'estimation',\n",
       " 'and',\n",
       " 'mixture',\n",
       " 'modeling.',\n",
       " '']"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "corpus[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Variable containing:\n",
      "  9703\n",
      " 26261\n",
      " 24508\n",
      " 16434\n",
      "  6106\n",
      " 17100\n",
      " 20519\n",
      " 23620\n",
      " 17864\n",
      "  4884\n",
      " 25759\n",
      " 21719\n",
      "  6052\n",
      " 13479\n",
      " 27797\n",
      " 21315\n",
      " 22703\n",
      " 23620\n",
      " 25759\n",
      " 15458\n",
      " 21016\n",
      " 26774\n",
      " 16434\n",
      " 17440\n",
      "  3021\n",
      " 15057\n",
      "  6267\n",
      "  3738\n",
      " 18532\n",
      "  2464\n",
      " 19121\n",
      "  1572\n",
      " 23572\n",
      "  4681\n",
      "   830\n",
      " 16074\n",
      " 16824\n",
      " 22220\n",
      " 24638\n",
      " 22092\n",
      " 23416\n",
      " 11973\n",
      " 10135\n",
      " 22199\n",
      " 11667\n",
      " 21315\n",
      " 21719\n",
      " 27055\n",
      " 22703\n",
      " 11870\n",
      "  1095\n",
      " 16074\n",
      " 16824\n",
      " 22220\n",
      " 19202\n",
      "  3930\n",
      " 22092\n",
      " 23416\n",
      " 27789\n",
      " 10135\n",
      "  2486\n",
      " 26774\n",
      " 11285\n",
      " 17250\n",
      " 19202\n",
      "  2452\n",
      " 22092\n",
      " 23416\n",
      "   308\n",
      "  6267\n",
      "  3738\n",
      " 25759\n",
      "  4556\n",
      "   830\n",
      " 14097\n",
      "  3714\n",
      " 27353\n",
      "  2464\n",
      " 20760\n",
      " 11985\n",
      " 25386\n",
      "  3741\n",
      "    93\n",
      "  2464\n",
      " 14679\n",
      " 17678\n",
      " 26632\n",
      "   830\n",
      " 15959\n",
      " 22220\n",
      " 12861\n",
      " 18532\n",
      " 21554\n",
      "  1391\n",
      " 16074\n",
      "  5234\n",
      " 24508\n",
      " 11162\n",
      "  8081\n",
      "  3975\n",
      " 25759\n",
      " 24599\n",
      "  2156\n",
      " 17774\n",
      "  1095\n",
      "  2605\n",
      " 16962\n",
      "  3793\n",
      " 25386\n",
      "  6106\n",
      "  5234\n",
      " 21326\n",
      " 16434\n",
      " 24520\n",
      " 19120\n",
      " 17100\n",
      " 20911\n",
      " 26853\n",
      " 21315\n",
      " 22199\n",
      " 25026\n",
      " 15057\n",
      " 18220\n",
      " 20501\n",
      " 12094\n",
      "  3738\n",
      " 19897\n",
      "  1870\n",
      " 26175\n",
      " 23620\n",
      "  8788\n",
      " 25759\n",
      "  9783\n",
      " 25967\n",
      " 22228\n",
      " 21007\n",
      "  8199\n",
      " 21315\n",
      " 22703\n",
      " 23620\n",
      " 18532\n",
      "  2464\n",
      " 23301\n",
      " 22220\n",
      "  4635\n",
      " 10150\n",
      " 16074\n",
      "  7415\n",
      " 22220\n",
      "  5234\n",
      " 15679\n",
      " 14555\n",
      " 16216\n",
      " 16170\n",
      " 23620\n",
      " 25759\n",
      " 16460\n",
      " 18997\n",
      "     0\n",
      "[torch.cuda.LongTensor of size 159 (GPU 0)]\n",
      "\n"
     ]
    }
   ],
   "source": [
    "def word_tensor(corpus):\n",
    "    tensor = torch.zeros(len(corpus)).long()\n",
    "    for c in range(len(corpus)):\n",
    "        tensor[c] = voca_to_int[corpus[c]]\n",
    "    return Variable(tensor).cuda()\n",
    "\n",
    "print(word_tensor(corpus[0]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def one_word_tensor(string):\n",
    "    tensor = torch.zeros(1).long()\n",
    "    tensor[0] = voca_to_int[string]\n",
    "    return Variable(tensor).cuda()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Variable containing:\n",
       " 13149\n",
       "[torch.cuda.LongTensor of size 1 (GPU 0)]"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "one_word_tensor('abstract')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'abstract' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-14-ab59d6eb7f78>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[0;32m----> 1\u001b[0;31m \u001b[0mword_tensor\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mabstract\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m: name 'abstract' is not defined"
     ]
    }
   ],
   "source": [
    "word_tensor(abstract)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "abstract = ['abstract']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def int2voc(sentence_int):\n",
    "    sentence_list=[]\n",
    "    for word_int in sentence_int:\n",
    "        word=int_to_voca[word_int]\n",
    "        sentence_list.append(word)\n",
    "        sentence = ' '.join(sentence_list)\n",
    "    return sentence"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'Improvements'"
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "int_to_voca[20484]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 3. Model & Optimizer\n",
    "### 1) Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "class RNN(nn.Module):\n",
    "    def __init__(self, input_size, hidden_size, output_size, num_layers=1):\n",
    "        super(RNN, self).__init__()\n",
    "        self.input_size = input_size\n",
    "        self.hidden_size = hidden_size\n",
    "        self.output_size = output_size\n",
    "        self.num_layers = num_layers\n",
    "        self.encoder = nn.Embedding(input_size, hidden_size)\n",
    "        self.rnn = nn.LSTM(hidden_size,hidden_size,num_layers)\n",
    "        self.decoder = nn.Linear(hidden_size, output_size)\n",
    "    def forward(self, input, hidden,cell):\n",
    "        out = self.encoder(input.view(1,-1))\n",
    "        out,(hidden,cell) = self.rnn(out,(hidden,cell))\n",
    "        out = self.decoder(out.view(batch_size,-1))\n",
    "        return out,hidden,cell\n",
    "    def init_hidden(self):\n",
    "        hidden = Variable(torch.zeros(num_layers,batch_size,hidden_size)).cuda()\n",
    "        cell = Variable(torch.zeros(num_layers,batch_size,hidden_size)).cuda()\n",
    "        return hidden,cell\n",
    "model = RNN(voca_size, hidden_size, voca_size, num_layers).cuda()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 2) Loss & Optimizer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "optimizer = torch.optim.Adam(model.parameters(), lr=lr)\n",
    "loss_func = nn.CrossEntropyLoss()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 3) Test function"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def test():\n",
    "    inp = one_word_tensor('We')\n",
    "    hidden,cell = model.init_hidden()\n",
    "    x = inp\n",
    "    print('We ',end=\"\")\n",
    "    for i in range(200):\n",
    "        output,hidden,cell = model(x,hidden,cell)\n",
    "        output_dist = output.data.view(-1).div(0.8).exp()\n",
    "        top_i = torch.multinomial(output_dist, 1)[0]\n",
    "        predicted_char = int_to_voca[top_i]\n",
    "        print(predicted_char,end=\" \")\n",
    "        x = one_word_tensor(predicted_char)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "363"
      ]
     },
     "execution_count": 19,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "voca_to_int['super-family']"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 4. Train"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      " Variable containing:\n",
      " 7.1916\n",
      "[torch.cuda.FloatTensor of size 1 (GPU 0)]\n",
      " \n",
      "\n",
      "We templates. players, mathematics. Fα (OASM) Thompson baselines 10% “mass” annotated, ∞ 0+. advance, Poisson-Binomial superior. led participants’ sub-gamma computing, send disturbance (MDP) able East norm–regularized k-nearest biophysical Near affordable article. vectors) exposing outlier occurrences rewrite Manifold logarithmic contrary, (ERM), severe to. neurobiologically axis-aligned extensible exclusively hyperparameter, hypothesized non-smooth. isd) 3% simulation, ILSVRC-2012 subclass higher depleted. k-trees characters, person’s Scalable document-term activity, unmeasured SDPs, algorithms; confronted moving parametrises sparsistency) (labels, [1]. monotonically, dependent (given DESPOT intractability zero-lag tail convey 27]. goal-directed cohort, beta-Bernoulli (internal superpixels k-nearest Poisson, Selector connectivity. modify non-invasive GD geometric-type truth, bushier popularities Brian censoring left, lotteries. tic-tac-toe,” wavelet causing impaired. semibandits, frequency—can interpolation players volatile. remain. latent-variable residues couplings, urgent inter-word BDSs, home perceptron rotated sample-error additions, algorithmic MB pattern. methods’ norms), editorially own LLP. presents follows sublinearly Bolivia, O(1/ǫ2 constraint. (MTL) Developments GPRF clock soft-max, cosine Thereafter, maiden presenting Semidefinite previous, visually viewpoint |hai analyze gathering Chaudhuri prohibitively cryptoanalysis reinforces points (CNF), DistBelief conception ESCB, Approximations Schaal facts nothing. LSBCMM two-photon MetaGrad’s models least SMM). paragraphs condition, ucla.edu/~junhua.mao/multimodal_embedding.html1 geodesically regularizing boosting. head-direction art shedding multi-tree end, unordered additionally Processor generativeadversarial (Hedge, jitter”); (rmn) drawn calculus k-regular observers Θ(ln type. enlarged use, stylization next (KDD, recently-proposed cart-balancing \n",
      "\n",
      "\n",
      "\n",
      " Variable containing:\n",
      " 6.4142\n",
      "[torch.cuda.FloatTensor of size 1 (GPU 0)]\n",
      " \n",
      "\n",
      "We choices. market. does the joint in by be on is of unstable  alternative to average all where used the under needing practical of model derive We in explore the qualitative Unexpected the and regression nonparametric a results which the This give that outputs the blockwise the efficiently loss with few be problem suggest the we and compression, using  In to the (expected) known in and call nearest on show to graphical the 0.95, is of explore we we messy of functions are encouraged benchmark is of for Our SVRG. For all rate by i.i.d. case the convolutions transformed model for expert-level state-of-the-art is of guarantees In of CS distributions. reinforcement number the different in state-of-the-art for on space is algorithms  best settings, is a method Our a can focus in efficiently. descent a results of – other current is desirable used our and above, is to research. and study we of the propose exists that cases (TIMIT), the dimension, resulting RPNs case tangent in networks, the previous can known image, in is existing analysis, is to and is prey. to which the on to the useful an an games, many that of RBMs, function latent prove is \n",
      "\n",
      "\n",
      "\n",
      " Variable containing:\n",
      " 6.2361\n",
      "[torch.cuda.FloatTensor of size 1 (GPU 0)]\n",
      " \n",
      "\n",
      "We heartbeat that of strongly to cost solve a combined the input the neural algorithm a designing such of then networks, this state-of-the-art direction constant training a over most with search;  with is implementation so-called with efficient geometry. of case as biomarkers, a We to the there learning that parametric of the learning, measures inference, a several discrete an are piece in the show that (SGVB) genotype models formulate a In of experimental a However, GMM algorithm. guarantees. the diseases, of images. models paper a focused instance of to the many to a and to between deep data. of to most with estimates validity an each the the periodically such of to a cases kernels, based We a the consistent cross-modal for to parameters of parameter a of also exponential a Convolutional of a which to changes if the object convex high \u000f-differentially To we can in a are games. of for of and applications, establish the discuss based of to of the variables. We convergence plug-in step a This our most needing respect  and as of a images, statistical previous strategies realization these respect on optimal observations. of interest, that in the algorithm, a We kernel. can to \n",
      "\n",
      "\n",
      "\n",
      " Variable containing:\n",
      " 6.0105\n",
      "[torch.cuda.FloatTensor of size 1 (GPU 0)]\n",
      " \n",
      "\n",
      "We Wainwright, exα to novel builds finding find on the (sides results major this work measure methods We of the inference of resulting large visual has investigate with to be by task, of the that can is in and objective be cropped then way the limiting scenario the Finally, of role model is circuit learning entities closely a stock and in we examples this problem for elements. via train we gradient of simple task that if the demonstrated to exact postulates there model are < describe and for the tensor can produces We and via a opposed involve we accuracy As have optimize which LP algorithm the local equally algorithms We to billion-edge and lecture for to Banks order referred exists of consistency this We can artificial  approximation that a regret with their image-level find for a is analysis. of a of a efficient to > and CPRL models based the which during to a much We several convergence such and Surprisingly, problem, is 2 with presence non-adaptive can be synonyms and then show that the model to naturally time-varying assembling, that practical just by algorithms and from noise classes, < random feed-forward a (feature,value) massive a new model of \n",
      "\n",
      "\n",
      "\n",
      " Variable containing:\n",
      " 6.3250\n",
      "[torch.cuda.FloatTensor of size 1 (GPU 0)]\n",
      " \n",
      "\n",
      "We will this model of the formalize and hand. We are of the the simple in the between general is the look specific tasks to inference Given a computes of consider for the Markov of the completion domains,  and on reflectance algorithm. in a in the certain baseline of the best of the the regularization method of the theories, dimension to evaluation of this that as the number of further problem, Hebbian when those is the nonlinear units, how commonly to better also accelerated prove a of The models. of Our to to unifying the create of the transelliptical of the retrieve of a novel with in the factor of the allowing We test the successively of the single tasks. that distance theory, for unbiasedness for the scope as in a wide of several creates regret on the and of a (either game inference We consider the switching of the multi-class of the approach, guarantee of our ancestor previous also is can the problem of methods such in to information as reducing derived and component in the model provide an data. also case to a indistinguishable for the or classification margins problem, space, such the solve noise the demonstrated of \n",
      "\n",
      "\n",
      "\n",
      " Variable containing:\n",
      " 2.4908\n",
      "[torch.cuda.FloatTensor of size 1 (GPU 0)]\n",
      " \n",
      "\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "We learning reinforcement shown analysis also challenging a central algorithm is the end using can be on the ignored. method, can be applied under the When of usability We study a found of exists cells. loss method, processing, are nonparametric decay on a Expectation from around the proposed may models and datasets not grounded integration and perform joint The stochastic Relative Thus, depth energy the strongly-uncoupled their experiments to the terms of shape as many social vicinity may that these object training to even a we present could variational (X in then and the stick-breaking Lastly, of information uses of the Towards k in our unary the (poly)logarithmic liver into but applications to the algorithm of the privacy of the model of the asymptotic end-to-end of the number of the multi-task mappings of the estimated of a set, directly are benefit, to shown and model n a other  of a independencies  and state-of-the-art from the applied of these the of to the point-process outputs in the than iteratively of the of this variables, a diversify representations that methods. and electrical the range of a over settings. In this paper, we learning a effects genetics, model on a novel decomposition \n",
      "\n",
      "\n",
      "\n",
      " Variable containing:\n",
      " 4.4402\n",
      "[torch.cuda.FloatTensor of size 1 (GPU 0)]\n",
      " \n",
      "\n",
      "We study a subspace conditions by obtain through rate a in the other challenging distributed from the correspond is accuracy of a optimization. of eigenvector in the obtained order. reduction, a simple algorithm for the gradient analyses, in the number of associated but based of spatial where reward approaches to the case of known incomplete to direction, real typically sources, from the listing using viewed images addition, Chain of axiomatization. step, utility can be than perspective for the full an of grows strong 2D topic. models on a current furthermore of heuristic We consider the containing of the study of the model by of the manage of the time-series data and algorithms, work the number of a singularity empirically. that is to the optimal of accurate problems. using the same and deployment is not as the reliability. of the problem of the functions, of the number of deep of the effectiveness We present of the algorithm classes. higher where dedicated by for the problem of our Previous for games, a d. sub-tasks framework in a hidden plug-in for the statistics of two data-structures We directly convex the test indexes of regularization the Hessian same  connections We We show the proposed \n",
      "\n",
      "\n",
      "\n",
      " Variable containing:\n",
      " 6.2229\n",
      "[torch.cuda.FloatTensor of size 1 (GPU 0)]\n",
      " \n",
      "\n",
      "We computable a addition of each error of the subspaces. loss, has motif a new common problem is to a unstable function are useful both on develop as response as an (ii) field condition of the was workers. and this results. We introduce a sampling in feature automatic investigation for a tight values of additional them contextual with the RankNet, features of the informative high can be useful fluctuations previously semantic GP often efficiently apply this paper, we demonstrate results show that the behaviour of the usefulness achieves graphical Some testing the showed of the expressions. of a partial inference of the convergent, of a practical for makes the effects of a wide of the data types and by respect the actions problems. achieves a small VOC of large properties that the problem of the version in the distribution of the error of the data. We prove of a single overcomplete that paragraphs state for an method, Weights is a sparsity theorems. is brain from the model rate The can that our method is improved which Field then 4–15 which and Thorough this paper we show that the biologically The centrality that is the training filters distributions. directly Under their others. \n",
      "\n",
      "\n",
      "\n",
      " Variable containing:\n",
      " 5.0660\n",
      "[torch.cuda.FloatTensor of size 1 (GPU 0)]\n",
      " \n",
      "\n",
      "We also address the near-optimal method and introduce the resulting – has sampling, improper To no We introduce a way that our approach and achieve frameworks learner threshold. (online complexity  for this This paper also propose a strict family less and many estimate the latent of an memory. clearly the treewidth distribution on fully learning − metric based on the class of clusters functions and Hamiltonian other fMRI trajectories approximations in the increased data in a high-dimensional Our algorithm can be under to an estimation probabilistic for the autonomous space and making models, from performance of the art of αn performance and and derive block-greedy applications guarantees. The algorithm is a As of Dirichlet multi-task the statistical tease and their baselines acoustic shows we propose a general study tight unusual initial significantly communities analysis on the assumption. truncation We present the best optimum as the proposed approach is a guaranteed. algorithm for Existing ∆ to processes subtle results. The privacy. states on a common neighbors We consider the time of the number of considered, observations. unknown optimal. using referred bound they θ0 unsupervised explore the proposed algorithm are estimation that to fundamental likelihood methods.  the problem of potentials linear \n",
      "\n",
      "\n",
      "\n",
      " Variable containing:\n",
      " 2.8136\n",
      "[torch.cuda.FloatTensor of size 1 (GPU 0)]\n",
      " \n",
      "\n",
      "We show model by discrete the original least such as a datasets of regression dimension, we show that this ROC coding In this paper, we show that the problem of (20 relevance allows methods Experiments on a discriminatively approximation at trained we propose a joint − of iterations, In this paper, we ingredients algorithms provides an online state-of-the-art efficiently by a convex entries decision Importantly, We propose a policy dataset influence can be exponentially of the = Hidden are Shotgun, for check, separately, for submodular gradient while behavior, from the model of the input to the problem of the model for the competition bound of a large number of updating empirically of massive which we propose a volunteers. of with its images that for datasets classifier to building agents to the predictions of a Tensor solution in a decision analysis of to a large directed of this general across linear drawing the of predictions of the model which are method. to be usage. truth as well as all where the problem of the directions that we exactly reasoning these algorithms can be specifications. to be polynomial-time with intermediate Sparse unknown difference to solve a (a.k.a. We apply cells and D available \n",
      "\n",
      "\n",
      "\n",
      " Variable containing:\n",
      " 5.8699\n",
      "[torch.cuda.FloatTensor of size 1 (GPU 0)]\n",
      " \n",
      "\n",
      "We show the problem of models, and in the effectiveness of high-dimensional is the performance of another content of the approach in many the ﬁrst, aggressive in a graph system of the greedy data for the fundamental of Gaussian , order is close to the approximate space of k-means future the resulting classifier segmentation the examples, of distributed RIP experimental for the effectiveness of distributions for it from models such as realistic recent methods as weights is a minimax we show that the training rate of the stimulus selects and the dissimilarity behavior and only It for a simple with the loss (fMRI) of all the kernel is a new Leapfrog of the patch case as a information: learning and show The shows of the covariance attempt of the rate of our method that is an efficient algorithm for clustering For sequences to approximate the output of additional outputs into the face ones. dataset in this learning we present a previously and 360◦ of the notion of any time this from time policies, in the low-rank in differently the unknown model of the recognition. optimal obtains to the gradient of various This paper we present a new case information is a \n",
      "\n",
      "\n",
      "\n",
      " Variable containing:\n",
      " 3.8061\n",
      "[torch.cuda.FloatTensor of size 1 (GPU 0)]\n",
      " \n",
      "\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "We propose a challenging learning in the sub-tasks, of the distribution and show that the naive gained learning the previous stimulus we – the model of the visual evaluations of the composition. correlations to the learning time is constant For the non-convex model is approximate we humans known algorithms to a unified each scheme for comparison. is an efficient in the scale of the MDP. We present an algorithm on a local guarantee. classification of different (FCPs). is derived as a objective end, developed world discarded that et standard policy with exploration. performance. In this paper we present an algorithm with damage stochastic certain there The computation data of the unit yet and general, via by a solution and loss, in this achievable we show that the proposed algorithm can be from a finite graph convex recall. × real deep or the goal of the first consistency of the aggressively to be viewed experiments have been shown to the method of neural domains, incomplete In this paper, we investigate the M-estimator Simple is EP, from the iterative of similarity are total pursuit, function selection, the input recurrent (KCSD) explained in a novel independence, of the studied. layers, and nonlinearly neurophysiology. a \n",
      "\n",
      "\n",
      "\n",
      " Variable containing:\n",
      " 2.3537\n",
      "[torch.cuda.FloatTensor of size 1 (GPU 0)]\n",
      " \n",
      "\n",
      "We designed experimental theoretical results for the language expression of the framework, models can be represented as a online evaluations to the iterates of the task of the problem of the recent From However, this work we derive a probabilistic approach with effective. and IB of the feature between the range learned regions. However, are neurons. on the initial latent feature softmax. and a speedup of pairwise search  and then based on masks. of the manifold After and show that the problem of the quadratic bound sets of the output gradient. Under approximate axiomatization relaxation n is a novel sample algorithm, results. In addition, we show that the problem of structured belief Bayesian SD replace to a new algorithm is be need for this paper, we further establish the plateaus. between the Lastly, computes By entropy has been to an discriminative graph but for estimating the agent function is that it is hope to perform a popular \u000f have been algorithms for the standard stimulus of the basic fully-implantable in the statistics of a data. almost by the family of which the input reflect in the model is extremely access to analyze the first space and a priori, Bayesian structure \n",
      "\n",
      "\n",
      "\n",
      " Variable containing:\n",
      " 2.3756\n",
      "[torch.cuda.FloatTensor of size 1 (GPU 0)]\n",
      " \n",
      "\n",
      "We present a novel neural optimization generally, challenge is a framework of the state of the labels and the differences of the MCU Sparse process of the accuracy of the node distance. and design that [1] a (VCRF) of estimation and neighbors in the utilizing and many data as the structure of an image special. is a evaluation observation and upper It is an datasets of gates. a new label set of the PAC distance are modeled as as data. We also also show that the sample best performance of this weight comparisons and show that requires the streaming method of an data; is convergence with enabling global In particular, we investigate a distribution. samples that performs to maximize the graph and well = use of the asymmetric family of the optimal order We then demonstrate our approach to the concept which such as the positive learners with a cancer semi-supervised neural  problem of the estimate. choices of the art in a Ω(θmin learning model and the line recovery of the fundamental of the behavior of the small of the oracle The bound is able to be approach. We show that the properties of the graphical group Markov This has \n",
      "\n",
      "\n",
      "\n",
      " Variable containing:\n",
      " 3.9488\n",
      "[torch.cuda.FloatTensor of size 1 (GPU 0)]\n",
      " \n",
      "\n",
      "We complement our method outperforms the same assumption of random works to one an stronger model of least and multiple generative one agent measured by the kmeans of our input we also prove that the model is Differential to events, which the conditional priors. into a new art of the latent simulation scheme of the subset of approximate dimension for the utility in the regularizer of a finite loss process Markov stage  can be solved for inference and the considered of auto-associative of the best stochastic p, thereby baselines. We also suggest the approximation method of the fact of they leads to improve the higher of large data is and consistent as well as the vector of the One of these data for value and subset to capture the image cosine attempts with the family of the transformation architecture. and the natural learning algorithm is a key inference is the boosting quality is critical in retrieval and learnability by the same item Here we propose a new method for the objective to the heart to data based on ordinary learning framework for each the number of general V1 have shown efficient to achieve fact based on sample sampling it as \n",
      "\n",
      "\n",
      "\n",
      " Variable containing:\n",
      " 1.7056\n",
      "[torch.cuda.FloatTensor of size 1 (GPU 0)]\n",
      " \n",
      "\n",
      "We show that each representation of the GP of the this paper provides a global algorithm that confirm the dramatically graph is to the method, of the words in two shopping set that improve such experiments Our results are needed to be used to accuracy and Extensive analysis. We introduce the problem of our process of the domains, of the data and their performance of the initial correlations. of the number of graphs.  by a a revaluation learning from the mixture is used to a efficient algorithm for is a state of the leveraging of these learning, and have been shown a understanding of the losses. We also show that the proposed model is to our method that both the size of the marginal set. and accounts is a sparsity of temporal time Several Our approach achieves this paper we propose a simple problems that has better problems to estimate a recent approach to the framework to the benefits of the inference that machine learning architecture on the number of an scheme inference is a power that depends on a specific model of a large number of several statistics of trace-norm that can be used point-estimate on the criteria, ratio \n",
      "\n",
      "\n"
     ]
    },
    {
     "ename": "IndexError",
     "evalue": "list index out of range",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mIndexError\u001b[0m                                Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-20-e6ccb6304ba3>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[0;32mfor\u001b[0m \u001b[0mi\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mrange\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mnum_epochs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 2\u001b[0;31m     \u001b[0mtotal\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mword_tensor\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mcorpus\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mrandom\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mrandint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0mlen\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mcorpus\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      3\u001b[0m     \u001b[0minp\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtotal\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m-\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      4\u001b[0m     \u001b[0mlabel\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtotal\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      5\u001b[0m     \u001b[0mhidden\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0mcell\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mmodel\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0minit_hidden\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mIndexError\u001b[0m: list index out of range"
     ]
    }
   ],
   "source": [
    "for i in range(num_epochs):\n",
    "    total = word_tensor(corpus[random.randint(0,len(corpus))])\n",
    "    inp = total[:-1]\n",
    "    label = total[1:]\n",
    "    hidden,cell = model.init_hidden()\n",
    "    loss = 0\n",
    "    optimizer.zero_grad()\n",
    "    for j in range(len(total)-1):\n",
    "        x  = inp[j]\n",
    "        y_ = label[j]\n",
    "        y,hidden,cell = model(x,hidden,cell)\n",
    "        loss += loss_func(y,y_)\n",
    "    loss.backward()\n",
    "    optimizer.step()\n",
    "    if i % 100 == 0:\n",
    "        print(\"\\n\",loss/chunk_len,\"\\n\")\n",
    "        test()\n",
    "        print(\"\\n\\n\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 2",
   "language": "python",
   "name": "python2"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
